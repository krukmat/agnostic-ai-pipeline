{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Owner LoRA Fine-tuning en Google Colab\n",
    "\n",
    "**Objetivo**: Entrenar modelo Product Owner usando Qwen2.5-7B + LoRA\n",
    "\n",
    "**Requirements**: Google Colab Free con GPU T4 (15GB VRAM)\n",
    "\n",
    "**Tiempo estimado**: 3-5 horas para 3 epochs con ~200 samples\n",
    "\n",
    "**Setup**: Solo necesitas clonar el repo (el dataset est√° incluido)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Verificar GPU y CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar GPU disponible\n",
    "!nvidia-smi\n",
    "\n",
    "# Verificar PyTorch y CUDA\n",
    "import torch\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"\\n‚úÖ GPU is ready!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå No GPU detected!\")\n",
    "    print(f\"   Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: T4 GPU\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Instalar Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar todas las librer√≠as necesarias\n",
    "print(\"üì¶ Installing dependencies...\\n\")\n",
    "\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers==4.36.0 peft==0.7.0 bitsandbytes==0.41.0 accelerate==0.25.0 datasets==2.16.0 typer\n",
    "\n",
    "# Verificar instalaci√≥n\n",
    "print(\"\\nüîç Verifying installation...\\n\")\n",
    "import transformers\n",
    "import peft\n",
    "import bitsandbytes\n",
    "import datasets\n",
    "import typer\n",
    "\n",
    "print(f\"‚úÖ transformers: {transformers.__version__}\")\n",
    "print(f\"‚úÖ peft: {peft.__version__}\")\n",
    "print(f\"‚úÖ bitsandbytes: {bitsandbytes.__version__}\")\n",
    "print(f\"‚úÖ datasets: {datasets.__version__}\")\n",
    "print(f\"\\n‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Clonar Repositorio desde GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clonar el repositorio desde el branch correcto (incluye el dataset)\nprint(\"üì• Clonando repositorio desde GitHub...\\n\")\n\n!git clone -b dspy-multi-role https://github.com/krukmat/agnostic-ai-pipeline.git\n%cd agnostic-ai-pipeline\n\n# Verificar branch y dataset\nprint(\"\\nüîç Verificando branch y dataset...\\n\")\n!git branch --show-current\n!ls -lh artifacts/distillation/po_teacher_supervised.jsonl\n\nprint(\"\\n‚úÖ Repositorio clonado exitosamente en branch dspy-multi-role!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Verificar Dataset y Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç Verificando archivos necesarios...\\n\")\n",
    "\n",
    "# Paths\n",
    "script_path = Path(\"/content/agnostic-ai-pipeline/scripts/train_po_lora.py\")\n",
    "dataset_path = Path(\"/content/agnostic-ai-pipeline/artifacts/distillation/po_teacher_supervised.jsonl\")\n",
    "\n",
    "# Verificar script\n",
    "if script_path.exists():\n",
    "    print(f\"‚úÖ Script encontrado: {script_path}\")\n",
    "else:\n",
    "    print(f\"‚ùå Script NO encontrado: {script_path}\")\n",
    "\n",
    "# Verificar dataset\n",
    "if dataset_path.exists():\n",
    "    print(f\"‚úÖ Dataset encontrado: {dataset_path}\")\n",
    "    \n",
    "    # Contar registros\n",
    "    with open(dataset_path) as f:\n",
    "        samples = [line for line in f if line.strip()]\n",
    "        print(f\"‚úÖ Dataset tiene {len(samples)} registros\")\n",
    "    \n",
    "    # Ver primer sample\n",
    "    with open(dataset_path) as f:\n",
    "        first = json.loads(f.readline())\n",
    "        print(f\"\\nüìù Keys: {list(first.keys())}\")\n",
    "        print(f\"üìè Prompt length: {len(first['prompt'])} chars\")\n",
    "        print(f\"üìè Response length: {len(first['response'])} chars\")\n",
    "        print(f\"\\nüîç Preview prompt:\")\n",
    "        print(first['prompt'][:300] + \"...\")\n",
    "        print(f\"\\n‚úÖ Dataset format is correct!\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset NO encontrado: {dataset_path}\")\n",
    "    print(f\"\\nüîç Buscando dataset en el repo...\")\n",
    "    !find /content/agnostic-ai-pipeline -name \"po_teacher_supervised.jsonl\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Ejecutar Training\n",
    "\n",
    "**Par√°metros optimizados para T4 (15GB VRAM)**:\n",
    "- 4-bit quantization para reducir memoria\n",
    "- Gradient checkpointing activado\n",
    "- Batch size 1 + gradient accumulation 8 = batch efectivo de 8\n",
    "- Max length 2048 (ajusta a 1536 si hay OOM)\n",
    "\n",
    "**Tiempo estimado**: 3-5 horas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar training\n",
    "!python scripts/train_po_lora.py \\\n",
    "    --data-path /content/agnostic-ai-pipeline/artifacts/distillation/po_teacher_supervised.jsonl \\\n",
    "    --base-model Qwen/Qwen2.5-7B-Instruct \\\n",
    "    --output-dir /content/agnostic-ai-pipeline/artifacts/models/po_student_v1 \\\n",
    "    --rank 32 \\\n",
    "    --alpha 64 \\\n",
    "    --dropout 0.05 \\\n",
    "    --epochs 3 \\\n",
    "    --batch-size 1 \\\n",
    "    --gradient-accumulation-steps 8 \\\n",
    "    --lr 1e-4 \\\n",
    "    --max-length 2048 \\\n",
    "    --load-4bit \\\n",
    "    --bnb-compute-dtype float16 \\\n",
    "    --gradient-checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö® Si el training falla con \"CUDA out of memory\", ejecuta esta celda alternativa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comando ALTERNATIVO con par√°metros m√°s conservadores\n",
    "# Descomenta y ejecuta si el comando anterior fall√≥ con OOM\n",
    "\n",
    "# !python scripts/train_po_lora.py \\\n",
    "#     --data-path /content/agnostic-ai-pipeline/artifacts/distillation/po_teacher_supervised.jsonl \\\n",
    "#     --base-model Qwen/Qwen2.5-7B-Instruct \\\n",
    "#     --output-dir /content/agnostic-ai-pipeline/artifacts/models/po_student_v1 \\\n",
    "#     --rank 16 \\\n",
    "#     --alpha 32 \\\n",
    "#     --dropout 0.05 \\\n",
    "#     --epochs 3 \\\n",
    "#     --batch-size 1 \\\n",
    "#     --gradient-accumulation-steps 16 \\\n",
    "#     --lr 1e-4 \\\n",
    "#     --max-length 1536 \\\n",
    "#     --load-4bit \\\n",
    "#     --bnb-compute-dtype float16 \\\n",
    "#     --gradient-checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Monitorear GPU (Ejecutar en otra celda mientras entrena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver uso de GPU en tiempo real (actualiza cada 5 segundos)\n",
    "!nvidia-smi --query-gpu=utilization.gpu,utilization.memory,memory.used,memory.total --format=csv -l 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Verificar Checkpoints Guardados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver checkpoints guardados despu√©s del training\n",
    "print(\"üìÅ Checkpoints guardados:\\n\")\n",
    "!ls -lh /content/agnostic-ai-pipeline/artifacts/models/po_student_v1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Test de Inferencia R√°pido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "print(\"üîÑ Loading model...\\n\")\n",
    "\n",
    "base_model = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "adapter_path = \"/content/agnostic-ai-pipeline/artifacts/models/po_student_v1\"\n",
    "\n",
    "# Cargar tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Cargar modelo base en 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Cargar adapter LoRA\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "\n",
    "print(\"‚úÖ Model loaded!\\n\")\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"\"\"[INSTRUCTIONS]\n",
    "You are a Product Owner AI agent validating business requirements.\n",
    "\n",
    "[REQUIREMENTS]\n",
    "business_domain: Blog Platform\n",
    "primary_features:\n",
    "  - Users can create blog posts with title and content\n",
    "  - Posts must support markdown formatting\n",
    "  - User authentication is required\n",
    "\n",
    "[YOUR RESPONSE]\"\"\"\n",
    "\n",
    "print(\"üí≠ Test prompt:\")\n",
    "print(test_prompt)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Generate\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"ü§ñ Model response:\")\n",
    "print(response[len(test_prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Descargar Modelo Entrenado\n",
    "\n",
    "**Opci√≥n recomendada**: Comprimir y descargar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"üì¶ Compressing model...\\n\")\n",
    "!zip -r po_student_v1.zip /content/agnostic-ai-pipeline/artifacts/models/po_student_v1\n",
    "\n",
    "print(\"\\n‚¨áÔ∏è Downloading...\")\n",
    "files.download('po_student_v1.zip')\n",
    "\n",
    "print(\"\\n‚úÖ Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Troubleshooting\n",
    "\n",
    "### Error: CUDA out of memory\n",
    "**Soluci√≥n**: Ejecuta la celda alternativa (Cell 5.2) con par√°metros m√°s conservadores\n",
    "\n",
    "### Error: No module named 'bitsandbytes'\n",
    "**Soluci√≥n**: Reinstala bitsandbytes\n",
    "```python\n",
    "!pip uninstall -y bitsandbytes\n",
    "!pip install bitsandbytes==0.41.0 --no-cache-dir\n",
    "```\n",
    "\n",
    "### Error: No GPU detected\n",
    "**Soluci√≥n**: \n",
    "1. Runtime ‚Üí Change runtime type\n",
    "2. Hardware accelerator: **T4 GPU**\n",
    "3. Save y reconectar\n",
    "\n",
    "### Training muy lento (< 1 it/s)\n",
    "**Soluci√≥n**: Usa la celda alternativa con `--max-length 1536` o `--gradient-accumulation-steps 4`\n",
    "\n",
    "---\n",
    "\n",
    "## üìä M√©tricas Esperadas\n",
    "\n",
    "```\n",
    "Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [12:34<00:00,  0.05it/s]\n",
    "{'loss': 1.2345, 'learning_rate': 9.5e-05, 'epoch': 1.0}\n",
    "\n",
    "Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [12:31<00:00,  0.05it/s]\n",
    "{'loss': 0.8912, 'learning_rate': 5.0e-05, 'epoch': 2.0}\n",
    "\n",
    "Epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [12:29<00:00,  0.05it/s]\n",
    "{'loss': 0.6234, 'learning_rate': 5.0e-06, 'epoch': 3.0}\n",
    "```\n",
    "\n",
    "**Indicadores de √©xito**:\n",
    "- ‚úÖ Loss decreciente (1.2 ‚Üí 0.6)\n",
    "- ‚úÖ No crashes por OOM\n",
    "- ‚úÖ ~0.03-0.05 it/s en T4\n",
    "- ‚úÖ Checkpoints guardados cada epoch\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Referencias\n",
    "\n",
    "- **Setup Guide**: `docs/COLAB_TRAINING_SETUP.md`\n",
    "- **Training Script**: `scripts/train_po_lora.py`\n",
    "- **HuggingFace Qwen2.5**: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct\n",
    "- **PEFT Docs**: https://huggingface.co/docs/peft/\n",
    "\n",
    "---\n",
    "\n",
    "**√öltima actualizaci√≥n**: 2025-11-13"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}