{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Owner LoRA Fine-tuning en Google Colab\n",
    "\n",
    "**Objetivo**: Entrenar modelo Product Owner usando Qwen2.5-7B + LoRA\n",
    "\n",
    "**Requirements**: Google Colab Free con GPU T4 (15GB VRAM)\n",
    "\n",
    "**Tiempo estimado**: 3-5 horas para 3 epochs con ~200 samples\n",
    "\n",
    "**Setup**: Solo necesitas clonar el repo (el dataset est\u00e1 incluido)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\ufe0f\u20e3 Verificar GPU y CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar GPU disponible\n",
    "!nvidia-smi\n",
    "\n",
    "# Verificar PyTorch y CUDA\n",
    "import torch\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"\\n\u2705 GPU is ready!\")\n",
    "else:\n",
    "    print(f\"\\n\u274c No GPU detected!\")\n",
    "    print(f\"   Runtime \u2192 Change runtime type \u2192 Hardware accelerator: T4 GPU\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\ufe0f\u20e3 Instalar Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Instalar todas las librer\u00edas necesarias (compatibles con CUDA 12.x)\nprint(\"\ud83d\udce6 Installing dependencies...\\n\")\n\n# NO reinstalar PyTorch - Colab ya tiene PyTorch con CUDA 12.x\n# Instalar versiones compatibles con CUDA 12.x\n!pip install -q transformers>=4.40.0 peft>=0.10.0 bitsandbytes>=0.43.0 accelerate>=0.28.0 datasets>=2.18.0 typer\n\n# Verificar instalaci\u00f3n\nprint(\"\\n\ud83d\udd0d Verifying installation...\\n\")\nimport torch\nimport transformers\nimport peft\nimport bitsandbytes\nimport datasets\nimport typer\n\nprint(f\"\u2705 torch: {torch.__version__}\")\nprint(f\"\u2705 CUDA available: {torch.cuda.is_available()}\")\nprint(f\"\u2705 CUDA version: {torch.version.cuda}\")\nprint(f\"\u2705 transformers: {transformers.__version__}\")\nprint(f\"\u2705 peft: {peft.__version__}\")\nprint(f\"\u2705 bitsandbytes: {bitsandbytes.__version__}\")\nprint(f\"\u2705 datasets: {datasets.__version__}\")\nprint(f\"\\n\u2705 All packages installed successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5\ufe0f\u20e3 Configurar entorno (W&B / memoria CUDA)\n",
    "\n",
    "Evita prompts de Weights & Biases y reduce la fragmentaci\u00f3n de VRAM fijando estas variables globales antes de entrenar."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "print(f\"WANDB_DISABLED = {os.environ['WANDB_DISABLED']}\")\n",
    "print(f\"PYTORCH_CUDA_ALLOC_CONF = {os.environ['PYTORCH_CUDA_ALLOC_CONF']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\ufe0f\u20e3 Clonar Repositorio desde GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clonar el repositorio desde el branch correcto (incluye el dataset)\nprint(\"\ud83d\udce5 Clonando repositorio desde GitHub...\\n\")\n\n!git clone -b dspy-multi-role https://github.com/krukmat/agnostic-ai-pipeline.git\n%cd agnostic-ai-pipeline\n\n# Verificar branch y dataset\nprint(\"\\n\ud83d\udd0d Verificando branch y dataset...\\n\")\n!git branch --show-current\n!ls -lh artifacts/distillation/po_teacher_supervised.jsonl\n\nprint(\"\\n\u2705 Repositorio clonado exitosamente en branch dspy-multi-role!\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 3.5\ufe0f\u20e3 Montar Google Drive (IMPORTANTE - Evita p\u00e9rdida de datos)\n\n**Motivo**: Si Colab se desconecta, todo en `/content/` se borra. Guardando en Drive, el modelo sobrevive.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Montar Google Drive para guardar el modelo autom\u00e1ticamente\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Crear directorio para modelos entrenados\n!mkdir -p /content/drive/MyDrive/colab_models\n\nprint(\"\u2705 Google Drive montado exitosamente!\")\nprint(\"\ud83d\udcc1 Ubicaci\u00f3n de backup: /content/drive/MyDrive/colab_models/\")\nprint(\"\\n\u26a0\ufe0f IMPORTANTE: El modelo se guardar\u00e1 DIRECTAMENTE en Drive\")\nprint(\"   Si Colab se desconecta, tus checkpoints estar\u00e1n seguros en Drive\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\ufe0f\u20e3 Verificar Dataset y Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\ud83d\udd0d Verificando archivos necesarios...\\n\")\n",
    "\n",
    "# Paths\n",
    "script_path = Path(\"/content/agnostic-ai-pipeline/scripts/train_po_lora.py\")\n",
    "dataset_path = Path(\"/content/agnostic-ai-pipeline/artifacts/distillation/po_teacher_supervised.jsonl\")\n",
    "\n",
    "# Verificar script\n",
    "if script_path.exists():\n",
    "    print(f\"\u2705 Script encontrado: {script_path}\")\n",
    "else:\n",
    "    print(f\"\u274c Script NO encontrado: {script_path}\")\n",
    "\n",
    "# Verificar dataset\n",
    "if dataset_path.exists():\n",
    "    print(f\"\u2705 Dataset encontrado: {dataset_path}\")\n",
    "    \n",
    "    # Contar registros\n",
    "    with open(dataset_path) as f:\n",
    "        samples = [line for line in f if line.strip()]\n",
    "        print(f\"\u2705 Dataset tiene {len(samples)} registros\")\n",
    "    \n",
    "    # Ver primer sample\n",
    "    with open(dataset_path) as f:\n",
    "        first = json.loads(f.readline())\n",
    "        print(f\"\\n\ud83d\udcdd Keys: {list(first.keys())}\")\n",
    "        print(f\"\ud83d\udccf Prompt length: {len(first['prompt'])} chars\")\n",
    "        print(f\"\ud83d\udccf Response length: {len(first['response'])} chars\")\n",
    "        print(f\"\\n\ud83d\udd0d Preview prompt:\")\n",
    "        print(first['prompt'][:300] + \"...\")\n",
    "        print(f\"\\n\u2705 Dataset format is correct!\")\n",
    "else:\n",
    "    print(f\"\u274c Dataset NO encontrado: {dataset_path}\")\n",
    "    print(f\"\\n\ud83d\udd0d Buscando dataset en el repo...\")\n",
    "    !find /content/agnostic-ai-pipeline -name \"po_teacher_supervised.jsonl\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Ejecutar training con BACKUP AUTOM\u00c1TICO a Google Drive\n# \u26a0\ufe0f IMPORTANTE: Este comando guarda el modelo directamente en Drive\n# Si Colab se desconecta, tus checkpoints estar\u00e1n seguros\n\n!python scripts/train_po_lora.py \\\n    --data-path /content/agnostic-ai-pipeline/artifacts/distillation/po_teacher_supervised.jsonl \\\n    --base-model Qwen/Qwen2.5-7B-Instruct \\\n    --output-dir /content/drive/MyDrive/colab_models/po_student_v1 \\\n    --rank 32 \\\n    --alpha 64 \\\n    --dropout 0.05 \\\n    --epochs 3 \\\n    --batch-size 1 \\\n    --gradient-accumulation-steps 8 \\\n    --lr 1e-4 \\\n    --max-length 2048 \\\n    --load-4bit \\\n    --bnb-compute-dtype float16 \\\n    --gradient-checkpointing\n\n# Copiar tambi\u00e9n a /content por si quieres usar el modelo inmediatamente\nprint(\"\\n\ud83d\udccb Copiando modelo a /content para acceso r\u00e1pido...\")\n!cp -r /content/drive/MyDrive/colab_models/po_student_v1 /content/agnostic-ai-pipeline/artifacts/models/ 2>/dev/null || true\nprint(\"\u2705 Training completado! Modelo guardado en Drive y /content\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar training\n",
    "!python scripts/train_po_lora.py \\\n",
    "    --data-path /content/agnostic-ai-pipeline/artifacts/distillation/po_teacher_supervised.jsonl \\\n",
    "    --base-model Qwen/Qwen2.5-7B-Instruct \\\n",
    "    --output-dir /content/agnostic-ai-pipeline/artifacts/models/po_student_v1 \\\n",
    "    --rank 32 \\\n",
    "    --alpha 64 \\\n",
    "    --dropout 0.05 \\\n",
    "    --epochs 3 \\\n",
    "    --batch-size 1 \\\n",
    "    --gradient-accumulation-steps 8 \\\n",
    "    --lr 1e-4 \\\n",
    "    --max-length 2048 \\\n",
    "    --load-4bit \\\n",
    "    --bnb-compute-dtype float16 \\\n",
    "    --gradient-checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Comando ALTERNATIVO con par\u00e1metros m\u00e1s conservadores\n# Descomenta y ejecuta si el comando anterior fall\u00f3 con OOM\n\n# !python scripts/train_po_lora.py \\\n#     --data-path /content/agnostic-ai-pipeline/artifacts/distillation/po_teacher_supervised.jsonl \\\n#     --base-model Qwen/Qwen2.5-7B-Instruct \\\n#     --output-dir /content/drive/MyDrive/colab_models/po_student_v1 \\\n#     --rank 16 \\\n#     --alpha 32 \\\n#     --dropout 0.05 \\\n#     --epochs 3 \\\n#     --batch-size 1 \\\n#     --gradient-accumulation-steps 16 \\\n#     --lr 1e-4 \\\n#     --max-length 1536 \\\n#     --load-4bit \\\n#     --bnb-compute-dtype float16 \\\n#     --gradient-checkpointing\n\n# # Copiar tambi\u00e9n a /content\n# !cp -r /content/drive/MyDrive/colab_models/po_student_v1 /content/agnostic-ai-pipeline/artifacts/models/ 2>/dev/null || true"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comando ALTERNATIVO con par\u00e1metros m\u00e1s conservadores\n",
    "# Descomenta y ejecuta si el comando anterior fall\u00f3 con OOM\n",
    "\n",
    "# !python scripts/train_po_lora.py \\\n",
    "#     --data-path /content/agnostic-ai-pipeline/artifacts/distillation/po_teacher_supervised.jsonl \\\n",
    "#     --base-model Qwen/Qwen2.5-7B-Instruct \\\n",
    "#     --output-dir /content/agnostic-ai-pipeline/artifacts/models/po_student_v1 \\\n",
    "#     --rank 16 \\\n",
    "#     --alpha 32 \\\n",
    "#     --dropout 0.05 \\\n",
    "#     --epochs 3 \\\n",
    "#     --batch-size 1 \\\n",
    "#     --gradient-accumulation-steps 16 \\\n",
    "#     --lr 1e-4 \\\n",
    "#     --max-length 1536 \\\n",
    "#     --load-4bit \\\n",
    "#     --bnb-compute-dtype float16 \\\n",
    "#     --gradient-checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\ufe0f\u20e3 Monitorear GPU (Ejecutar en otra celda mientras entrena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver uso de GPU en tiempo real (actualiza cada 5 segundos)\n",
    "!nvidia-smi --query-gpu=utilization.gpu,utilization.memory,memory.used,memory.total --format=csv -l 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Ver checkpoints guardados despu\u00e9s del training\nprint(\"\ud83d\udcc1 Checkpoints guardados:\\n\")\n\n# Ubicaci\u00f3n principal en Drive (persistente)\nprint(\"\ud83d\udd10 DRIVE (Ubicaci\u00f3n principal - sobrevive desconexiones):\")\n!ls -lh /content/drive/MyDrive/colab_models/po_student_v1/ 2>/dev/null || echo \"   \u26a0\ufe0f No encontrado en Drive\"\n\n# Ubicaci\u00f3n temporal en /content (solo si se copi\u00f3)\nprint(\"\\n\ud83d\udcbe /content (Ubicaci\u00f3n temporal):\")\n!ls -lh /content/agnostic-ai-pipeline/artifacts/models/po_student_v1/ 2>/dev/null || echo \"   \u26a0\ufe0f No encontrado en /content\"\n\nprint(\"\\n\u2705 Tu modelo est\u00e1 guardado en Drive de forma permanente\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver checkpoints guardados despu\u00e9s del training\n",
    "print(\"\ud83d\udcc1 Checkpoints guardados:\\n\")\n",
    "!ls -lh /content/agnostic-ai-pipeline/artifacts/models/po_student_v1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8\ufe0f\u20e3 Test de Inferencia R\u00e1pido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "print(\"\ud83d\udd04 Loading model...\\n\")\n",
    "\n",
    "base_model = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "adapter_path = \"/content/agnostic-ai-pipeline/artifacts/models/po_student_v1\"\n",
    "\n",
    "# Cargar tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Cargar modelo base en 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Cargar adapter LoRA\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "\n",
    "print(\"\u2705 Model loaded!\\n\")\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"\"\"[INSTRUCTIONS]\n",
    "You are a Product Owner AI agent validating business requirements.\n",
    "\n",
    "[REQUIREMENTS]\n",
    "business_domain: Blog Platform\n",
    "primary_features:\n",
    "  - Users can create blog posts with title and content\n",
    "  - Posts must support markdown formatting\n",
    "  - User authentication is required\n",
    "\n",
    "[YOUR RESPONSE]\"\"\"\n",
    "\n",
    "print(\"\ud83d\udcad Test prompt:\")\n",
    "print(test_prompt)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Generate\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\ud83e\udd16 Model response:\")\n",
    "print(response[len(test_prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9\ufe0f\u20e3 Evaluar baseline vs student (`scripts/eval_po_student.py`)\n",
    "\n",
    "Estas celdas ejecutan el nuevo evaluador con el mismo prompt supervisado, generan \u226520 ejemplos y guardan los resultados en Google Drive (`/content/drive/MyDrive/colab_models/inference_results`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "repo_dir = \"/content/agnostic-ai-pipeline\"\n",
    "output_dir = \"/content/drive/MyDrive/colab_models/inference_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"PYTHONPATH\"] = repo_dir\n",
    "\n",
    "cmd = [\n",
    "    \"python\",\n",
    "    \"scripts/eval_po_student.py\",\n",
    "    \"--dataset-path\", \"artifacts/synthetic/product_owner/product_owner_val.jsonl\",\n",
    "    \"--output-dir\", output_dir,\n",
    "    \"--tag\", \"baseline\",\n",
    "    \"--base-model\", \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"--max-samples\", \"20\",\n",
    "    \"--temperature\", \"0.2\",\n",
    "    \"--top-p\", \"0.9\",\n",
    "    \"--max-new-tokens\", \"900\",\n",
    "    \"--retries\", \"1\",\n",
    "    \"--load-4bit\",\n",
    "    \"--bnb-compute-dtype\", \"float16\"\n",
    "]\n",
    "\n",
    "print(\"\n",
    "\ud83d\ude80 Running baseline evaluation...\n",
    "\")\n",
    "subprocess.run(cmd, cwd=repo_dir, check=True, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "repo_dir = \"/content/agnostic-ai-pipeline\"\n",
    "output_dir = \"/content/drive/MyDrive/colab_models/inference_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"PYTHONPATH\"] = repo_dir\n",
    "\n",
    "cmd = [\n",
    "    \"python\",\n",
    "    \"scripts/eval_po_student.py\",\n",
    "    \"--dataset-path\", \"artifacts/synthetic/product_owner/product_owner_val.jsonl\",\n",
    "    \"--output-dir\", output_dir,\n",
    "    \"--tag\", \"student\",\n",
    "    \"--base-model\", \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"--adapter-path\", \"artifacts/models/po_student_v1\",\n",
    "    \"--max-samples\", \"20\",\n",
    "    \"--temperature\", \"0.2\",\n",
    "    \"--top-p\", \"0.9\",\n",
    "    \"--max-new-tokens\", \"900\",\n",
    "    \"--retries\", \"1\",\n",
    "    \"--load-4bit\",\n",
    "    \"--bnb-compute-dtype\", \"float16\"\n",
    "]\n",
    "\n",
    "print(\"\n",
    "\ud83d\ude80 Running student evaluation...\n",
    "\")\n",
    "subprocess.run(cmd, cwd=repo_dir, check=True, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "results_dir = Path(\"/content/drive/MyDrive/colab_models/inference_results\")\n",
    "print(f\"\ud83d\udcc1 Resultados guardados en: {results_dir}\")\n",
    "for path in sorted(results_dir.glob(\"*.json\")):\n",
    "    data = json.loads(path.read_text())\n",
    "    metrics = data.get(\"metrics\", {})\n",
    "    mean = metrics.get(\"mean\")\n",
    "    status = data.get(\"valid_samples\", 0)\n",
    "    print(f\"- {path.name}: valid={status} mean={mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd3d Descargar resultados de inferencia (opcional)\n",
    "\n",
    "Compr\u00edmelos desde Drive para subirlos al repo o compartirlos."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from google.colab import files\n",
    "\n",
    "results_root = \"/content/drive/MyDrive/colab_models\"\n",
    "results_dir = os.path.join(results_root, \"inference_results\")\n",
    "zip_path = os.path.join(results_root, \"inference_results.zip\")\n",
    "\n",
    "if not os.path.isdir(results_dir):\n",
    "    raise SystemExit(f\"\u26a0\ufe0f No se encontr\u00f3 {results_dir}. Ejecuta antes las celdas de evaluaci\u00f3n.\")\n",
    "\n",
    "print(\"\ud83d\udce6 Empaquetando inference_results en Drive...\")\n",
    "subprocess.run([\"zip\", \"-r\", \"inference_results.zip\", \"inference_results\"], cwd=results_root, check=True)\n",
    "\n",
    "print(\"\u2b07\ufe0f Descargando zip...\")\n",
    "files.download(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "from google.colab import files\n\nprint(\"\ud83d\udce6 Compressing model from Drive...\\n\")\n\n# Comprimir desde Drive (ubicaci\u00f3n permanente)\n!cd /content/drive/MyDrive/colab_models && zip -r po_student_v1.zip po_student_v1\n\nprint(\"\\n\u2b07\ufe0f Downloading...\")\nfiles.download('/content/drive/MyDrive/colab_models/po_student_v1.zip')\n\nprint(\"\\n\u2705 Download complete!\")\nprint(\"\ud83d\udca1 TIP: Puedes acceder al modelo directamente desde Drive en cualquier momento\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd01 Subir resultados al repo (opcional)\n",
    "\n",
    "Si quieres conservar los JSON/zip directamente en tu branch, usa estas celdas. Requieren un PAT o credenciales GitHub v\u00e1lidas."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Configurar credenciales Git (ejecuta una vez por sesi\u00f3n)\n",
    "!git config --global user.name \"Tu Nombre\"\n",
    "!git config --global user.email \"tu.email@example.com\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "repo_dir = Path(\"/content/agnostic-ai-pipeline\")\n",
    "results_dir = Path(\"/content/drive/MyDrive/colab_models/inference_results\")\n",
    "repo_results = repo_dir / \"inference_results\"\n",
    "repo_results.mkdir(exist_ok=True)\n",
    "\n",
    "# Copiar todos los JSON generados en Drive hacia el repo para versionarlos\n",
    "for src in results_dir.glob(\"*.json\"):\n",
    "    dst = repo_results / src.name\n",
    "    print(f\"\ud83d\udcc4 Copiando {src.name} -> {dst}\")\n",
    "    dst.write_text(src.read_text())\n",
    "\n",
    "# Copiar tambi\u00e9n el zip si existe\n",
    "zip_src = results_dir.parent / \"inference_results.zip\"\n",
    "if zip_src.exists():\n",
    "    dst = repo_results / zip_src.name\n",
    "    print(f\"\ud83d\udce6 Copiando {zip_src.name} -> {dst}\")\n",
    "    dst.write_bytes(zip_src.read_bytes())\n",
    "\n",
    "print(\"\u2705 Archivos listos en el repo. Ahora haz commit/push con la celda siguiente.\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "cd /content/agnostic-ai-pipeline\n",
    "\n",
    "# Aseg\u00farate de estar en el branch correcto\n",
    "BRANCH=$(git branch --show-current)\n",
    "echo \"\ud83d\udccc Branch actual: $BRANCH\"\n",
    "\n",
    "git status -sb\n",
    "\n",
    "git add inference_results\n",
    "\n",
    "git commit -m \"chore(po): add latest evaluation artifacts\" || echo \"\u26a0\ufe0f Nada para commitear\"\n",
    "\n",
    "git push origin \"$BRANCH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"\ud83d\udce6 Compressing model...\\n\")\n",
    "!zip -r po_student_v1.zip /content/agnostic-ai-pipeline/artifacts/models/po_student_v1\n",
    "\n",
    "print(\"\\n\u2b07\ufe0f Downloading...\")\n",
    "files.download('po_student_v1.zip')\n",
    "\n",
    "print(\"\\n\u2705 Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udd27 Troubleshooting\n",
    "\n",
    "### Error: CUDA out of memory\n",
    "**Soluci\u00f3n**: Ejecuta la celda alternativa (Cell 5.2) con par\u00e1metros m\u00e1s conservadores\n",
    "\n",
    "### Error: No module named 'bitsandbytes'\n",
    "**Soluci\u00f3n**: Reinstala bitsandbytes\n",
    "```python\n",
    "!pip uninstall -y bitsandbytes\n",
    "!pip install bitsandbytes==0.41.0 --no-cache-dir\n",
    "```\n",
    "\n",
    "### Error: No GPU detected\n",
    "**Soluci\u00f3n**: \n",
    "1. Runtime \u2192 Change runtime type\n",
    "2. Hardware accelerator: **T4 GPU**\n",
    "3. Save y reconectar\n",
    "\n",
    "### Training muy lento (< 1 it/s)\n",
    "**Soluci\u00f3n**: Usa la celda alternativa con `--max-length 1536` o `--gradient-accumulation-steps 4`\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcca M\u00e9tricas Esperadas\n",
    "\n",
    "```\n",
    "Epoch 1/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [12:34<00:00,  0.05it/s]\n",
    "{'loss': 1.2345, 'learning_rate': 9.5e-05, 'epoch': 1.0}\n",
    "\n",
    "Epoch 2/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [12:31<00:00,  0.05it/s]\n",
    "{'loss': 0.8912, 'learning_rate': 5.0e-05, 'epoch': 2.0}\n",
    "\n",
    "Epoch 3/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [12:29<00:00,  0.05it/s]\n",
    "{'loss': 0.6234, 'learning_rate': 5.0e-06, 'epoch': 3.0}\n",
    "```\n",
    "\n",
    "**Indicadores de \u00e9xito**:\n",
    "- \u2705 Loss decreciente (1.2 \u2192 0.6)\n",
    "- \u2705 No crashes por OOM\n",
    "- \u2705 ~0.03-0.05 it/s en T4\n",
    "- \u2705 Checkpoints guardados cada epoch\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcda Referencias\n",
    "\n",
    "- **Setup Guide**: `docs/COLAB_TRAINING_SETUP.md`\n",
    "- **Training Script**: `scripts/train_po_lora.py`\n",
    "- **HuggingFace Qwen2.5**: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct\n",
    "- **PEFT Docs**: https://huggingface.co/docs/peft/\n",
    "\n",
    "---\n",
    "\n",
    "**\u00daltima actualizaci\u00f3n**: 2025-11-13"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}