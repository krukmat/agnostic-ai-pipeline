# Fase 9: Multi-Role DSPy MIPROv2 Optimization - Plan Detallado

**Fecha Inicio**: 2025-11-09
**Branch**: `dspy-multi-role`
**Objetivo**: Extender optimizaci√≥n DSPy MIPROv2 a roles Architect, Developer y QA
**Precedente**: Fase 8 - BA optimizado con 85.35% score (+13.35% vs baseline 72%)

---

## üìã Resumen Ejecutivo

### Contexto

Fase 8 demostr√≥ que DSPy MIPROv2 es **extremadamente efectivo** para optimizaci√≥n de roles:
- **Tiempo**: 3 horas vs 200+ horas de fine-tuning
- **Score**: 85.35% (mejora de +13.35% vs baseline)
- **Costo**: $0 (100% local con Ollama)
- **Iterabilidad**: Alta (cambios en segundos)

**Decisi√≥n**: Extender este enfoque exitoso a los 3 roles restantes del pipeline.

### Objetivos Fase 9

1. **Architect**: Optimizar generaci√≥n de historias t√©cnicas (epics ‚Üí stories)
2. **Developer**: Optimizar generaci√≥n de c√≥digo + tests
3. **QA**: Optimizar generaci√≥n de reportes de calidad

**Meta Global**: Pipeline completo con 4/4 roles optimizados, manteniendo 100% local + $0 costo.

---

## üéØ Objetivos por Rol

### 9.1 - Architect Role Optimization

**Input**: Requirements YAML (desde BA)
**Output**: `stories.yaml`, `architecture.yaml`, `epics.yaml`

**Complejidad**: ‚≠ê‚≠ê‚≠ê‚≠ê (Alta - decisiones arquitect√≥nicas complejas)

**Baseline Esperado**: ~60-65%
**Target Optimizado**: ~80-85%
**Mejora Esperada**: +20-25%

**M√©tricas Clave**:
- Story completeness (fields: id, title, description, acceptance_criteria, dependencies)
- Story granularity (no demasiado grandes ni peque√±as)
- Architecture validity (componentes, tech stack, patrones)
- Epic coherence (agrupaci√≥n l√≥gica de stories)

**Desaf√≠os**:
- Output multi-archivo (stories.yaml, architecture.yaml, epics.yaml)
- Dependencias entre stories (orden de implementaci√≥n)
- Trade-offs arquitect√≥nicos (simplicidad vs escalabilidad)

---

### 9.2 - Developer Role Optimization

**Input**: Single story (YAML) + architecture context
**Output**: C√≥digo fuente + tests

**Complejidad**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Muy Alta - c√≥digo ejecutable + tests)

**Baseline Esperado**: ~55-60%
**Target Optimizado**: ~75-80%
**Mejora Esperada**: +20-25%

**M√©tricas Clave**:
- Code syntax correctness (parseable, lintable)
- Test coverage (‚â•80% l√≠neas cubiertas)
- Story alignment (implementa acceptance criteria)
- Code quality (no duplicaci√≥n, patrones adecuados)
- Test execution (tests pasan en CI)

**Desaf√≠os**:
- Output complejo (m√∫ltiples archivos de c√≥digo + tests)
- Sintaxis correcta en m√∫ltiples lenguajes
- Tests que realmente validan funcionalidad
- Integraci√≥n con c√≥digo existente

---

### 9.3 - QA Role Optimization

**Input**: C√≥digo implementado + tests + story
**Output**: `qa_report.yaml` (defects, test_summary, recommendations)

**Complejidad**: ‚≠ê‚≠ê‚≠ê (Media-Alta - an√°lisis de calidad)

**Baseline Esperado**: ~65-70%
**Target Optimizado**: ~85-90%
**Mejora Esperada**: +20-25%

**M√©tricas Clave**:
- Defect detection rate (encuentra bugs reales)
- False positive rate (no reporta no-bugs como bugs)
- Test execution summary accuracy
- Recommendation quality (accionables)
- Report completeness (fields requeridos)

**Desaf√≠os**:
- Requiere ejecutar tests reales (no solo an√°lisis est√°tico)
- Balance entre exhaustividad y practicidad
- Variedad de tipos de defectos (funcionales, performance, seguridad)

---

## üìä Estructura General del Plan

### Fases por Rol (Secuencial)

Cada rol sigue el mismo pipeline probado en Fase 8:

```
1. Dataset Preparation (1-2 d√≠as)
   ‚îú‚îÄ‚îÄ 1.1. Generate synthetic concepts
   ‚îú‚îÄ‚îÄ 1.2. Generate outputs from baseline model
   ‚îú‚îÄ‚îÄ 1.3. Filter by quality (score ‚â• baseline threshold)
   ‚îî‚îÄ‚îÄ 1.4. Train/val split (80/20)

2. Baseline Evaluation (0.5 d√≠as)
   ‚îú‚îÄ‚îÄ 2.1. Run baseline model on validation set
   ‚îú‚îÄ‚îÄ 2.2. Calculate metrics
   ‚îî‚îÄ‚îÄ 2.3. Document baseline score

3. MIPROv2 Optimization (0.5-1 d√≠a)
   ‚îú‚îÄ‚îÄ 3.1. Configure optimization parameters
   ‚îú‚îÄ‚îÄ 3.2. Run MIPROv2 (bootstrapping + instruction optimization)
   ‚îú‚îÄ‚îÄ 3.3. Monitor progress
   ‚îî‚îÄ‚îÄ 3.4. Save optimized program

4. Evaluation & Analysis (0.5 d√≠as)
   ‚îú‚îÄ‚îÄ 4.1. Run optimized model on validation set
   ‚îú‚îÄ‚îÄ 4.2. Compare vs baseline
   ‚îú‚îÄ‚îÄ 4.3. Analyze improvements
   ‚îî‚îÄ‚îÄ 4.4. Document results

5. Integration (0.5 d√≠as)
   ‚îú‚îÄ‚îÄ 5.1. Update pipeline to use optimized model
   ‚îú‚îÄ‚îÄ 5.2. Run end-to-end test
   ‚îî‚îÄ‚îÄ 5.3. Commit changes
```

**Total por rol**: ~3-4 d√≠as
**Total Fase 9**: ~9-12 d√≠as (secuencial) o ~5-6 d√≠as (paralelo)

---

## üìÅ Estructura de Artefactos

### Datasets (por rol)

```
artifacts/synthetic/
‚îú‚îÄ‚îÄ architect/
‚îÇ   ‚îú‚îÄ‚îÄ concepts.jsonl                    # Input concepts para arquitecturas
‚îÇ   ‚îú‚îÄ‚îÄ architect_synthetic_raw.jsonl     # 200+ ejemplos sin filtrar
‚îÇ   ‚îú‚îÄ‚îÄ architect_synthetic_filtered.jsonl # 100-120 ejemplos filtrados
‚îÇ   ‚îú‚îÄ‚îÄ architect_train.jsonl             # 80-96 ejemplos (80%)
‚îÇ   ‚îî‚îÄ‚îÄ architect_val.jsonl               # 20-24 ejemplos (20%)
‚îú‚îÄ‚îÄ developer/
‚îÇ   ‚îú‚îÄ‚îÄ stories.jsonl                     # Stories para implementar
‚îÇ   ‚îú‚îÄ‚îÄ developer_synthetic_raw.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ developer_synthetic_filtered.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ developer_train.jsonl
‚îÇ   ‚îî‚îÄ‚îÄ developer_val.jsonl
‚îî‚îÄ‚îÄ qa/
    ‚îú‚îÄ‚îÄ implementations.jsonl             # C√≥digo + tests para evaluar
    ‚îú‚îÄ‚îÄ qa_synthetic_raw.jsonl
    ‚îú‚îÄ‚îÄ qa_synthetic_filtered.jsonl
    ‚îú‚îÄ‚îÄ qa_train.jsonl
    ‚îî‚îÄ‚îÄ qa_val.jsonl
```

### Modelos Optimizados

```
artifacts/dspy/
‚îú‚îÄ‚îÄ architect_optimized/
‚îÇ   ‚îú‚îÄ‚îÄ program.pkl                       # Programa DSPy compilado
‚îÇ   ‚îú‚îÄ‚îÄ config.json                       # Configuraci√≥n usada
‚îÇ   ‚îî‚îÄ‚îÄ evaluation_report.json           # Resultados vs baseline
‚îú‚îÄ‚îÄ developer_optimized/
‚îÇ   ‚îú‚îÄ‚îÄ program.pkl
‚îÇ   ‚îú‚îÄ‚îÄ config.json
‚îÇ   ‚îî‚îÄ‚îÄ evaluation_report.json
‚îî‚îÄ‚îÄ qa_optimized/
    ‚îú‚îÄ‚îÄ program.pkl
    ‚îú‚îÄ‚îÄ config.json
    ‚îî‚îÄ‚îÄ evaluation_report.json
```

### Documentaci√≥n

```
docs/
‚îú‚îÄ‚îÄ fase9_multi_role_dspy_plan.md         # Este documento (plan maestro)
‚îú‚îÄ‚îÄ fase9_architect_optimization.md       # Detalles espec√≠ficos Architect
‚îú‚îÄ‚îÄ fase9_developer_optimization.md       # Detalles espec√≠ficos Developer
‚îú‚îÄ‚îÄ fase9_qa_optimization.md              # Detalles espec√≠ficos QA
‚îî‚îÄ‚îÄ fase9_final_report.md                 # Resultados finales y comparaci√≥n
```

---

## üîß Scripts y Herramientas

### Scripts Existentes (reutilizables de Fase 8)

1. **`scripts/tune_dspy.py`** ‚úÖ
   - Ya soporta m√∫ltiples roles (par√°metro `--role`)
   - MIPROv2 optimization
   - M√©tricas customizables

2. **`scripts/generate_synthetic_dataset.py`** ‚ö†Ô∏è
   - Requiere adaptaci√≥n por rol (diferentes outputs)
   - Architect: genera stories.yaml + architecture.yaml
   - Developer: genera c√≥digo + tests
   - QA: genera qa_report.yaml

3. **`scripts/filter_synthetic_data.py`** ‚ö†Ô∏è
   - Requiere m√©tricas espec√≠ficas por rol
   - Architect: metric `architect_stories_metric`
   - Developer: metric `developer_code_metric`
   - QA: metric `qa_report_metric`

4. **`scripts/split_dataset.py`** ‚úÖ
   - Gen√©rico, funciona para todos los roles

### Scripts Nuevos a Crear

1. **`scripts/generate_architect_concepts.py`**
   - Similar a `generate_business_concepts.py`
   - Genera requirements sint√©ticos como input para Architect

2. **`scripts/generate_developer_stories.py`**
   - Genera stories sint√©ticas como input para Developer
   - Incluye architecture context

3. **`scripts/generate_qa_implementations.py`**
   - Genera c√≥digo + tests sint√©ticos como input para QA
   - Incluye story context

4. **`dspy_baseline/metrics.py`** (extender)
   - `architect_stories_metric(gold, pred, trace=None)`
   - `developer_code_metric(gold, pred, trace=None)`
   - `qa_report_metric(gold, pred, trace=None)`

---

## üìù Tareas Detalladas - Fase 9.1: Architect

### 9.1.1 - An√°lisis de Output Architect Actual

**Objetivo**: Entender formato actual y definir estructura del dataset.

**Tareas**:
1. Revisar `planning/stories.yaml` generados por Architect actual
2. Revisar `planning/architecture.yaml` generados
3. Identificar campos clave para m√©tricas
4. Documentar schema esperado

**Criterios de Aceptaci√≥n**:
- Schema documentado en `docs/fase9_architect_schema.md`
- Ejemplos de outputs "gold standard" identificados

**Tiempo Estimado**: 0.5 d√≠as

---

### 9.1.2 - Dise√±o de M√©trica Architect

**Objetivo**: Definir m√©trica `architect_stories_metric` para evaluar calidad.

**Componentes de M√©trica**:
1. **Story Completeness** (30 pts)
   - Todos los campos requeridos presentes
   - IDs √∫nicos y secuenciales
   - Descriptions no vac√≠as

2. **Story Quality** (25 pts)
   - Acceptance criteria espec√≠ficos y verificables
   - Titles descriptivos y concisos
   - Estimates razonables

3. **Architecture Validity** (25 pts)
   - Componentes definidos correctamente
   - Tech stack coherente
   - Patrones apropiados al problema

4. **Dependency Correctness** (20 pts)
   - Dependencies apuntan a stories existentes
   - No ciclos en grafo de dependencias
   - Orden de implementaci√≥n viable

**Score Total**: 100 pts (normalizar a 0-1)

**Tareas**:
1. Implementar `architect_stories_metric()` en `dspy_baseline/metrics.py`
2. Crear tests unitarios para la m√©trica
3. Validar con ejemplos reales

**Criterios de Aceptaci√≥n**:
- M√©trica implementada y testeada
- Score coherente con juicio humano (muestreo 10 ejemplos)

**Tiempo Estimado**: 1 d√≠a

---

### 9.1.3 - Generaci√≥n de Conceptos Sint√©ticos (Architect Input)

**Objetivo**: Generar 200+ requirements sint√©ticos como input para Architect.

**Estrategia**:
- Reutilizar `artifacts/synthetic/ba_train_v2_fixed.jsonl` (98 ejemplos)
- Generar 102 ejemplos adicionales con `mistral:7b-instruct`
- Total: 200 examples

**Tareas**:
1. Crear `scripts/generate_architect_concepts.py`
2. Usar BA outputs existentes como seed
3. Generar variaciones sint√©ticas (diferentes dominios)
4. Guardar en `artifacts/synthetic/architect/concepts.jsonl`

**Criterios de Aceptaci√≥n**:
- 200 requirements YAML sint√©ticos generados
- Diversidad de dominios (web, mobile, data, ML, etc.)

**Tiempo Estimado**: 0.5 d√≠as

---

### 9.1.4 - Generaci√≥n de Dataset Sint√©tico Architect

**Objetivo**: Generar outputs de Architect (stories.yaml) para 200 inputs.

**Proceso**:
1. Ejecutar Architect baseline sobre 200 concepts
2. Generar `stories.yaml` + `architecture.yaml` para cada uno
3. Guardar en `artifacts/synthetic/architect/architect_synthetic_raw.jsonl`

**Formato JSONL**:
```json
{
  "input": {
    "requirements": "..."  // YAML string
  },
  "output": {
    "stories": "...",      // YAML string
    "architecture": "...", // YAML string
    "epics": "..."         // YAML string (opcional)
  },
  "metadata": {
    "concept_id": "architect_001",
    "generated_at": "2025-11-09T...",
    "model": "mistral:7b-instruct"
  }
}
```

**Tareas**:
1. Adaptar `scripts/generate_synthetic_dataset.py` para Architect
2. Ejecutar generaci√≥n (ETA: ~1-2 horas con Ollama)
3. Validar outputs (YAML v√°lido, campos presentes)

**Criterios de Aceptaci√≥n**:
- 200 ejemplos generados
- 100% con YAML v√°lido
- Guardado en `architect_synthetic_raw.jsonl`

**Tiempo Estimado**: 0.5 d√≠as

---

### 9.1.5 - Filtrado de Dataset por Score

**Objetivo**: Filtrar ejemplos con score ‚â• 0.60 (baseline threshold).

**Proceso**:
1. Calcular `architect_stories_metric` para cada ejemplo
2. Filtrar ejemplos con score ‚â• 0.60
3. Guardar en `architect_synthetic_filtered.jsonl`
4. Objetivo: 100-120 ejemplos de calidad

**Tareas**:
1. Adaptar `scripts/filter_synthetic_data.py` para Architect
2. Ejecutar filtrado
3. Generar reporte de distribuci√≥n de scores

**Criterios de Aceptaci√≥n**:
- 100-120 ejemplos con score ‚â• 0.60
- Reporte JSON con estad√≠sticas

**Tiempo Estimado**: 0.25 d√≠as

---

### 9.1.6 - Train/Val Split

**Objetivo**: Dividir dataset en 80% train / 20% val.

**Resultado**:
- `architect_train.jsonl`: 80-96 ejemplos
- `architect_val.jsonl`: 20-24 ejemplos

**Tareas**:
1. Ejecutar `scripts/split_dataset.py` con seed fijo
2. Verificar distribuci√≥n balanceada

**Criterios de Aceptaci√≥n**:
- Split 80/20 exacto
- Ambos sets tienen diversidad de dominios

**Tiempo Estimado**: 0.1 d√≠as

---

### 9.1.7 - Baseline Evaluation

**Objetivo**: Establecer baseline score de Architect sin optimizaci√≥n.

**Proceso**:
1. Ejecutar Architect baseline sobre validation set
2. Calcular `architect_stories_metric` promedio
3. Documentar baseline score

**Expected Baseline**: ~60-65%

**Tareas**:
1. Ejecutar benchmark con `mistral:7b-instruct`
2. Calcular m√©tricas
3. Guardar resultados en `artifacts/benchmarks/architect_baseline.json`

**Criterios de Aceptaci√≥n**:
- Baseline score documentado
- Benchmark repetible (script + seed)

**Tiempo Estimado**: 0.25 d√≠as

---

### 9.1.8 - MIPROv2 Optimization

**Objetivo**: Optimizar Architect con DSPy MIPROv2.

**Configuraci√≥n**:
- Provider: `ollama`
- Model: `mistral:7b-instruct`
- Num candidates: 4-8
- Num trials: 4-10
- Max bootstrapped demos: 4-6
- Seed: 0 (reproducibilidad)

**Comando**:
```bash
PYTHONPATH=. .venv/bin/python scripts/tune_dspy.py \
  --role architect \
  --trainset artifacts/synthetic/architect/architect_train.jsonl \
  --metric dspy_baseline.metrics:architect_stories_metric \
  --num-candidates 8 \
  --num-trials 10 \
  --max-bootstrapped-demos 6 \
  --seed 0 \
  --output artifacts/dspy/architect_optimized \
  --provider ollama \
  --model mistral:7b-instruct \
  2>&1 | tee /tmp/mipro_architect.log
```

**Tiempo Esperado**: 1-2 horas (similar a BA)

**Tareas**:
1. Configurar DSPy program para Architect
2. Ejecutar MIPROv2 optimization
3. Monitorear progreso (`tail -f /tmp/mipro_architect.log`)
4. Guardar programa optimizado

**Criterios de Aceptaci√≥n**:
- Optimizaci√≥n completa sin errores
- Programa optimizado guardado en `artifacts/dspy/architect_optimized/program.pkl`
- Log completo en `/tmp/mipro_architect.log`

**Tiempo Estimado**: 0.5 d√≠as (incluyendo setup y monitoreo)

---

### 9.1.9 - Evaluation & Comparison

**Objetivo**: Comparar modelo optimizado vs baseline.

**M√©tricas a Comparar**:
- Score promedio (validation set)
- Desviaci√≥n est√°ndar
- Mejora absoluta y relativa
- Por componente (completeness, quality, architecture, dependencies)

**Expected Results**:
- Baseline: 60-65%
- Optimized: 80-85%
- Mejora: +20-25%

**Tareas**:
1. Ejecutar modelo optimizado sobre validation set
2. Calcular m√©tricas
3. Generar reporte comparativo
4. Crear visualizaciones (gr√°ficos)

**Criterios de Aceptaci√≥n**:
- Reporte JSON completo
- Markdown summary
- Mejora ‚â• +15% (m√≠nimo aceptable)

**Tiempo Estimado**: 0.5 d√≠as

---

### 9.1.10 - Integration & Testing

**Objetivo**: Integrar modelo optimizado en pipeline.

**Cambios Requeridos**:
1. Actualizar `scripts/run_architect.py`:
   - Cargar programa DSPy optimizado si existe
   - Fallback a modelo base si no

2. Actualizar `config.yaml`:
   - Agregar flag `use_dspy_optimized: true` para Architect

3. Testing end-to-end:
   - Ejecutar `make ba CONCEPT="Test"` ‚Üí `make plan`
   - Verificar que stories generados tienen alta calidad

**Tareas**:
1. Modificar `scripts/run_architect.py`
2. Crear tests de integraci√≥n
3. Ejecutar full pipeline test
4. Documentar cambios

**Criterios de Aceptaci√≥n**:
- Pipeline funciona end-to-end
- Architect usa modelo optimizado
- Calidad de outputs mejorada visiblemente

**Tiempo Estimado**: 0.5 d√≠as

---

## üìù Tareas Detalladas - Fase 9.2: Developer

### 9.2.1 - An√°lisis de Output Developer Actual

**Objetivo**: Entender formato actual de c√≥digo generado.

**Tareas**:
1. Revisar archivos generados en `project/backend-fastapi/`
2. Analizar estructura de tests
3. Identificar patrones de c√≥digo
4. Documentar schema esperado

**Criterios de Aceptaci√≥n**:
- Schema documentado en `docs/fase9_developer_schema.md`
- Ejemplos de c√≥digo "gold standard" identificados

**Tiempo Estimado**: 0.5 d√≠as

---

### 9.2.2 - Dise√±o de M√©trica Developer

**Objetivo**: Definir m√©trica `developer_code_metric` para evaluar c√≥digo generado.

**Componentes de M√©trica**:
1. **Syntax Correctness** (25 pts)
   - C√≥digo parseable (AST v√°lido)
   - Sin errores de sintaxis
   - Imports resueltos

2. **Test Completeness** (25 pts)
   - Tests presentes (‚â•1 test por funci√≥n)
   - Coverage ‚â•80%
   - Assertions significativas

3. **Story Alignment** (25 pts)
   - Implementa acceptance criteria
   - Nombres de funciones/clases alineados con story
   - L√≥gica coherente con descripci√≥n

4. **Code Quality** (25 pts)
   - No duplicaci√≥n excesiva
   - Funciones con single responsibility
   - Documentaci√≥n (docstrings)
   - Linting (PEP8, etc.)

**Score Total**: 100 pts (normalizar a 0-1)

**Tareas**:
1. Implementar `developer_code_metric()` en `dspy_baseline/metrics.py`
2. Integrar con tools (ast, coverage.py, pylint)
3. Crear tests unitarios

**Criterios de Aceptaci√≥n**:
- M√©trica implementada y testeada
- Validaci√≥n con ejemplos reales

**Tiempo Estimado**: 1.5 d√≠as (m√°s compleja que otras m√©tricas)

---

### 9.2.3 - Generaci√≥n de Stories Sint√©ticas (Developer Input)

**Objetivo**: Generar 200+ stories sint√©ticas como input para Developer.

**Estrategia**:
- Usar outputs de Architect (stories.yaml) como seed
- Generar variaciones sint√©ticas
- Incluir architecture context

**Tareas**:
1. Crear `scripts/generate_developer_stories.py`
2. Generar 200 stories diversas
3. Guardar en `artifacts/synthetic/developer/stories.jsonl`

**Criterios de Aceptaci√≥n**:
- 200 stories con acceptance criteria claros
- Diversidad de tipos (CRUD, business logic, API, UI, etc.)

**Tiempo Estimado**: 0.5 d√≠as

---

### 9.2.4 - Generaci√≥n de Dataset Sint√©tico Developer

**Objetivo**: Generar c√≥digo + tests para 200 stories.

**Proceso**:
1. Ejecutar Developer baseline sobre 200 stories
2. Generar c√≥digo fuente + tests para cada uno
3. Guardar en `developer_synthetic_raw.jsonl`

**Formato JSONL**:
```json
{
  "input": {
    "story": "...",        // YAML string
    "architecture": "..."  // Context
  },
  "output": {
    "code_files": [
      {"path": "main.py", "content": "..."},
      {"path": "test_main.py", "content": "..."}
    ]
  },
  "metadata": {
    "story_id": "dev_001",
    "generated_at": "...",
    "model": "mistral:7b-instruct"
  }
}
```

**Tareas**:
1. Adaptar `scripts/generate_synthetic_dataset.py` para Developer
2. Ejecutar generaci√≥n (ETA: ~2-3 horas)
3. Validar outputs (c√≥digo parseable)

**Criterios de Aceptaci√≥n**:
- 200 ejemplos generados
- ‚â•80% con c√≥digo sint√°cticamente correcto
- Tests presentes en cada ejemplo

**Tiempo Estimado**: 1 d√≠a

---

### 9.2.5 - Filtrado de Dataset por Score

**Objetivo**: Filtrar ejemplos con score ‚â• 0.55.

**Tareas**:
1. Calcular `developer_code_metric` para cada ejemplo
2. Filtrar ejemplos con score ‚â• 0.55
3. Guardar en `developer_synthetic_filtered.jsonl`
4. Objetivo: 100-120 ejemplos

**Criterios de Aceptaci√≥n**:
- 100-120 ejemplos de calidad
- Reporte de distribuci√≥n de scores

**Tiempo Estimado**: 0.5 d√≠as

---

### 9.2.6 - Train/Val Split

**Resultado**:
- `developer_train.jsonl`: 80-96 ejemplos
- `developer_val.jsonl`: 20-24 ejemplos

**Tiempo Estimado**: 0.1 d√≠as

---

### 9.2.7 - Baseline Evaluation

**Expected Baseline**: ~55-60%

**Tiempo Estimado**: 0.25 d√≠as

---

### 9.2.8 - MIPROv2 Optimization

**Configuraci√≥n similar a Architect**

**Tiempo Esperado**: 1-2 horas

**Tiempo Estimado**: 0.5 d√≠as

---

### 9.2.9 - Evaluation & Comparison

**Expected Results**:
- Baseline: 55-60%
- Optimized: 75-80%
- Mejora: +20-25%

**Tiempo Estimado**: 0.5 d√≠as

---

### 9.2.10 - Integration & Testing

**Cambios en**: `scripts/run_dev.py`

**Tiempo Estimado**: 0.5 d√≠as

---

## üìù Tareas Detalladas - Fase 9.3: QA

### 9.3.1 - An√°lisis de Output QA Actual

**Objetivo**: Entender formato actual de `qa_report.yaml`.

**Tiempo Estimado**: 0.5 d√≠as

---

### 9.3.2 - Dise√±o de M√©trica QA

**Componentes de M√©trica**:
1. **Defect Detection Accuracy** (30 pts)
2. **Test Summary Correctness** (25 pts)
3. **Recommendation Quality** (25 pts)
4. **Report Completeness** (20 pts)

**Tiempo Estimado**: 1 d√≠a

---

### 9.3.3 - Generaci√≥n de Implementations Sint√©ticas (QA Input)

**Estrategia**:
- Usar outputs de Developer (c√≥digo + tests) como seed
- Generar variaciones (con y sin bugs)

**Tiempo Estimado**: 0.5 d√≠as

---

### 9.3.4 - Generaci√≥n de Dataset Sint√©tico QA

**Tiempo Estimado**: 0.5 d√≠as

---

### 9.3.5 - Filtrado de Dataset por Score

**Threshold**: ‚â• 0.65

**Tiempo Estimado**: 0.25 d√≠as

---

### 9.3.6 - Train/Val Split

**Tiempo Estimado**: 0.1 d√≠as

---

### 9.3.7 - Baseline Evaluation

**Expected Baseline**: ~65-70%

**Tiempo Estimado**: 0.25 d√≠as

---

### 9.3.8 - MIPROv2 Optimization

**Tiempo Estimado**: 0.5 d√≠as

---

### 9.3.9 - Evaluation & Comparison

**Expected Results**:
- Baseline: 65-70%
- Optimized: 85-90%
- Mejora: +20-25%

**Tiempo Estimado**: 0.5 d√≠as

---

### 9.3.10 - Integration & Testing

**Cambios en**: `scripts/run_qa.py`

**Tiempo Estimado**: 0.5 d√≠as

---

## üìä Resumen de Timeline

### Por Rol (Secuencial)

| Rol | Tareas | Tiempo Estimado | Baseline Esperado | Target Optimizado |
|-----|--------|-----------------|-------------------|-------------------|
| **Architect** | 9.1.1 - 9.1.10 | 4 d√≠as | 60-65% | 80-85% |
| **Developer** | 9.2.1 - 9.2.10 | 5 d√≠as | 55-60% | 75-80% |
| **QA** | 9.3.1 - 9.3.10 | 3.5 d√≠as | 65-70% | 85-90% |

**Total Secuencial**: 12.5 d√≠as (~2.5 semanas)

### Optimizaci√≥n Paralela (Si es posible)

Si se ejecutan roles en paralelo (con ayuda adicional o m√∫ltiples sesiones):
- **Total Paralelo**: 5 d√≠as (~1 semana)

---

## üéØ M√©tricas de √âxito Fase 9

| M√©trica | Target | Medici√≥n |
|---------|--------|----------|
| Architect optimizado | ‚â•80% | `architect_stories_metric` en validation set |
| Developer optimizado | ‚â•75% | `developer_code_metric` en validation set |
| QA optimizado | ‚â•85% | `qa_report_metric` en validation set |
| Mejora promedio | ‚â•+20% | (optimized - baseline) / baseline |
| Costo total | $0 | 100% local con Ollama |
| Tiempo total | ‚â§15 d√≠as | Desde inicio hasta integraci√≥n completa |
| Pipeline completo optimizado | 4/4 roles | BA, Architect, Dev, QA con DSPy MIPROv2 |

---

## üö® Riesgos y Mitigaciones

### Riesgo 1: M√©tricas complejas (Developer)

**Probabilidad**: Media
**Impacto**: Alto

**Mitigaci√≥n**:
- Comenzar con m√©tricas simples (syntax correctness)
- Iterar hacia m√©tricas m√°s sofisticadas
- Validar cada componente de m√©trica independientemente

---

### Riesgo 2: Dataset sint√©tico de baja calidad

**Probabilidad**: Media
**Impacto**: Alto

**Mitigaci√≥n**:
- Filtrado agresivo (thresholds altos)
- Revisi√≥n manual de muestra (10-20 ejemplos)
- Generaci√≥n iterativa con prompts mejorados

---

### Riesgo 3: Optimizaci√≥n no mejora suficiente (<15%)

**Probabilidad**: Baja (basado en √©xito de Fase 8)
**Impacto**: Medio

**Mitigaci√≥n**:
- Ajustar hiperpar√°metros MIPROv2 (m√°s candidates, m√°s trials)
- Aumentar dataset (200 ‚Üí 300 ejemplos)
- Refinar m√©tricas (pueden estar penalizando incorrectamente)

---

### Riesgo 4: Tiempo excede estimaci√≥n

**Probabilidad**: Media
**Impacto**: Bajo

**Mitigaci√≥n**:
- Priorizar roles (Architect > Developer > QA)
- Fases paralelas si es posible
- Reducir scope (2 roles en Fase 9, 1 rol en Fase 10)

---

## üì¶ Entregables Fase 9

### Scripts

- `scripts/generate_architect_concepts.py`
- `scripts/generate_developer_stories.py`
- `scripts/generate_qa_implementations.py`
- Extensiones en `scripts/generate_synthetic_dataset.py`
- Extensiones en `scripts/filter_synthetic_data.py`

### M√©tricas

- `dspy_baseline/metrics.py`:
  - `architect_stories_metric()`
  - `developer_code_metric()`
  - `qa_report_metric()`

### Datasets

- `artifacts/synthetic/architect/` (6 archivos)
- `artifacts/synthetic/developer/` (6 archivos)
- `artifacts/synthetic/qa/` (6 archivos)

### Modelos Optimizados

- `artifacts/dspy/architect_optimized/program.pkl`
- `artifacts/dspy/developer_optimized/program.pkl`
- `artifacts/dspy/qa_optimized/program.pkl`

### Documentaci√≥n

- `docs/fase9_multi_role_dspy_plan.md` (este documento)
- `docs/fase9_architect_optimization.md`
- `docs/fase9_developer_optimization.md`
- `docs/fase9_qa_optimization.md`
- `docs/fase9_final_report.md`

---

## üèÅ Criterios de Completitud Fase 9

### M√≠nimo Viable (MVP)

1. ‚úÖ Al menos 2/3 roles optimizados (Architect + Developer)
2. ‚úÖ Mejora ‚â•+15% en cada rol optimizado
3. ‚úÖ Pipeline funcional end-to-end
4. ‚úÖ Documentaci√≥n completa
5. ‚úÖ $0 costo (100% local)

### Objetivo Ideal

1. ‚úÖ 3/3 roles optimizados (Architect + Developer + QA)
2. ‚úÖ Mejora ‚â•+20% en cada rol
3. ‚úÖ Pipeline optimizado completo (4/4 roles con BA)
4. ‚úÖ Benchmarks reproducibles
5. ‚úÖ Tiempo total ‚â§15 d√≠as

---

## üöÄ Pr√≥ximos Pasos Inmediatos

### Paso 1: Setup Inicial

```bash
# Crear estructura de directorios
mkdir -p artifacts/synthetic/{architect,developer,qa}
mkdir -p artifacts/dspy/{architect_optimized,developer_optimized,qa_optimized}

# Verificar dependencias
.venv/bin/python -c "import dspy; print('DSPy OK')"

# Confirmar Ollama disponible
ollama list | grep mistral
```

### Paso 2: Comenzar con Architect (Fase 9.1)

```bash
# Leer plan espec√≠fico
cat docs/fase9_architect_optimization.md

# Comenzar con tarea 9.1.1
# (An√°lisis de output Architect actual)
```

### Paso 3: Crear Plan de Trabajo Diario

Ver secci√≥n "Orden de Ejecuci√≥n Recomendado" abajo.

---

## üìÖ Orden de Ejecuci√≥n Recomendado

### Semana 1 (D√≠as 1-5): Architect

- **D√≠a 1**: Tareas 9.1.1 - 9.1.3 (an√°lisis, m√©trica, conceptos)
- **D√≠a 2**: Tareas 9.1.4 - 9.1.6 (generaci√≥n dataset, filtrado, split)
- **D√≠a 3**: Tareas 9.1.7 - 9.1.8 (baseline evaluation, optimization)
- **D√≠a 4**: Tarea 9.1.9 (evaluation & comparison)
- **D√≠a 5**: Tarea 9.1.10 (integration & testing) + buffer

### Semana 2 (D√≠as 6-10): Developer

- **D√≠a 6**: Tareas 9.2.1 - 9.2.2 (an√°lisis, m√©trica)
- **D√≠a 7**: Tareas 9.2.3 - 9.2.4 (generaci√≥n stories, dataset)
- **D√≠a 8**: Tareas 9.2.5 - 9.2.7 (filtrado, split, baseline)
- **D√≠a 9**: Tarea 9.2.8 (optimization)
- **D√≠a 10**: Tareas 9.2.9 - 9.2.10 (evaluation, integration) + buffer

### Semana 3 (D√≠as 11-13): QA

- **D√≠a 11**: Tareas 9.3.1 - 9.3.4 (an√°lisis, m√©trica, generaci√≥n)
- **D√≠a 12**: Tareas 9.3.5 - 9.3.8 (filtrado, split, baseline, optimization)
- **D√≠a 13**: Tareas 9.3.9 - 9.3.10 (evaluation, integration)

### Semana 3 (D√≠as 14-15): Cierre

- **D√≠a 14**: Reporte final, benchmarks comparativos
- **D√≠a 15**: Documentaci√≥n, limpieza, commit final

---

## üìñ Referencias

- **Fase 8 Success Case**: `docs/fase8_progress.md`
- **DSPy Documentation**: https://dspy-docs.vercel.app/
- **MIPROv2 Paper**: https://arxiv.org/abs/2406.11695
- **Pipeline Architecture**: `docs/architecture.md`

---

**√öltima Actualizaci√≥n**: 2025-11-09 20:30
**Branch**: `dspy-multi-role`
**Status**: ‚è≥ PENDING - Ready to start with 9.1.1
