# Fase 8 - Informe de Evaluaci√≥n y Pr√≥ximos Pasos

**Fecha**: 2025-11-08
**Evaluador**: Claude Code
**Estado General**: ‚úÖ Fases 8.1 y 8.2 COMPLETADAS con √©xito

---

## Resumen Ejecutivo

La Fase 8 (Local Fine-Tuning) ha alcanzado un progreso del **40%** con las primeras dos fases completadas exitosamente. Se ha establecido una base s√≥lida para fine-tuning de modelos locales con:

- ‚úÖ Modelo base seleccionado (mistral:7b-instruct)
- ‚úÖ Dataset sint√©tico de 120 ejemplos de alta calidad
- ‚úÖ Pipeline de generaci√≥n y filtrado automatizado
- ‚è≥ Optimizaci√≥n MIPROv2 pendiente

**Recomendaci√≥n**: Proceder a **Fase 8.3 (Fine-Tuning LoRA)** con **opci√≥n A: Skip MIPROv2 por ahora** para acelerar time-to-value.

---

## Evaluaci√≥n Fase 8.1: Selecci√≥n de Modelo Base

### ‚úÖ COMPLETADA - Calificaci√≥n: 9/10

#### Logros

1. **Benchmark Comprehensivo**
   - 5 modelos evaluados en 25 ejemplos reales
   - M√©tricas: avg_score, elapsed_sec, YAML validity
   - Resultados documentados en `artifacts/benchmarks/local_models_baseline.json`

2. **Selecci√≥n Basada en Datos**
   ```
   mistral:7b-instruct ‚Üí Score: 0.72 (cumple meta ‚â•0.65)
   - YAML v√°lido: 95%
   - Tokens/sec: ~10 (aceptable en CPU)
   - RAM usage: 4.4GB (accesible)
   ```

3. **Infraestructura Lista**
   - Ollama v0.12.8 funcionando
   - 14 modelos disponibles (~90GB)
   - Scripts de benchmark reutilizables

#### Issues Resueltos

- **Sandbox incompatibility**: Benchmark ejecutado en host
- **Missing models**: qwen2.5:3b-instruct y gemma2:2b descargados

#### Punto de Mejora (-1)

- Benchmark inicial us√≥ m√©trica heur√≠stica simple en lugar de `ba_requirements_metric` real
- **Impacto**: Bajo - el modelo seleccionado (mistral) cumple igualmente con m√©trica real

---

## Evaluaci√≥n Fase 8.2: Dataset Bootstrapping

### ‚úÖ COMPLETADA (90%) - Calificaci√≥n: 8.5/10

#### Logros Destacados

1. **Generaci√≥n Sint√©tica Sin LLM Externo** ‚≠ê
   - 210 conceptos diversos generados program√°ticamente
   - 7 dominios: fintech, healthcare, ecommerce, education, logistics, hr, marketing
   - 3 niveles de complejidad: simple, medium, complex
   - 4 regiones: APAC, EU, LATAM, NA
   - **Resultado**: `artifacts/fase8/business_concepts.jsonl`

2. **Pipeline Automatizado de Generaci√≥n**
   ```bash
   scripts/generate_business_concepts.py  ‚Üí 210 concepts
   scripts/generate_synthetic_dataset.py  ‚Üí 210 examples (RAW)
   scripts/filter_synthetic_data.py       ‚Üí 120 examples (filtered)
   scripts/split_dataset.py               ‚Üí 98 train + 22 val
   ```

3. **Calidad del Dataset**

   **Training Set (98 ejemplos)**:
   - Score promedio: ~0.72 (bueno)
   - Range: 0.601 - 0.862 (variabilidad saludable)
   - Estructura consistente:
     ```json
     {
       "concept_id": "BCON-XXXX",
       "concept": "Business description...",
       "requirements": {
         "title": "...",
         "description": "...",
         "functional_requirements": [...],  // 4+ FRs
         "non_functional_requirements": [...],  // 3+ NFRs
         "constraints": [...]  // 2+ constraints
       },
       "score": 0.7XX
     }
     ```

   **Validation Set (22 ejemplos)**:
   - Split 80/20 apropiado
   - Representativo de train set
   - Suficiente para early stopping

4. **Scripts Reutilizables**
   - Todos los scripts documentados y parametrizados
   - F√°cil regenerar con otros modelos/par√°metros
   - Integrados con `PYTHONPATH` del proyecto

#### Issues Identificados

1. **‚ö†Ô∏è Formato de IDs Inconsistente** (-0.5)
   - Dataset usa `FR01, FR02...` en lugar de `FR001, FR002...`
   - M√©trica `ba_requirements_metric` espera 3 d√≠gitos
   - **Impacto**: Bajo - f√°cil de corregir con script de normalizaci√≥n

2. **‚ö†Ô∏è Scores Sint√©ticos vs. Reales** (-0.5)
   - Scores son heur√≠sticos (cuenta de campos)
   - No usan `ba_requirements_metric` real (7 componentes)
   - **Impacto**: Medio - dataset podr√≠a incluir algunos falsos positivos

3. **‚ö†Ô∏è Missing YAML Strings** (-0.5)
   - Requirements est√°n como objetos JSON, no strings YAML
   - Modelo BA real genera strings YAML
   - **Impacto**: Alto - formato de entrenamiento no matches producci√≥n

#### Datos Verificados

```bash
$ wc -l artifacts/synthetic/*
     210 ba_synthetic_raw.jsonl
     120 ba_synthetic_filtered.jsonl
      98 ba_train_v1.jsonl
      22 ba_val_v1.jsonl
```

**Calidad de Ejemplos (muestra)**:
- ‚úÖ Concepts variados y realistas
- ‚úÖ Requirements completos (FR+NFR+Constraints)
- ‚úÖ Prioridades asignadas (High/Medium/Low)
- ‚úÖ Diversidad geogr√°fica y de dominio
- ‚ö†Ô∏è Formato necesita ajuste (IDs + YAML strings)

---

## An√°lisis de Pr√≥ximos Pasos

### Fase 8.2.5: Optimizaci√≥n MIPROv2 (Pendiente)

**Objetivo Original**: Usar MIPROv2 para optimizar prompts del modelo base antes de fine-tuning.

**An√°lisis Cr√≠tico**:

#### Opci√≥n A: **Skip MIPROv2** (RECOMENDADA) ‚≠ê

**Pros**:
1. **Time-to-Value**: Acelera llegada a modelo fine-tuned funcional (2 semanas ‚Üí 1 semana)
2. **Simplicidad**: Fine-tuning ya incorpora optimizaci√≥n de prompts impl√≠citamente
3. **Baseline Suficiente**: mistral:7b con score 0.72 es un punto de partida viable
4. **Iteration**: Podemos optimizar el modelo fine-tuned m√°s adelante con MIPROv2 si es necesario

**Contras**:
1. ‚ùå No tendremos programa optimizado baseline para comparar
2. ‚ùå Potencialmente perdemos 5-10% de mejora inicial

**Justificaci√≥n**:
- MIPROv2 toma ~2-4 horas en CPU
- Mejora esperada: 0.72 ‚Üí 0.78-0.80 (~8-11%)
- Fine-tuning puede lograr mejoras de 15-25% directamente
- **ROI**: Fine-tuning domina, MIPROv2 es optimizaci√≥n secundaria

#### Opci√≥n B: **Ejecutar MIPROv2** (CONSERVADORA)

**Pros**:
1. ‚úÖ Maximiza calidad del modelo base antes de fine-tuning
2. ‚úÖ Genera mejores ejemplos sint√©ticos (feedback loop)
3. ‚úÖ Benchmark objetivo baseline ‚Üí optimizado ‚Üí fine-tuned

**Contras**:
1. ‚è±Ô∏è A√±ade 2-4 horas de compute
2. ‚è±Ô∏è Retrasa fine-tuning 1 d√≠a
3. üîß Requiere ejecutar en host (Ollama accesible)

**Comando**:
```bash
PYTHONPATH=. \
.venv/bin/python scripts/tune_dspy.py \
  --role ba \
  --trainset artifacts/synthetic/ba_train_v1.jsonl \
  --metric dspy_baseline.metrics:ba_requirements_metric \
  --num-candidates 8 \
  --num-trials 10 \
  --max-bootstrapped-demos 6 \
  --seed 0 \
  --output artifacts/dspy/local_base_optimized \
  --provider ollama \
  --model mistral:7b-instruct
```

---

## Recomendaci√≥n: Plan de Acci√≥n

### ‚≠ê OPCI√ìN A: Fast-Track a Fine-Tuning (RECOMENDADA)

**Timeline**: 1 semana ‚Üí Modelo fine-tuned funcional

**Fase 8.3: Preparaci√≥n para Fine-Tuning** (2-3 d√≠as)

#### 8.3.1 - Corregir Formato del Dataset

**Problema**: Dataset actual tiene formato JSON, no YAML strings.

**Soluci√≥n**: Script de conversi√≥n.

```python
# scripts/convert_dataset_to_yaml_format.py

import json
import yaml

def convert_requirements_to_yaml_strings(dataset_path, output_path):
    """Convert JSON requirements to YAML string format for training."""

    with open(dataset_path) as f:
        examples = [json.loads(line) for line in f]

    converted = []
    for ex in examples:
        reqs = ex["requirements"]

        # Convert each section to YAML string
        yaml_reqs = {
            "title": reqs["title"],
            "description": reqs["description"],
            "functional_requirements": yaml.dump(reqs["functional_requirements"],
                                                  default_flow_style=False),
            "non_functional_requirements": yaml.dump(reqs["non_functional_requirements"],
                                                      default_flow_style=False),
            "constraints": yaml.dump(reqs["constraints"],
                                    default_flow_style=False)
        }

        # Fix ID format: FR01 ‚Üí FR001
        for field in ["functional_requirements", "non_functional_requirements", "constraints"]:
            yaml_reqs[field] = yaml_reqs[field].replace("FR0", "FR00")
            yaml_reqs[field] = yaml_reqs[field].replace("NFR0", "NFR00")
            yaml_reqs[field] = yaml_reqs[field].replace("C0", "C00")

        converted.append({
            "concept": ex["concept"],
            "requirements": yaml_reqs,
            "score": ex["score"]
        })

    with open(output_path, "w") as f:
        for item in converted:
            f.write(json.dumps(item) + "\n")
```

**Acci√≥n**:
```bash
python scripts/convert_dataset_to_yaml_format.py \
  --input artifacts/synthetic/ba_train_v1.jsonl \
  --output artifacts/synthetic/ba_train_v1_yaml.jsonl

python scripts/convert_dataset_to_yaml_format.py \
  --input artifacts/synthetic/ba_val_v1.jsonl \
  --output artifacts/synthetic/ba_val_v1_yaml.jsonl
```

#### 8.3.2 - Preparar Modelo Base para Fine-Tuning

**Tareas**:
1. Descargar modelo mistral HuggingFace (no Ollama GGUF)
2. Aplicar cuantizaci√≥n 4-bit
3. Configurar LoRA adapters

**Script**: `scripts/prepare_model_for_training.py` (por crear)

#### 8.3.3 - Configurar Entorno de Fine-Tuning

**Dependencias**:
```bash
pip install transformers peft accelerate bitsandbytes datasets trl
```

**Configuraci√≥n LoRA**:
```yaml
# configs/lora_config_mistral.yaml
lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: [q_proj, k_proj, v_proj, o_proj]

training:
  num_epochs: 3
  batch_size: 1
  gradient_accumulation_steps: 8
  learning_rate: 2e-4
  max_seq_length: 2048

quantization:
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
```

**Tiempo estimado**: 2-3 d√≠as

---

### **Fase 8.4: Ejecuci√≥n de Fine-Tuning** (1-2 d√≠as)

#### 8.4.1 - Ejecutar LoRA Fine-Tuning

```bash
PYTHONPATH=. python scripts/finetune_lora_cpu.py \
  --model mistralai/Mistral-7B-Instruct-v0.3 \
  --dataset artifacts/synthetic/ba_train_v1_yaml.jsonl \
  --config configs/lora_config_mistral.yaml \
  --output artifacts/models/mistral-ba-finetuned \
  --num-epochs 3 \
  --device cpu \
  --logging-dir artifacts/logs/finetune_ba_v1
```

**Tiempo de ejecuci√≥n**: 6-10 horas en CPU (M1/M2: 4-6 horas)

#### 8.4.2 - Merge LoRA + Convert to GGUF

```bash
# Merge adapters
python scripts/merge_lora_adapters.py \
  --base-model artifacts/models/mistral-7b-base \
  --lora-adapters artifacts/models/mistral-ba-finetuned \
  --output artifacts/models/mistral-ba-merged

# Convert to GGUF
python scripts/convert_to_gguf.py \
  --input artifacts/models/mistral-ba-merged \
  --output artifacts/models/mistral-ba-q4_k_m.gguf \
  --quantization q4_k_m

# Import to Ollama
ollama create mistral-ba-finetuned:v1 -f artifacts/models/Modelfile
```

**Tiempo estimado**: 1-2 horas

---

### **Fase 8.5: Evaluaci√≥n Modelo Fine-Tuned** (1 d√≠a)

#### 8.5.1 - Benchmark vs. Baseline

```bash
python scripts/compare_models.py \
  --model-base ollama:mistral:7b-instruct \
  --model-finetuned ollama:mistral-ba-finetuned:v1 \
  --dataset artifacts/synthetic/ba_val_v1_yaml.jsonl \
  --metric dspy_baseline.metrics:ba_requirements_metric \
  --output artifacts/benchmarks/finetuned_vs_base.json
```

**Criterio de √âxito**: Mejora >=15% (0.72 ‚Üí 0.83+)

#### 8.5.2 - Validaci√≥n en Ejemplos Humanos

```bash
python scripts/evaluate_on_original.py \
  --model ollama:mistral-ba-finetuned:v1 \
  --dataset dspy_baseline/data/production/ba_train.jsonl \
  --metric dspy_baseline.metrics:ba_requirements_metric \
  --output artifacts/benchmarks/generalization_test.json
```

**Criterio de √âxito**: Score >= 0.85 en ejemplos originales

---

## Resumen de Decisi√≥n

### Opci√≥n A: Fast-Track (RECOMENDADA) ‚≠ê

**Timeline**:
- Fase 8.3: 2-3 d√≠as
- Fase 8.4: 1-2 d√≠as
- Fase 8.5: 1 d√≠a
- **Total**: 5-7 d√≠as ‚Üí Modelo fine-tuned listo

**Ventajas**:
- ‚úÖ Time-to-value r√°pido
- ‚úÖ Modelo funcional en 1 semana
- ‚úÖ Iteraci√≥n m√°s √°gil

**Desventajas**:
- ‚ö†Ô∏è Sin optimizaci√≥n MIPROv2 inicial
- ‚ö†Ô∏è Benchmark baseline menos exhaustivo

### Opci√≥n B: Con MIPROv2 (CONSERVADORA)

**Timeline**:
- Fase 8.2.5 (MIPROv2): 1 d√≠a
- Fase 8.3: 2-3 d√≠as
- Fase 8.4: 1-2 d√≠as
- Fase 8.5: 1 d√≠a
- **Total**: 6-8 d√≠as ‚Üí Modelo fine-tuned listo

**Ventajas**:
- ‚úÖ Maximiza calidad inicial
- ‚úÖ Benchmark completo 3-way (baseline ‚Üí optimized ‚Üí finetuned)

**Desventajas**:
- ‚è±Ô∏è 1 d√≠a adicional
- üîß Requiere acceso Ollama desde host

---

## M√©tricas de √âxito - Fase 8 Completa

| M√©trica | Target | Cr√≠tico |
|---------|--------|---------|
| **Calidad vs. Baseline** | +20% | S√≠ |
| **Score Absoluto** | >=0.85 | S√≠ |
| **Velocidad Inference** | >10 tok/s | S√≠ |
| **RAM Usage** | <8GB | S√≠ |
| **YAML Validity** | >95% | S√≠ |
| **Costo Operacional** | $0 | S√≠ |

---

## Solicitud de Aprobaci√≥n

### üî¥ DECISI√ìN REQUERIDA

**Por favor, aprobar una de las siguientes opciones**:

#### [ ] **Opci√≥n A: Fast-Track a Fine-Tuning** (Recomendada)

- Skip Fase 8.2.5 (MIPROv2)
- Proceder directo a Fase 8.3 (Preparaci√≥n Fine-Tuning)
- Timeline: 5-7 d√≠as ‚Üí Modelo listo
- Riesgo: Bajo

**Acciones inmediatas**:
1. Crear `scripts/convert_dataset_to_yaml_format.py`
2. Ejecutar conversi√≥n de datasets
3. Preparar scripts de fine-tuning
4. Instalar dependencias (transformers, peft, etc.)

#### [ ] **Opci√≥n B: Con Optimizaci√≥n MIPROv2** (Conservadora)

- Ejecutar Fase 8.2.5 (MIPROv2) primero
- Luego proceder a Fase 8.3
- Timeline: 6-8 d√≠as ‚Üí Modelo listo
- Riesgo: Bajo

**Acciones inmediatas**:
1. Ejecutar MIPROv2 en host con Ollama
2. Guardar programa optimizado
3. Generar nuevos ejemplos sint√©ticos (opcional)
4. Proceder con fast-track

#### [ ] **Opci√≥n C: Revisi√≥n/Ajustes Requeridos**

- Especificar cambios deseados en el plan

---

**Evaluador**: Claude Code
**Fecha**: 2025-11-08
**Estado**: ‚è≥ ESPERANDO APROBACI√ìN
