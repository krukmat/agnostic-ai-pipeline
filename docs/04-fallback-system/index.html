<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Part 4: Inside the Fallback System | Agnostic AI Pipeline</title>
    <style>
        :root {
            --primary: #2563eb;
            --success: #16a34a;
            --warning: #ca8a04;
            --danger: #dc2626;
            --bg: #ffffff;
            --text: #1f2937;
            --border: #e5e7eb;
            --code-bg: #f9fafb;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: var(--text);
            background: var(--bg);
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem 1rem;
        }

        h1 {
            font-size: 2.5rem;
            font-weight: 800;
            margin-bottom: 1rem;
            line-height: 1.2;
        }

        h2 {
            font-size: 1.875rem;
            font-weight: 700;
            margin-top: 3rem;
            margin-bottom: 1rem;
            border-top: 2px solid var(--border);
            padding-top: 2rem;
        }

        h3 {
            font-size: 1.5rem;
            font-weight: 600;
            margin-top: 2rem;
            margin-bottom: 1rem;
        }

        p {
            margin-bottom: 1.25rem;
        }

        .subtitle {
            font-size: 1.25rem;
            color: #6b7280;
            margin-bottom: 1rem;
        }

        .series-nav {
            background: var(--code-bg);
            padding: 1rem;
            border-radius: 8px;
            margin-bottom: 2rem;
            font-size: 0.95rem;
            color: #6b7280;
        }

        .series-nav a {
            color: var(--primary);
            text-decoration: none;
        }

        .series-nav a:hover {
            text-decoration: underline;
        }

        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: 'SF Mono', Monaco, 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border: 1px solid var(--border);
        }

        pre code {
            padding: 0;
            background: none;
        }

        img {
            max-width: 100%;
            height: auto;
            margin: 2rem 0;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 2rem 0;
        }

        th, td {
            padding: 0.75rem;
            text-align: left;
            border-bottom: 1px solid var(--border);
        }

        th {
            background: var(--code-bg);
            font-weight: 600;
        }

        blockquote {
            border-left: 4px solid var(--primary);
            padding-left: 1.5rem;
            margin: 2rem 0;
            font-style: italic;
            color: #6b7280;
        }

        ul, ol {
            margin-bottom: 1.5rem;
            padding-left: 2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        .callout {
            background: #eff6ff;
            border-left: 4px solid var(--primary);
            padding: 1rem 1.5rem;
            margin: 2rem 0;
            border-radius: 4px;
        }

        .warning {
            background: #fef3c7;
            border-left-color: var(--warning);
        }

        .footer {
            margin-top: 4rem;
            padding-top: 2rem;
            border-top: 2px solid var(--border);
            text-align: center;
            color: #6b7280;
        }

        .footer a {
            color: var(--primary);
            text-decoration: none;
        }

        .footer a:hover {
            text-decoration: underline;
        }

        .diagram-caption {
            text-align: center;
            font-style: italic;
            color: #6b7280;
            font-size: 0.9rem;
            margin-top: -1rem;
            margin-bottom: 2rem;
        }

        .stats-box {
            background: #f0fdf4;
            border: 2px solid #86efac;
            padding: 1.5rem;
            border-radius: 8px;
            margin: 2rem 0;
        }

        .stats-box h3 {
            margin-top: 0;
            color: #15803d;
        }

        @media (max-width: 640px) {
            body {
                padding: 1rem 0.75rem;
            }

            h1 {
                font-size: 2rem;
            }

            h2 {
                font-size: 1.5rem;
            }

            h3 {
                font-size: 1.25rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }
        }
    </style>
</head>
<body>
    <h1>How My AI Pipeline Automatically Recovered from 8 Model Failures</h1>
    <p class="subtitle">What happens when Gemini times out, GPT-4 hits rate limits, or Claude returns garbage? Here's how automatic fallback works—and why it saved my pipeline.</p>

    <div class="series-nav">
        <strong>Part 4 of 7</strong> •
        <a href="../03-cost-engineering/">← Part 3: Cost Engineering</a> •
        <a href="../02-multi-role-pipeline/">Part 2: Multi-Role Pipeline</a> •
        <a href="../00-vision-ok/">Part 1: The Vision</a>
    </div>

    <hr style="margin: 2rem 0; border: none; border-top: 1px solid var(--border);">

    <h2>The Reality: Models Fail. A Lot.</h2>

    <p>Here's what happened on Day 4 of testing:</p>

    <pre><code>[Dev] Story S6: Implementing OAuth2 flow
[Dev] Using primary model: vertex_sdk/gemini-2.5-pro
[Dev] Request failed: 503 Service Unavailable
[Dev] Duration: 45 seconds
[Dev] Cost: $0 (no response)</code></pre>

    <p>Vertex AI was down. Our primary model couldn't respond.</p>

    <p>In a typical setup, this is where you stop, file a ticket, wait for Google to fix it, and lose half a day.</p>

    <p>But here's what actually happened:</p>

    <pre><code>[Dev] Primary model failed: vertex_sdk/gemini-2.5-pro
[Dev] Analyzing failure... API unavailable (503)
[Dev] Scoring backup models by specialty...
[Dev] Backup candidate: codex_cli/gpt-4-turbo (score: 85, cost: $5)
[Dev] Backup candidate: ollama/qwen2.5-coder (score: 78, cost: $0)
[Dev] Selected: codex_cli/gpt-4-turbo (cost acceptable, high score)
[Dev] Retry attempt 2/3 with gpt-4-turbo...
[Dev] Success. Duration: 8 seconds. Cost: $4.80
[Dev] Story S6 complete.</code></pre>

    <p>The pipeline automatically:</p>
    <ol>
        <li>Detected the failure type (API unavailable)</li>
        <li>Scored backup models by specialty and cost</li>
        <li>Fell back to GPT-4</li>
        <li>Completed the story successfully</li>
    </ol>

    <p><strong>Total delay:</strong> 53 seconds (45s waiting for Gemini + 8s GPT-4 success)</p>
    <p><strong>Without fallback:</strong> Hours (waiting for Vertex AI to come back online)</p>

    <h2>Why Models Fail (And It's Not Just Downtime)</h2>

    <p>Over 14 days and 47 iterations, I tracked every failure. Here's what I learned:</p>

    <div class="stats-box">
        <h3>Failure Statistics</h3>
        <p><strong>Total LLM calls:</strong> 312<br>
        <strong>Failures:</strong> 42 (13.5%)<br>
        <strong>Automatic recoveries:</strong> 34 (81% recovery rate)<br>
        <strong>Manual intervention:</strong> 8 (19% couldn't auto-recover)</p>
    </div>

    <h3>The Five Failure Types</h3>

    <p><strong>1. API Timeouts (32% of failures)</strong></p>
    <pre><code>Error: Request timeout after 120 seconds
Provider: vertex_sdk, openai, claude_cli</code></pre>

    <p><strong>2. Rate Limits (28% of failures)</strong></p>
    <pre><code>Error: 429 Too Many Requests
Retry-After: 60 seconds
Provider: claude_cli, openai</code></pre>

    <p><strong>3. Malformed Output (24% of failures)</strong></p>
    <pre><code>Error: JSON parsing failed
Expected: { "code": "...", "tests": "..." }
Received: "Sure! Here's the code:\n```python\n..."</code></pre>

    <p><strong>4. Context Window Exceeded (10% of failures)</strong></p>
    <pre><code>Error: Input tokens (145,328) exceed max context (128,000)
Provider: vertex_sdk/gemini-2.5-pro</code></pre>

    <p><strong>5. API Unavailable (6% of failures)</strong></p>
    <pre><code>Error: 503 Service Unavailable
Provider: vertex_sdk, claude_cli</code></pre>

    <h2>The Fallback Architecture</h2>

    <img src="diagram1-fallback-flow.png" alt="Complete Fallback Flow">
    <p class="diagram-caption">Complete fallback flow: from primary failure to automatic recovery</p>

    <h3>Step 1: Primary Model Attempt</h3>
    <pre><code>async def implement_story(story: dict, config: dict):
    primary_model = config['roles']['dev']

    try:
        response = await llm_client.chat(
            system=prompts['developer'],
            user=story['description'],
            model=primary_model['model']
        )
        return response, {"status": "success"}

    except Exception as e:
        return None, {"status": "failed", "error": str(e)}</code></pre>

    <h3>Step 2: Failure Analysis</h3>
    <pre><code>def analyze_failure_and_suggest_model(error_msg, story, config):
    failure_type = detect_failure_type(error_msg)

    if failure_type == "timeout":
        specialties = ["fast_inference", "code_generation"]
    elif failure_type == "rate_limit":
        specialties = ["different_provider", "code_generation"]
    # ... more failure types

    scored_backups = score_backup_models(
        backup_models=config['roles']['dev']['backup_models'],
        specialties=specialties
    )

    return scored_backups[0]</code></pre>

    <h2>Specialty Scoring System</h2>

    <img src="diagram2-specialty-scoring.png" alt="Specialty Scoring System">
    <p class="diagram-caption">How the scoring algorithm evaluates backup models based on required specialties</p>

    <p>Each model has specialties defined in config:</p>
    <pre><code>providers:
  codex_cli:
    specialties:
      code_generation: 10
      structured_output: 8
      fast_inference: 6
    cost_tier: high

  ollama:
    specialties:
      code_generation: 7
      fast_inference: 10
      different_provider: 10
    cost_tier: free</code></pre>

    <h2>Real Examples from Production</h2>

    <h3>Example 1: Vertex AI Downtime (Day 4)</h3>
    <p><strong>Story S6:</strong> Implement OAuth2 authentication</p>

    <p><strong>Primary attempt:</strong></p>
    <pre><code>[Dev] Model: vertex_sdk/gemini-2.5-pro
[Dev] Error: 503 Service Unavailable
[Dev] Duration: 45 seconds
[Dev] Cost: $0</code></pre>

    <p><strong>Failure analysis:</strong></p>
    <pre><code>failure_type = "api_unavailable"
specialties = ["different_provider", "code_generation"]</code></pre>

    <p><strong>Backup scoring:</strong></p>
    <pre><code>codex_cli/gpt-4-turbo: score=85
ollama/qwen2.5-coder: score=78</code></pre>

    <p><strong>Backup attempt:</strong></p>
    <pre><code>[Dev] Model: codex_cli/gpt-4-turbo
[Dev] Status: Success
[Dev] Duration: 8 seconds
[Dev] Cost: $4.80</code></pre>

    <h3>Example 2: Claude Rate Limit (Day 9)</h3>
    <p><strong>Story S12:</strong> Generate comprehensive QA test suite</p>

    <p><strong>Primary attempt:</strong></p>
    <pre><code>[QA] Model: claude_cli/claude-3-5-sonnet-latest
[QA] Error: 429 Too Many Requests, Retry-After: 60
[QA] Duration: 2 seconds
[QA] Cost: $0</code></pre>

    <p><strong>Backup attempt:</strong></p>
    <pre><code>[QA] Model: ollama/qwen2.5-coder:32b
[QA] Status: Success
[QA] Duration: 4 seconds
[QA] Cost: $0</code></pre>

    <p><strong>Total cost:</strong> $0 (local fallback saved the day)</p>

    <h2>The model_history Metadata</h2>

    <p>Every story tracks its complete model history:</p>
    <pre><code>- id: S6
  description: "Implement OAuth2 authentication"
  status: done
  model_history:
    - attempt: 1
      model: gemini-2.5-pro
      provider: vertex_sdk
      status: failed
      error: "503 Service Unavailable"
      duration: 45
      cost: 0

    - attempt: 2
      model: gpt-4-turbo
      provider: codex_cli
      status: success
      duration: 8
      cost: 4.80
      reason: "Fallback due to API unavailable"</code></pre>

    <h2>Recovery Budget Configuration</h2>

    <div class="callout warning">
        <h3>⚠️ Dangerous Config</h3>
        <pre><code>pipeline:
  max_recovery_attempts: 10      # Too many retries
  allow_cost_increase: true      # No cost limit
  prefer_local: false            # Prefer cloud</code></pre>
        <p>This can lead to cost explosions with multiple expensive retries!</p>
    </div>

    <div class="callout">
        <h3>✅ Safe Config</h3>
        <pre><code>pipeline:
  max_recovery_attempts: 2       # Fail fast
  allow_cost_increase: false     # Block expensive fallbacks
  prefer_local: true             # Try free models first
  max_recovery_cost: 5.0         # Cap at $5 per recovery</code></pre>
    </div>

    <h2>Real Stats from 14 Days</h2>

    <table>
        <thead>
            <tr>
                <th>Metric</th>
                <th>Value</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Total LLM calls</td>
                <td>312</td>
            </tr>
            <tr>
                <td>Primary failures</td>
                <td>42 (13.5%)</td>
            </tr>
            <tr>
                <td>Successful fallbacks</td>
                <td>34 (81% recovery)</td>
            </tr>
            <tr>
                <td>Failed after all retries</td>
                <td>8 (19% manual intervention)</td>
            </tr>
            <tr>
                <td>Cost saved via local fallback</td>
                <td>$38-45</td>
            </tr>
            <tr>
                <td>Time saved vs manual retry</td>
                <td>~4-6 hours</td>
            </tr>
        </tbody>
    </table>

    <p><strong>Most common fallback path:</strong></p>
    <ul>
        <li>Gemini (fail) → Ollama (success): 18 times</li>
        <li>GPT-4 (fail) → Ollama (success): 9 times</li>
        <li>Claude (rate limit) → Ollama (success): 7 times</li>
    </ul>

    <p style="font-size: 1.25rem; font-weight: 600; color: var(--success); margin-top: 2rem;">Local Ollama saved my ass 34 times in 14 days.</p>

    <h2>Try It Yourself</h2>

    <p><strong>Step 1: Configure fallback chain</strong></p>
    <pre><code>roles:
  dev:
    provider: vertex_sdk
    model: gemini-2.5-pro
    backup_models:
      - provider: ollama
        model: qwen2.5-coder:32b
      - provider: codex_cli
        model: gpt-4-turbo

pipeline:
  max_recovery_attempts: 2
  prefer_local: true
  max_recovery_cost: 5.0</code></pre>

    <p><strong>Step 2: Run iteration</strong></p>
    <pre><code>make iteration CONCEPT="User authentication system"</code></pre>

    <p><strong>Step 3: Check model_history</strong></p>
    <pre><code>cat planning/stories.yaml | grep -A 20 "model_history"</code></pre>

    <h2>What's Next</h2>
    <p>In <strong>Part 5</strong>, we'll explore A2A (Agent-to-Agent) mode: running each role as an independent HTTP service.</p>
    <p>In <strong>Part 6</strong>, I'll show you how to build your own custom providers and validators.</p>

    <hr style="margin: 3rem 0; border: none; border-top: 1px solid var(--border);">

    <div class="series-nav">
        <strong>Part 4 of 7:</strong> Inside the Fallback System<br>
        <a href="../03-cost-engineering/">← Part 3: Cost Engineering</a> |
        <a href="../02-multi-role-pipeline/">Part 2: Multi-Role Pipeline</a> |
        <a href="../00-vision-ok/">Part 1: The Vision</a> |
        Part 5: A2A Mode (coming soon)
    </div>

    <div class="footer">
        <p><strong>Repository:</strong> <a href="https://github.com/krukmat/agnostic-ai-pipeline" target="_blank">https://github.com/krukmat/agnostic-ai-pipeline</a></p>
        <p><strong>Try it:</strong> <code>git clone https://github.com/krukmat/agnostic-ai-pipeline.git</code></p>
        <p style="margin-top: 1rem; font-style: italic;">Questions? Hit a weird failure mode? Open an issue.<br>Let's make this fallback system bulletproof together.</p>
    </div>
</body>
</html>
