{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Owner LoRA Model - Evaluation (Baseline vs Student)\n",
    "\n",
    "Este notebook eval√∫a el modelo Product Owner entrenado con LoRA contra el baseline.\n",
    "\n",
    "**Requisitos**:\n",
    "- Google Colab con GPU (T4 Free)\n",
    "- El notebook clona autom√°ticamente el repositorio\n",
    "\n",
    "**Pasos**:\n",
    "1. Verificar GPU\n",
    "2. Instalar dependencias\n",
    "3. Clonar repositorio con el modelo LoRA\n",
    "4. Ejecutar evaluaci√≥n baseline (Qwen2.5-7B sin LoRA)\n",
    "5. Ejecutar evaluaci√≥n student (Qwen2.5-7B + LoRA)\n",
    "6. Comparar resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verificar GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Instalar Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install -q transformers>=4.36.0 peft>=0.7.0 bitsandbytes>=0.41.0 accelerate>=0.25.0 torch typer pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clonar Repositorio y Verificar Modelo LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Clonar repositorio con el modelo\n",
    "print(\"üì• Clonando repositorio con el modelo LoRA...\")\n",
    "\n",
    "repo_url = \"https://github.com/krukmat/agnostic-ai-pipeline.git\"\n",
    "repo_branch = \"dspy-multi-role\"\n",
    "repo_path = \"/content/agnostic-ai-pipeline\"\n",
    "\n",
    "if not os.path.exists(repo_path):\n",
    "    !git clone --depth 1 --branch {repo_branch} {repo_url} {repo_path}\n",
    "    print(f\"‚úÖ Repositorio clonado (branch: {repo_branch})\")\n",
    "else:\n",
    "    print(f\"‚úÖ Repositorio ya existe en: {repo_path}\")\n",
    "\n",
    "# 2. Verificar que el modelo est√° en el repo\n",
    "model_path = f\"{repo_path}/artifacts/models/po_student_v1\"\n",
    "valset_path = f\"{repo_path}/artifacts/synthetic/product_owner/product_owner_val.jsonl\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"\\n‚ùå ERROR: Modelo no encontrado en: {model_path}\")\n",
    "    raise FileNotFoundError(\"Modelo LoRA no encontrado en el repositorio\")\n",
    "\n",
    "if not os.path.exists(valset_path):\n",
    "    print(f\"\\n‚ùå ERROR: Dataset de validaci√≥n no encontrado en: {valset_path}\")\n",
    "    raise FileNotFoundError(\"Dataset de validaci√≥n no encontrado\")\n",
    "\n",
    "print(f\"‚úÖ Modelo encontrado en: {model_path}\")\n",
    "print(f\"‚úÖ Dataset de validaci√≥n encontrado: {valset_path}\")\n",
    "\n",
    "# 3. Verificar archivos cr√≠ticos del modelo\n",
    "print(f\"\\nüìÇ Contenido del modelo:\")\n",
    "!ls -lh {model_path}\n",
    "\n",
    "required_files = [\"adapter_config.json\", \"adapter_model.safetensors\", \"tokenizer_config.json\"]\n",
    "missing_files = []\n",
    "\n",
    "for file in required_files:\n",
    "    file_path = os.path.join(model_path, file)\n",
    "    if not os.path.exists(file_path):\n",
    "        missing_files.append(file)\n",
    "    else:\n",
    "        file_size = os.path.getsize(file_path) / 1024**2  # MB\n",
    "        print(f\"  ‚úì {file} ({file_size:.1f} MB)\")\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n‚ö†Ô∏è  ADVERTENCIA: Faltan archivos del modelo: {missing_files}\")\n",
    "    raise FileNotFoundError(f\"Archivos cr√≠ticos faltantes: {missing_files}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Todos los archivos del modelo est√°n presentes\")\n",
    "\n",
    "# 4. Cambiar al directorio del repo\n",
    "os.chdir(repo_path)\n",
    "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluaci√≥n Baseline (Qwen2.5-7B sin LoRA)\n",
    "\n",
    "Esta evaluaci√≥n usa el modelo base sin el adapter LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content/agnostic-ai-pipeline\n",
    "\n",
    "PYTHONPATH=. python scripts/eval_po_student.py \\\n",
    "  --tag baseline \\\n",
    "  --base-model Qwen/Qwen2.5-7B-Instruct \\\n",
    "  --max-samples 20 \\\n",
    "  --retries 2 \\\n",
    "  --max-new-tokens 1200 \\\n",
    "  --load-4bit \\\n",
    "  --bnb-compute-dtype float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluaci√≥n Student (Qwen2.5-7B + LoRA)\n",
    "\n",
    "Esta evaluaci√≥n usa el modelo base con el adapter LoRA entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content/agnostic-ai-pipeline\n",
    "\n",
    "PYTHONPATH=. python scripts/eval_po_student.py \\\n",
    "  --tag student \\\n",
    "  --base-model Qwen/Qwen2.5-7B-Instruct \\\n",
    "  --adapter-path artifacts/models/po_student_v1 \\\n",
    "  --max-samples 20 \\\n",
    "  --retries 2 \\\n",
    "  --max-new-tokens 1200 \\\n",
    "  --load-4bit \\\n",
    "  --bnb-compute-dtype float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Buscar archivos de resultados\n",
    "results_dir = Path(\"/content/agnostic-ai-pipeline/inference_results\")\n",
    "baseline_files = sorted(results_dir.glob(\"baseline_*.json\"))\n",
    "student_files = sorted(results_dir.glob(\"student_*.json\"))\n",
    "\n",
    "if not baseline_files:\n",
    "    print(\"‚ö†Ô∏è  No se encontraron resultados de baseline\")\n",
    "else:\n",
    "    print(f\"\\nüìä Archivos de resultados encontrados:\")\n",
    "    print(f\"  Baseline: {len(baseline_files)} archivo(s)\")\n",
    "    print(f\"  Student: {len(student_files)} archivo(s)\")\n",
    "\n",
    "# Cargar el resultado m√°s reciente de cada uno\n",
    "if baseline_files and student_files:\n",
    "    with open(baseline_files[-1], 'r') as f:\n",
    "        baseline_data = json.load(f)\n",
    "    \n",
    "    with open(student_files[-1], 'r') as f:\n",
    "        student_data = json.load(f)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"COMPARACI√ìN DE RESULTADOS\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # M√©tricas generales\n",
    "    print(\"üìà M√âTRICAS GENERALES\\n\")\n",
    "    print(f\"{'M√©trica':<30} {'Baseline':<15} {'Student':<15} {'Diff'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    baseline_metrics = baseline_data.get('metrics', {})\n",
    "    student_metrics = student_data.get('metrics', {})\n",
    "    \n",
    "    if baseline_metrics and student_metrics:\n",
    "        for metric in ['mean', 'std', 'min', 'max']:\n",
    "            b_val = baseline_metrics.get(metric, 0)\n",
    "            s_val = student_metrics.get(metric, 0)\n",
    "            diff = s_val - b_val\n",
    "            diff_pct = (diff / b_val * 100) if b_val != 0 else 0\n",
    "            \n",
    "            print(f\"{metric.upper():<30} {b_val:<15.4f} {s_val:<15.4f} {diff:+.4f} ({diff_pct:+.1f}%)\")\n",
    "    \n",
    "    # Tasa de √©xito YAML\n",
    "    print(f\"\\nüìã TASA DE √âXITO YAML\\n\")\n",
    "    print(f\"{'Modelo':<30} {'Total':<10} {'V√°lidos':<10} {'Errores':<10} {'Tasa √âxito'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    b_total = baseline_data.get('total_samples', 0)\n",
    "    b_valid = baseline_data.get('valid_samples', 0)\n",
    "    b_failed = baseline_data.get('failed_samples', 0)\n",
    "    b_rate = (b_valid / b_total * 100) if b_total > 0 else 0\n",
    "    \n",
    "    s_total = student_data.get('total_samples', 0)\n",
    "    s_valid = student_data.get('valid_samples', 0)\n",
    "    s_failed = student_data.get('failed_samples', 0)\n",
    "    s_rate = (s_valid / s_total * 100) if s_total > 0 else 0\n",
    "    \n",
    "    print(f\"{'Baseline':<30} {b_total:<10} {b_valid:<10} {b_failed:<10} {b_rate:.1f}%\")\n",
    "    print(f\"{'Student':<30} {s_total:<10} {s_valid:<10} {s_failed:<10} {s_rate:.1f}%\")\n",
    "    \n",
    "    # Criterios de aceptaci√≥n\n",
    "    print(f\"\\n‚úÖ CRITERIOS DE ACEPTACI√ìN (9.D.4)\\n\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    yaml_valid_threshold = 0.90\n",
    "    quality_threshold = 0.90\n",
    "    \n",
    "    yaml_pass = (b_rate >= yaml_valid_threshold * 100) and (s_rate >= yaml_valid_threshold * 100)\n",
    "    quality_pass = (s_val >= quality_threshold * b_val) if baseline_metrics and student_metrics else False\n",
    "    \n",
    "    print(f\"1. YAML v√°lido ‚â•90%:\")\n",
    "    print(f\"   Baseline: {b_rate:.1f}% {'‚úÖ PASS' if b_rate >= yaml_valid_threshold * 100 else '‚ùå FAIL'}\")\n",
    "    print(f\"   Student:  {s_rate:.1f}% {'‚úÖ PASS' if s_rate >= yaml_valid_threshold * 100 else '‚ùå FAIL'}\")\n",
    "    \n",
    "    if baseline_metrics and student_metrics:\n",
    "        print(f\"\\n2. Student ‚â• 0.9 √ó Baseline:\")\n",
    "        target = quality_threshold * baseline_metrics.get('mean', 0)\n",
    "        actual = student_metrics.get('mean', 0)\n",
    "        print(f\"   Target:  {target:.4f}\")\n",
    "        print(f\"   Actual:  {actual:.4f} {'‚úÖ PASS' if actual >= target else '‚ùå FAIL'}\")\n",
    "    \n",
    "    overall_pass = yaml_pass and quality_pass\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RESULTADO GENERAL: {'‚úÖ PASS - Listo para 9.D.5' if overall_pass else '‚ùå FAIL - Requiere ajustes'}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Casos con errores\n",
    "    if b_failed > 0 or s_failed > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  CASOS CON ERROR DE FORMATO:\\n\")\n",
    "        \n",
    "        if b_failed > 0:\n",
    "            print(\"Baseline:\")\n",
    "            for result in baseline_data.get('results', []):\n",
    "                if result.get('status') == 'format_error':\n",
    "                    print(f\"  - {result.get('concept_id')} (tier: {result.get('tier')})\")\n",
    "        \n",
    "        if s_failed > 0:\n",
    "            print(\"\\nStudent:\")\n",
    "            for result in student_data.get('results', []):\n",
    "                if result.get('status') == 'format_error':\n",
    "                    print(f\"  - {result.get('concept_id')} (tier: {result.get('tier')})\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No se pueden comparar resultados: falta alg√∫n archivo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Descargar Resultados\n",
    "\n",
    "Descarga los archivos JSON para incluirlos en el repositorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Comprimir resultados\n",
    "results_dir = \"/content/agnostic-ai-pipeline/inference_results\"\n",
    "archive_path = \"/content/eval_results_20251115\"\n",
    "\n",
    "if os.path.exists(results_dir):\n",
    "    shutil.make_archive(archive_path, 'zip', results_dir)\n",
    "    print(f\"‚úÖ Resultados comprimidos en: {archive_path}.zip\")\n",
    "    \n",
    "    # Descargar\n",
    "    files.download(f\"{archive_path}.zip\")\n",
    "    print(\"‚úÖ Descarga iniciada\")\n",
    "else:\n",
    "    print(\"‚ùå No se encontr√≥ el directorio de resultados\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
