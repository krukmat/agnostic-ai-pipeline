{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Owner LoRA Training v2\n",
    "**Date**: 2025-11-15  \n",
    "**Task**: Train Qwen2.5-7B-Instruct with LoRA using optimized hyperparameters\n",
    "\n",
    "**Dataset**: 359 samples (aligned: 66.9%, needs: 30.4%, conflicts: 2.8%)  \n",
    "**Changes from v1**:\n",
    "- epochs: 3 \u2192 **4**\n",
    "- learning_rate: 1e-4 \u2192 **8e-5**\n",
    "- lr_scheduler: linear \u2192 **cosine**\n",
    "- warmup_ratio: 0.1 \u2192 **0.05**\n",
    "- gradient_accumulation: 8 \u2192 **12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Clonar repositorio y montar Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clonar el repositorio en el entorno actual\n",
    "!git clone -b dspy-multi-role https://github.com/krukmat/agnostic-ai-pipeline.git\n",
    "%cd agnostic-ai-pipeline\n",
    "\n",
    "print(\"\\n\ud83d\udcc1 Directorio actual:\")\n",
    "!pwd\n",
    "\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"\u2705 Google Drive montado\")\n",
    "except Exception:\n",
    "    print(\"\u2139\ufe0f Google Drive no disponible (Lightning u otro entorno). Continuando sin montarlo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if Path('/content').exists():\n",
    "    ENV_ROOT = Path('/content')\n",
    "elif Path('/workspace').exists():\n",
    "    ENV_ROOT = Path('/workspace')\n",
    "else:\n",
    "    ENV_ROOT = Path.cwd()\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATASET_PATH = PROJECT_ROOT / 'artifacts/distillation/po_teacher_supervised.jsonl'\n",
    "RUN_OUTPUT_DIR = ENV_ROOT / 'po_student_v2'\n",
    "ADAPTER_DIR = ENV_ROOT / 'po_student_v2_adapter'\n",
    "\n",
    "print(f\"\ud83d\udcc1 Proyecto: {PROJECT_ROOT}\")\n",
    "print(f\"\ud83d\udcbe Carpeta temporal: {ENV_ROOT}\")\n",
    "print(f\"\ud83d\udcc4 Dataset esperado: {DATASET_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/update dependencies (compatible with Qwen2.5 models)\n",
    "%%bash\n",
    "pip install -q --upgrade --no-cache-dir \\\\\\\n",
    "  \"transformers>=4.38.0\" \\\\\\\n",
    "  \"peft>=0.11.1\" \\\\\\\n",
    "  \"bitsandbytes>=0.43.2\" \\\\\\\n",
    "  \"accelerate>=0.28.0\" \\\\\\\n",
    "  \"datasets>=2.19.0\"\n",
    "pip install -q --upgrade --no-cache-dir \"transformers @ git+https://github.com/huggingface/transformers.git\"\n",
    "\n",
    "python - <<'PYBLOCK'\n",
    "import importlib\n",
    "importlib.invalidate_caches()\n",
    "import transformers, peft, datasets\n",
    "print(f'\u2705 transformers {transformers.__version__}')\n",
    "print(f'\u2705 peft {peft.__version__}')\n",
    "print(f'\u2705 datasets {datasets.__version__}')\n",
    "PYBLOCK\n",
    "\n",
    "import importlib\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "try:\n",
    "    import transformers, peft, datasets\n",
    "    print(f'\u2705 transformers {transformers.__version__}')\n",
    "    print(f'\u2705 peft {peft.__version__}')\n",
    "    print(f'\u2705 datasets {datasets.__version__}')\n",
    "except Exception as exc:\n",
    "    print('\u26a0\ufe0f Tras instalar, reinicia el runtime y vuelve a ejecutar esta celda si persiste un error.')\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Dataset\n",
    "\n",
    "**Option A**: Click the folder icon on the left, then upload `po_teacher_supervised.jsonl`  \n",
    "**Option B**: Mount Google Drive and copy from there  \n",
    "**Option C**: Use the code below to upload directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if 'ENV_ROOT' not in globals():\n",
    "    if Path('/content').exists():\n",
    "        ENV_ROOT = Path('/content')\n",
    "    elif Path('/workspace').exists():\n",
    "        ENV_ROOT = Path('/workspace')\n",
    "    else:\n",
    "        ENV_ROOT = Path.cwd()\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATASET_PATH = PROJECT_ROOT / 'artifacts/distillation/po_teacher_supervised.jsonl'\n",
    "RUN_OUTPUT_DIR = ENV_ROOT / 'po_student_v2'\n",
    "ADAPTER_DIR = ENV_ROOT / 'po_student_v2_adapter'\n",
    "\n",
    "print(f\"\ud83d\udcc4 Dataset esperado: {DATASET_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Direct upload (Colab only)\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    from google.colab import files  # type: ignore\n",
    "    uploaded = files.upload()\n",
    "    print('Sube po_teacher_supervised.jsonl y se guardar\u00e1 en /content/')\n",
    "except Exception:\n",
    "    print('\u26a0\ufe0f Esta opci\u00f3n solo est\u00e1 disponible en Colab. Usa Option B/C o coloca el archivo en DATASET_PATH manualmente.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: From Google Drive (solo Colab; ignora en Lightning)\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "    !cp /content/drive/MyDrive/path/to/po_teacher_supervised.jsonl /content/\n",
    "except Exception:\n",
    "    print('Google Drive no disponible. Usa Option A/C o coloca el archivo manualmente en DATASET_PATH.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if 'DATASET_PATH' not in globals():\n",
    "    raise RuntimeError('Define DATASET_PATH en la celda anterior antes de ejecutar esta verificaci\u00f3n.')\n",
    "\n",
    "if DATASET_PATH.exists():\n",
    "    print(f\"\u2705 Dataset encontrado: {DATASET_PATH}\")\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f Dataset no encontrado en {DATASET_PATH}.\")\n",
    "    print('   Sube el archivo o actualiza DATASET_PATH si est\u00e1s usando otra ubicaci\u00f3n.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load supervised dataset\n",
    "data = []\n",
    "with open(DATASET_PATH, \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data)} training examples\")\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_list(data)\n",
    "print(train_dataset)\n",
    "\n",
    "# Verify data structure\n",
    "print(\"\\nSample prompt (first 200 chars):\")\n",
    "print(train_dataset[0][\"prompt\"][:200])\n",
    "print(\"\\nSample response (first 200 chars):\")\n",
    "print(train_dataset[0][\"response\"][:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(\"This will take 5-7 minutes...\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"\\n\u2705 Model loaded: {model_name}\")\n",
    "print(f\"Model device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=32,                      # Rank\n",
    "    lora_alpha=64,             # Alpha\n",
    "    target_modules=[           # Target all linear layers\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,         # Dropout\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"\\n\u2705 LoRA configuration applied\")\n",
    "print(\"\\nTrainable parameters:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    full_texts = []\n",
    "    for prompt, response in zip(examples['prompt'], examples['response']):\n",
    "        messages = [\n",
    "            {'role': 'user', 'content': prompt},\n",
    "            {'role': 'assistant', 'content': response}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        full_texts.append(text)\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        full_texts,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=1536,\n",
    "    )\n",
    "\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "print(f'Tokenized dataset: {len(tokenized_dataset)} examples')\n",
    "print(f'Sample token count: {len(tokenized_dataset[0][\"input_ids\"])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Configure Training Arguments (OPTIMIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "globals_dict = globals()\n",
    "if 'RUN_OUTPUT_DIR' not in globals_dict:\n",
    "    if Path('/workspace').exists():\n",
    "        RUN_OUTPUT_DIR = Path('/workspace/po_student_v2')\n",
    "    elif Path('/content').exists():\n",
    "        RUN_OUTPUT_DIR = Path('/content/po_student_v2')\n",
    "    else:\n",
    "        RUN_OUTPUT_DIR = Path.cwd() / 'po_student_v2'\n",
    "if 'ADAPTER_DIR' not in globals_dict:\n",
    "    ADAPTER_DIR = RUN_OUTPUT_DIR.with_name('po_student_v2_adapter')\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir=str(RUN_OUTPUT_DIR),\n",
    "\n",
    "    # Training schedule (OPTIMIZED)\n",
    "    num_train_epochs=4,                    # \u2190 CHANGED from 3 to 4\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=24,        # \u2190 CHANGED from 8 to 12\n",
    "\n",
    "    # Learning rate (OPTIMIZED)\n",
    "    learning_rate=8e-5,                    # \u2190 CHANGED from 1e-4 to 8e-5\n",
    "    lr_scheduler_type=\"cosine\",            # \u2190 CHANGED from \"linear\" to \"cosine\"\n",
    "    warmup_ratio=0.05,                     # \u2190 CHANGED from 0.1 to 0.05\n",
    "\n",
    "    # Optimization\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    fp16=True,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # Logging and saving\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    torch_empty_cache_steps=10,\n",
    "    # evaluation disabled by default\n",
    "\n",
    "    # Evaluation (none for now)\n",
    "\n",
    "    # Performance\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\\n\u2705 Trainer configured with OPTIMIZED hyperparameters:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Epochs:                  {training_args.num_train_epochs}\")\n",
    "print(f\"  Learning rate:           {training_args.learning_rate}\")\n",
    "print(f\"  LR scheduler:            {training_args.lr_scheduler_type}\")\n",
    "print(f\"  Warmup ratio:            {training_args.warmup_ratio}\")\n",
    "print(f\"  Gradient accumulation:   {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Batch size per device:   {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Effective batch size:    {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Model\n",
    "\n",
    "**Expected time**: 30-45 minutes on T4 GPU  \n",
    "**Expected final loss**: < 0.7 (ideally ~0.5-0.6)\n",
    "\n",
    "**What to watch for**:\n",
    "- Loss should decrease steadily from ~1.5-2.0 to ~0.5-0.8\n",
    "- Learning rate follows cosine curve (smooth decay)\n",
    "- No OOM errors (if OOM, reduce batch size to 1 and increase grad_accum to 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\n\ud83d\ude80 Starting training...\\n\")\n",
    "print(\"=\"*80)\n",
    "trainer.train()\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\u2705 Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter\n",
    "output_dir = str(ADAPTER_DIR)\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"\\n\u2705 Adapter saved to {output_dir}\")\n",
    "print(\"\\nFiles:\")\n",
    "!ls -lh {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Adapter\n",
    "\n",
    "Choose one of the methods below to download the adapter to your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opcional: Guardar resultados y subir cambios al repositorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar identidad git (ejecutar una vez por sesi\u00f3n)\n",
    "!git config --global user.name \"TU_NOMBRE\"\n",
    "!git config --global user.email \"tu.email@example.com\"\n",
    "\n",
    "# Verificar estado\n",
    "!git status -sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiar resultados (adapter y logs) hacia el repositorio local\n",
    "from pathlib import Path\n",
    "\n",
    "repo_dir = Path.cwd()\n",
    "adapter_src = ADAPTER_DIR\n",
    "adapter_dst = repo_dir / 'artifacts/models/po_student_v2_adapter'\n",
    "\n",
    "if adapter_src.exists():\n",
    "    !rm -rf {adapter_dst}\n",
    "    !cp -r {adapter_src} {adapter_dst}\n",
    "    print(f\"\u2705 Adapter copiado a {adapter_dst}\")\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f Adapter no encontrado en {adapter_src}\")\n",
    "\n",
    "logs_dst = repo_dir / 'logs/distillation'\n",
    "logs_dst.mkdir(parents=True, exist_ok=True)\n",
    "print(\"\u2705 Carpeta de logs preparada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Direct upload (Colab only)\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    from google.colab import files  # type: ignore\n",
    "    uploaded = files.upload()\n",
    "    print('Sube po_teacher_supervised.jsonl y se guardar\u00e1 en /content/')\n",
    "except Exception:\n",
    "    print('\u26a0\ufe0f Esta opci\u00f3n solo est\u00e1 disponible en Colab. Usa Option B/C o coloca el archivo en DATASET_PATH manualmente.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Save to Google Drive (uncomment if using this option)\n",
    "# !cp -r /content/po_student_v2_adapter /content/drive/MyDrive/lora_adapters/\n",
    "# print(\"\u2705 Adapter saved to Google Drive: /content/drive/MyDrive/lora_adapters/po_student_v2_adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Verify Adapter (Optional)\n",
    "\n",
    "Quick sanity check to ensure the adapter loads correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Reload adapter to verify it works\n",
    "print(\"Loading adapter for verification...\")\n",
    "test_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    str(ADAPTER_DIR),\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\u2705 Adapter loaded successfully!\")\n",
    "print(\"Model is ready for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Training completed successfully!**\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Extract adapter** on local machine:\n",
    "   ```bash\n",
    "   unzip po_student_v2_adapter.zip -d artifacts/models/\n",
    "   ```\n",
    "\n",
    "2. **Run evaluation** (Step 4):\n",
    "   ```bash\n",
    "   PYTHONPATH=. .venv/bin/python scripts/eval_po_student.py \\\n",
    "     --adapter artifacts/models/po_student_v2_adapter \\\n",
    "     --max-samples 40 \\\n",
    "     --output inference_results/student_v2.json\n",
    "   ```\n",
    "\n",
    "3. **Compare results**:\n",
    "   - Check mean \u2265 0.82\n",
    "   - Check std \u2264 0.10\n",
    "   - Check delta vs baseline \u2264 0.03\n",
    "\n",
    "### Training Configuration Used:\n",
    "\n",
    "| Hyperparameter | Value |\n",
    "|----------------|-------|\n",
    "| Model | Qwen/Qwen2.5-7B-Instruct |\n",
    "| LoRA rank | 32 |\n",
    "| LoRA alpha | 64 |\n",
    "| Epochs | 4 |\n",
    "| Learning rate | 8e-5 |\n",
    "| LR scheduler | cosine |\n",
    "| Warmup ratio | 0.05 |\n",
    "| Batch size | 2 |\n",
    "| Gradient accumulation | 12 |\n",
    "| Effective batch size | 24 |\n",
    "| Dataset size | 359 samples |\n",
    "| Conflicts examples | 10 (2.8%) |"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}