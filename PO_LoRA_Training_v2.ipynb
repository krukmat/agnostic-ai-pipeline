{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Owner LoRA Training v2\n",
    "**Date**: 2025-11-15  \n",
    "**Task**: Train Qwen2.5-7B-Instruct with LoRA using optimized hyperparameters\n",
    "\n",
    "**Dataset**: 359 samples (aligned: 66.9%, needs: 30.4%, conflicts: 2.8%)  \n",
    "**Changes from v1**:\n",
    "- epochs: 3 ‚Üí **4**\n",
    "- learning_rate: 1e-4 ‚Üí **8e-5**\n",
    "- lr_scheduler: linear ‚Üí **cosine**\n",
    "- warmup_ratio: 0.1 ‚Üí **0.05**\n",
    "- gradient_accumulation: 8 ‚Üí **12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Clonar repositorio y montar Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clonar el repositorio en /content/agnostic-ai-pipeline\n",
    "!git clone -b dspy-multi-role https://github.com/krukmat/agnostic-ai-pipeline.git\n",
    "%cd agnostic-ai-pipeline\n",
    "\n",
    "print(\"\\nüìÅ Directorio actual:\")\n",
    "!pwd\n",
    "\n",
    "# Opcional: montar Google Drive para guardar artefactos persistentes\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úÖ Google Drive montado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q \\\n",
    "  transformers==4.36.0 \\\n",
    "  peft==0.7.0 \\\n",
    "  bitsandbytes==0.41.0 \\\n",
    "  accelerate==0.25.0 \\\n",
    "  datasets==2.16.0\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Dataset\n",
    "\n",
    "**Option A**: Click the folder icon on the left, then upload `po_teacher_supervised.jsonl`  \n",
    "**Option B**: Mount Google Drive and copy from there  \n",
    "**Option C**: Use the code below to upload directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Direct upload\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "# Upload: po_teacher_supervised.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: From Google Drive (uncomment if using this option)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !cp /content/drive/MyDrive/path/to/po_teacher_supervised.jsonl /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset uploaded\n",
    "!ls -lh /content/*.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load supervised dataset\n",
    "data = []\n",
    "with open(\"/content/po_teacher_supervised.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(data)} training examples\")\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_list(data)\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(train_dataset)\n",
    "\n",
    "# Verify data structure\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample prompt (first 300 chars):\")\n",
    "print(\"=\"*80)\n",
    "print(train_dataset[0][\"prompt\"][:300])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample response (first 300 chars):\")\n",
    "print(\"=\"*80)\n",
    "print(train_dataset[0][\"response\"][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(\"This will take 5-7 minutes...\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded: {model_name}\")\n",
    "print(f\"Model device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=32,                      # Rank\n",
    "    lora_alpha=64,             # Alpha\n",
    "    target_modules=[           # Target all linear layers\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,         # Dropout\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"\\n‚úÖ LoRA configuration applied\")\n",
    "print(\"\\nTrainable parameters:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize prompt/response pairs using chat template\"\"\"\n",
    "    full_texts = []\n",
    "    for prompt, response in zip(examples[\"prompt\"], examples[\"response\"]):\n",
    "        # Format as chat message\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        full_texts.append(text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        full_texts,\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    # Set labels (same as input_ids for causal LM)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "# Apply tokenization\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Tokenized dataset: {len(tokenized_dataset)} examples\")\n",
    "print(f\"Sample token count: {len(tokenized_dataset[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Configure Training Arguments (OPTIMIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir=\"/content/po_student_v2\",\n",
    "\n",
    "    # Training schedule (OPTIMIZED)\n",
    "    num_train_epochs=4,                    # ‚Üê CHANGED from 3 to 4\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=12,        # ‚Üê CHANGED from 8 to 12\n",
    "\n",
    "    # Learning rate (OPTIMIZED)\n",
    "    learning_rate=8e-5,                    # ‚Üê CHANGED from 1e-4 to 8e-5\n",
    "    lr_scheduler_type=\"cosine\",            # ‚Üê CHANGED from \"linear\" to \"cosine\"\n",
    "    warmup_ratio=0.05,                     # ‚Üê CHANGED from 0.1 to 0.05\n",
    "\n",
    "    # Optimization\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    fp16=True,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # Logging and saving\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # Evaluation (none for now)\n",
    "    evaluation_strategy=\"no\",\n",
    "\n",
    "    # Performance\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Trainer configured with OPTIMIZED hyperparameters:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Epochs:                  {training_args.num_train_epochs}\")\n",
    "print(f\"  Learning rate:           {training_args.learning_rate}\")\n",
    "print(f\"  LR scheduler:            {training_args.lr_scheduler_type}\")\n",
    "print(f\"  Warmup ratio:            {training_args.warmup_ratio}\")\n",
    "print(f\"  Gradient accumulation:   {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Batch size per device:   {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Effective batch size:    {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Model\n",
    "\n",
    "**Expected time**: 30-45 minutes on T4 GPU  \n",
    "**Expected final loss**: < 0.7 (ideally ~0.5-0.6)\n",
    "\n",
    "**What to watch for**:\n",
    "- Loss should decrease steadily from ~1.5-2.0 to ~0.5-0.8\n",
    "- Learning rate follows cosine curve (smooth decay)\n",
    "- No OOM errors (if OOM, reduce batch size to 1 and increase grad_accum to 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\nüöÄ Starting training...\\n\")\n",
    "print(\"=\"*80)\n",
    "trainer.train()\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter\n",
    "output_dir = \"/content/po_student_v2_adapter\"\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"\\n‚úÖ Adapter saved to {output_dir}\")\n",
    "print(\"\\nFiles:\")\n",
    "!ls -lh {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Adapter\n",
    "\n",
    "Choose one of the methods below to download the adapter to your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opcional: Guardar resultados y subir cambios al repositorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar identidad git (ejecutar una vez por sesi√≥n)\n",
    "!git config --global user.name \"TU_NOMBRE\"\n",
    "!git config --global user.email \"tu.email@example.com\"\n",
    "\n",
    "# Verificar estado\n",
    "!git status -sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiar resultados (adapter y logs) hacia el repositorio local\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n\",\n    "repo_dir = Path('/content/agnostic-ai-pipeline')\n",
    "adapter_src = Path('/content/po_student_v2_adapter')\n",
    "adapter_dst = repo_dir / 'artifacts/models/po_student_v2_adapter'\n",
    "\n",
    "if adapter_src.exists():\n",
    "    !rm -rf {adapter_dst}\n",
    "    !cp -r {adapter_src} {adapter_dst}\n",
    "    print(f\"‚úÖ Adapter copiado a {adapter_dst}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Adapter no encontrado en /content/po_student_v2_adapter\")\n",
    "\n",
    "# (Opcional) Guardar logs adicionales aqu√≠ si aplica\n",
    "logs_dst = repo_dir / 'logs/distillation'\n",
    "logs_dst.mkdir(parents=True, exist_ok=True)\n",
    "# Ejemplo: copiar log de entrenamiento si se produjo\n",
    "# !cp /content/agnostic-ai-pipeline/logs/distillation/train_po_student_v2.log {logs_dst}/\n",
    "print(\"‚úÖ Carpeta de logs preparada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "cd /content/agnostic-ai-pipeline\n",
    "\n",
    "git status -sb\n",
    "\n",
    "# Agregar cambios relevantes (adapter + logs + inferencias)\n",
    "git add artifacts/models/po_student_v2_adapter || true\n",
    "git add logs/distillation || true\n",
    "git add inference_results || true\n",
    "\n",
    "# Crear commit (usar mensaje adecuado)\n",
    "git commit -m \"chore(po): add po_student_v2 adapter and logs\" || echo \"‚ö†Ô∏è Nada que commitear\"\n",
    "\n",
    "# Push al branch actual (requiere credenciales PAT)\n",
    "BRANCH=$(git branch --show-current)\n",
    "git push origin \"$BRANCH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Compress and download via browser\n",
    "!zip -r po_student_v2_adapter.zip /content/po_student_v2_adapter\n",
    "\n",
    "from google.colab import files\n",
    "files.download('/content/po_student_v2_adapter.zip')\n",
    "\n",
    "print(\"\\n‚úÖ Download started (check browser downloads)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Save to Google Drive (uncomment if using this option)\n",
    "# !cp -r /content/po_student_v2_adapter /content/drive/MyDrive/lora_adapters/\n",
    "# print(\"‚úÖ Adapter saved to Google Drive: /content/drive/MyDrive/lora_adapters/po_student_v2_adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Verify Adapter (Optional)\n",
    "\n",
    "Quick sanity check to ensure the adapter loads correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Reload adapter to verify it works\n",
    "print(\"Loading adapter for verification...\")\n",
    "test_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"/content/po_student_v2_adapter\",\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Adapter loaded successfully!\")\n",
    "print(\"Model is ready for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Training completed successfully!**\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Extract adapter** on local machine:\n",
    "   ```bash\n",
    "   unzip po_student_v2_adapter.zip -d artifacts/models/\n",
    "   ```\n",
    "\n",
    "2. **Run evaluation** (Step 4):\n",
    "   ```bash\n",
    "   PYTHONPATH=. .venv/bin/python scripts/eval_po_student.py \\\n",
    "     --adapter artifacts/models/po_student_v2_adapter \\\n",
    "     --max-samples 40 \\\n",
    "     --output inference_results/student_v2.json\n",
    "   ```\n",
    "\n",
    "3. **Compare results**:\n",
    "   - Check mean ‚â• 0.82\n",
    "   - Check std ‚â§ 0.10\n",
    "   - Check delta vs baseline ‚â§ 0.03\n",
    "\n",
    "### Training Configuration Used:\n",
    "\n",
    "| Hyperparameter | Value |\n",
    "|----------------|-------|\n",
    "| Model | Qwen/Qwen2.5-7B-Instruct |\n",
    "| LoRA rank | 32 |\n",
    "| LoRA alpha | 64 |\n",
    "| Epochs | 4 |\n",
    "| Learning rate | 8e-5 |\n",
    "| LR scheduler | cosine |\n",
    "| Warmup ratio | 0.05 |\n",
    "| Batch size | 2 |\n",
    "| Gradient accumulation | 12 |\n",
    "| Effective batch size | 24 |\n",
    "| Dataset size | 359 samples |\n",
    "| Conflicts examples | 10 (2.8%) |"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
