{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Product Owner LoRA Model - Inference Testing\n\nEste notebook prueba el modelo Product Owner entrenado con LoRA.\n\n**Requisitos**:\n- Google Colab con GPU (T4 Free)\n- **OpciÃ³n A**: Modelo en Google Drive: `/content/drive/MyDrive/colab_models/po_student_v1`\n- **OpciÃ³n B**: El notebook clonarÃ¡ automÃ¡ticamente el repositorio si el modelo no estÃ¡ en Drive\n\n**Pasos**:\n1. Verificar GPU\n2. Instalar dependencias\n3. Montar Google Drive (opcional) o clonar repositorio\n4. Probar modelo baseline (sin LoRA)\n5. Probar modelo fine-tuned (con LoRA)\n6. Comparar resultados"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verificar GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Instalar Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO reinstalar PyTorch - Colab ya tiene PyTorch con CUDA 12.x\n",
    "!pip install -q transformers>=4.40.0 peft>=0.10.0 bitsandbytes>=0.43.0 accelerate>=0.28.0\n",
    "\n",
    "# Verificar instalaciÃ³n\n",
    "import transformers\n",
    "import peft\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "\n",
    "print(\"âœ… All packages installed successfully\")\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"peft: {peft.__version__}\")\n",
    "print(f\"bitsandbytes: {bitsandbytes.__version__}\")\n",
    "print(f\"accelerate: {accelerate.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Cargar Modelo LoRA\n\nBusca el modelo en Drive, local, o clona el repositorio automÃ¡ticamente."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pathlib import Path\n\n# 1. Montar Google Drive (opcional - si falla, usamos el repo)\nprint(\"ðŸ”„ Intentando montar Google Drive...\")\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive', force_remount=False)\n    \n    if os.path.exists('/content/drive/MyDrive'):\n        print(\"âœ… Google Drive montado correctamente\\n\")\n        drive_available = True\n    else:\n        print(\"âš ï¸  Drive montado pero MyDrive no accesible\\n\")\n        drive_available = False\nexcept Exception as e:\n    print(f\"âš ï¸  No se pudo montar Drive: {e}\")\n    print(\"ðŸ“¦ Usaremos el modelo desde el repositorio\\n\")\n    drive_available = False\n\n# 2. Buscar el modelo en ubicaciones posibles\nprint(\"ðŸ” Buscando modelo LoRA entrenado...\")\n\npossible_locations = [\n    \"/content/drive/MyDrive/colab_models/po_student_v1\",  # Drive principal\n    \"/content/drive/MyDrive/po_student_v1\",               # Drive alternativo\n    \"/content/po_student_v1\",                              # Local en Colab\n    \"/content/agnostic-ai-pipeline/artifacts/models/po_student_v1\",  # Repo clonado\n]\n\nmodel_path = None\nfor path in possible_locations:\n    if os.path.exists(path):\n        model_path = path\n        print(f\"âœ… Modelo encontrado en: {model_path}\")\n        break\n\n# 3. Si no se encontrÃ³ el modelo, clonar el repositorio\nif model_path is None:\n    print(\"âŒ Modelo no encontrado en Drive ni local\")\n    print(\"\\nðŸ“¥ Clonando repositorio con el modelo...\")\n    \n    repo_url = \"https://github.com/matiasleandrokruk/agnostic-ai-pipeline.git\"\n    repo_path = \"/content/agnostic-ai-pipeline\"\n    \n    if not os.path.exists(repo_path):\n        !git clone --depth 1 {repo_url} {repo_path}\n        print(f\"âœ… Repositorio clonado en: {repo_path}\")\n    else:\n        print(f\"âœ… Repositorio ya existe en: {repo_path}\")\n    \n    # Verificar si el modelo estÃ¡ en el repo\n    model_path = f\"{repo_path}/artifacts/models/po_student_v1\"\n    \n    if os.path.exists(model_path):\n        print(f\"âœ… Modelo encontrado en el repositorio: {model_path}\")\n    else:\n        print(f\"\\nâŒ ERROR: Modelo no encontrado ni en Drive ni en el repositorio\")\n        print(\"\\nðŸ“ Ubicaciones buscadas:\")\n        for path in possible_locations + [model_path]:\n            print(f\"  - {path}\")\n        print(\"\\nðŸ’¡ Soluciones:\")\n        print(\"1. Verifica que el modelo estÃ¡ en el repositorio en: artifacts/models/po_student_v1/\")\n        print(\"2. O copia el modelo a Google Drive: /content/drive/MyDrive/colab_models/po_student_v1\")\n        print(\"3. O ejecuta primero el training notebook para generar el modelo\")\n        raise FileNotFoundError(\"Modelo LoRA no encontrado en ninguna ubicaciÃ³n\")\n\n# 4. Verificar archivos crÃ­ticos del modelo\nprint(f\"\\nðŸ“‚ Contenido del modelo:\")\n!ls -lh {model_path}\n\nrequired_files = [\"adapter_config.json\", \"adapter_model.safetensors\"]\nmissing_files = []\n\nfor file in required_files:\n    file_path = os.path.join(model_path, file)\n    if not os.path.exists(file_path):\n        missing_files.append(file)\n    else:\n        file_size = os.path.getsize(file_path) / 1024**2  # MB\n        print(f\"  âœ“ {file} ({file_size:.1f} MB)\")\n\nif missing_files:\n    print(f\"\\nâš ï¸  ADVERTENCIA: Faltan archivos del modelo: {missing_files}\")\n    print(\"El modelo podrÃ­a estar incompleto. Verifica que el training terminÃ³ correctamente.\")\nelse:\n    print(\"\\nâœ… Todos los archivos del modelo estÃ¡n presentes\")\n\n# 5. Guardar el path del modelo para usar despuÃ©s\nMODEL_PATH = model_path\nprint(f\"\\nâœ… Listo para usar el modelo desde: {MODEL_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Definir Test Cases\n",
    "\n",
    "Test cases con prompts realistas de Product Owner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CASES = [\n",
    "    {\n",
    "        \"name\": \"basic_blog_validation\",\n",
    "        \"prompt\": \"\"\"[INSTRUCTIONS]\n",
    "You are a Product Owner reviewing requirements from a Business Analyst.\n",
    "Validate the requirements, identify missing details, and provide feedback.\n",
    "\n",
    "[REQUIREMENTS]\n",
    "Title: Simple Blog Platform\n",
    "Description: Users should be able to create and view blog posts.\n",
    "\n",
    "Features:\n",
    "- Users can create blog posts\n",
    "- Posts have title and content\n",
    "- Users can view all published posts\n",
    "\n",
    "[YOUR RESPONSE]\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ecommerce_requirements\",\n",
    "        \"prompt\": \"\"\"[INSTRUCTIONS]\n",
    "You are a Product Owner reviewing requirements from a Business Analyst.\n",
    "Validate the requirements, identify missing details, and provide feedback.\n",
    "\n",
    "[REQUIREMENTS]\n",
    "Title: E-commerce Product Catalog\n",
    "Description: Build a product catalog for an online store.\n",
    "\n",
    "Features:\n",
    "- Display list of products with images\n",
    "- Show product details (name, price, description)\n",
    "- Search products by name\n",
    "- Filter by category\n",
    "\n",
    "[YOUR RESPONSE]\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"incomplete_requirements\",\n",
    "        \"prompt\": \"\"\"[INSTRUCTIONS]\n",
    "You are a Product Owner reviewing requirements from a Business Analyst.\n",
    "Validate the requirements, identify missing details, and provide feedback.\n",
    "\n",
    "[REQUIREMENTS]\n",
    "Title: Social Media Integration\n",
    "Description: Add social media features.\n",
    "\n",
    "Features:\n",
    "- Share posts\n",
    "- Like content\n",
    "\n",
    "[YOUR RESPONSE]\"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"ðŸ“ {len(TEST_CASES)} test cases defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Funciones de Carga y GeneraciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "def load_model(base_model_name, adapter_path=None, load_4bit=True):\n",
    "    \"\"\"\n",
    "    Carga el modelo base o base + LoRA adapter.\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: Nombre del modelo base en HuggingFace\n",
    "        adapter_path: Path al adapter LoRA (None para baseline)\n",
    "        load_4bit: Usar quantization 4-bit\n",
    "    \n",
    "    Returns:\n",
    "        (tokenizer, model)\n",
    "    \"\"\"\n",
    "    print(f\"[load_model] Cargando base model: {base_model_name}...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model_kwargs = {\"device_map\": \"auto\"}\n",
    "    \n",
    "    if load_4bit:\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "        model_kwargs.update(\n",
    "            quantization_config=quant_config,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "    else:\n",
    "        model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model_name, **model_kwargs)\n",
    "    \n",
    "    if adapter_path:\n",
    "        print(f\"[load_model] Cargando LoRA adapter desde: {adapter_path}...\")\n",
    "        model = PeftModel.from_pretrained(model, adapter_path)\n",
    "        print(\"[load_model] âœ… LoRA adapter cargado\")\n",
    "    else:\n",
    "        print(\"[load_model] Modo baseline (sin adapter)\")\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def generate_response(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    prompt,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera respuesta para un prompt.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: Tokenizer instance\n",
    "        model: Model instance\n",
    "        prompt: Input prompt\n",
    "        max_new_tokens: MÃ¡ximo de tokens a generar\n",
    "        temperature: Temperatura de sampling\n",
    "        top_p: Nucleus sampling parameter\n",
    "    \n",
    "    Returns:\n",
    "        Generated text\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "    \n",
    "    # Mover inputs al device del modelo\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decodificar y extraer solo la parte generada (skip input prompt)\n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Intentar extraer solo la respuesta despuÃ©s del prompt\n",
    "    if \"[YOUR RESPONSE]\" in full_text:\n",
    "        response = full_text.split(\"[YOUR RESPONSE]\", 1)[1].strip()\n",
    "    else:\n",
    "        # Fallback: remover el tamaÃ±o del prompt original\n",
    "        response = full_text[len(prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"âœ… Funciones definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Probar Modelo Baseline (sin LoRA)\n",
    "\n",
    "Primero probamos el modelo base Qwen2.5-7B sin fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n\nprint(\"ðŸ”„ Cargando modelo BASELINE (sin LoRA)...\\n\")\ntry:\n    tokenizer_baseline, model_baseline = load_model(BASE_MODEL, adapter_path=None)\nexcept Exception as e:\n    print(f\"âŒ ERROR cargando modelo: {e}\")\n    print(\"\\nPosibles soluciones:\")\n    print(\"1. Verifica que tienes suficiente RAM/VRAM disponible\")\n    print(\"2. Reinicia el runtime: Runtime â†’ Restart runtime\")\n    print(\"3. Verifica que Runtime type = T4 GPU\")\n    raise\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TESTING BASELINE MODEL\")\nprint(\"=\"*80 + \"\\n\")\n\nbaseline_results = []\n\nfor idx, test_case in enumerate(TEST_CASES):\n    print(f\"\\n--- Test Case {idx + 1}/{len(TEST_CASES)}: {test_case['name']} ---\\n\")\n    \n    prompt = test_case[\"prompt\"]\n    print(\"[Prompt Preview]\")\n    print(prompt[:150] + \"...\")\n    \n    print(\"\\n[Generando respuesta...]\")\n    try:\n        response = generate_response(tokenizer_baseline, model_baseline, prompt)\n        print(\"\\n[Response]\")\n        print(response)\n    except Exception as e:\n        print(f\"\\nâŒ ERROR generando respuesta: {e}\")\n        response = f\"[ERROR: {str(e)}]\"\n    \n    print(\"\\n\" + \"=\"*80)\n    \n    baseline_results.append({\n        \"test_case\": test_case[\"name\"],\n        \"prompt\": prompt,\n        \"response\": response,\n        \"response_length\": len(response),\n    })\n\nprint(f\"\\nâœ… Baseline testing completado: {len(baseline_results)} test cases\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Liberar Memoria del Modelo Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liberar memoria antes de cargar el modelo fine-tuned\n",
    "import gc\n",
    "\n",
    "del model_baseline\n",
    "del tokenizer_baseline\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"âœ… Memoria liberada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Probar Modelo Fine-tuned (con LoRA)\n",
    "\n",
    "Ahora probamos el modelo con el adapter LoRA entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n\n# Usar la variable MODEL_PATH definida en la celda de montaje de Drive\n# Si no existe (porque ejecutaste esta celda directamente), usar path por defecto\ntry:\n    ADAPTER_PATH = MODEL_PATH\n    print(f\"âœ… Usando modelo desde: {ADAPTER_PATH}\")\nexcept NameError:\n    ADAPTER_PATH = \"/content/drive/MyDrive/colab_models/po_student_v1\"\n    print(f\"âš ï¸  MODEL_PATH no definido, usando path por defecto: {ADAPTER_PATH}\")\n    if not os.path.exists(ADAPTER_PATH):\n        raise FileNotFoundError(f\"Modelo no encontrado en {ADAPTER_PATH}. Ejecuta primero la celda 3 (Montar Google Drive)\")\n\nprint(\"\\nðŸ”„ Cargando modelo FINE-TUNED (con LoRA)...\\n\")\ntokenizer_ft, model_ft = load_model(BASE_MODEL, adapter_path=ADAPTER_PATH)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TESTING FINE-TUNED MODEL\")\nprint(\"=\"*80 + \"\\n\")\n\nfinetuned_results = []\n\nfor idx, test_case in enumerate(TEST_CASES):\n    print(f\"\\n--- Test Case {idx + 1}/{len(TEST_CASES)}: {test_case['name']} ---\\n\")\n    \n    prompt = test_case[\"prompt\"]\n    print(\"[Prompt Preview]\")\n    print(prompt[:150] + \"...\")\n    \n    print(\"\\n[Generando respuesta...]\")\n    try:\n        response = generate_response(tokenizer_ft, model_ft, prompt)\n        print(\"\\n[Response]\")\n        print(response)\n    except Exception as e:\n        print(f\"\\nâŒ ERROR generando respuesta: {e}\")\n        response = f\"[ERROR: {str(e)}]\"\n    \n    print(\"\\n\" + \"=\"*80)\n    \n    finetuned_results.append({\n        \"test_case\": test_case[\"name\"],\n        \"prompt\": prompt,\n        \"response\": response,\n        \"response_length\": len(response),\n    })\n\nprint(f\"\\nâœ… Fine-tuned testing completado: {len(finetuned_results)} test cases\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ComparaciÃ³n de Resultados\n",
    "\n",
    "Comparamos las respuestas de ambos modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for idx, (base_result, ft_result) in enumerate(zip(baseline_results, finetuned_results)):\n",
    "    print(f\"\\n### Test Case {idx + 1}: {base_result['test_case']}\")\n",
    "    print(\"\\n#### Baseline Model:\")\n",
    "    print(base_result[\"response\"][:200] + \"...\" if len(base_result[\"response\"]) > 200 else base_result[\"response\"])\n",
    "    print(f\"\\nLength: {base_result['response_length']} chars\")\n",
    "    \n",
    "    print(\"\\n#### Fine-tuned Model:\")\n",
    "    print(ft_result[\"response\"][:200] + \"...\" if len(ft_result[\"response\"]) > 200 else ft_result[\"response\"])\n",
    "    print(f\"\\nLength: {ft_result['response_length']} chars\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "\n",
    "# EstadÃ­sticas\n",
    "avg_baseline_length = sum(r[\"response_length\"] for r in baseline_results) / len(baseline_results)\n",
    "avg_ft_length = sum(r[\"response_length\"] for r in finetuned_results) / len(finetuned_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total test cases: {len(baseline_results)}\")\n",
    "print(f\"Average baseline response length: {avg_baseline_length:.0f} chars\")\n",
    "print(f\"Average fine-tuned response length: {avg_ft_length:.0f} chars\")\n",
    "print(f\"Difference: {avg_ft_length - avg_baseline_length:+.0f} chars ({(avg_ft_length / avg_baseline_length - 1) * 100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Guardar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Guardar resultados en Drive\n",
    "output_dir = \"/content/drive/MyDrive/colab_models/inference_results\"\n",
    "!mkdir -p {output_dir}\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Baseline results\n",
    "baseline_file = f\"{output_dir}/baseline_{timestamp}.json\"\n",
    "with open(baseline_file, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"model\": BASE_MODEL,\n",
    "        \"adapter_path\": None,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"results\": baseline_results,\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Baseline results saved to: {baseline_file}\")\n",
    "\n",
    "# Fine-tuned results\n",
    "finetuned_file = f\"{output_dir}/finetuned_{timestamp}.json\"\n",
    "with open(finetuned_file, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"model\": BASE_MODEL,\n",
    "        \"adapter_path\": ADAPTER_PATH,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"results\": finetuned_results,\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Fine-tuned results saved to: {finetuned_file}\")\n",
    "\n",
    "# Comparison summary\n",
    "comparison_file = f\"{output_dir}/comparison_{timestamp}.json\"\n",
    "with open(comparison_file, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"timestamp\": timestamp,\n",
    "        \"baseline_avg_length\": avg_baseline_length,\n",
    "        \"finetuned_avg_length\": avg_ft_length,\n",
    "        \"difference\": avg_ft_length - avg_baseline_length,\n",
    "        \"difference_percent\": (avg_ft_length / avg_baseline_length - 1) * 100,\n",
    "        \"test_cases\": len(baseline_results),\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Comparison summary saved to: {comparison_file}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… TESTING COMPLETO\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}