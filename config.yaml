providers:
  ollama:
    type: ollama
    base_url: http://localhost:11434
  openai:
    type: openai
    # si quisieras usar un proxy OpenAI-compat:
    # base_url: http://localhost:4010/v1
roles:
  ba:
    provider: ollama
    model: granite:7b
    temperature: 0.4
    max_tokens: 4096
  architect:
    provider: ollama
    model: qwen2.5-coder:32b-instruct-q8_0  # Más capacidad técnica (32B) con precisión (Q8)
    temperature: 0.1  # Bajísimo para especificaciones técnicas precisas y reproducibles
    max_tokens: 8192  # Más tokens para especificaciones ultra-detalladas
  dev:
    provider: ollama
    model: qwen2.5-coder:7b
    temperature: 0.2
    max_tokens: 2048
  qa:
    provider: ollama
    model: qwen2.5-coder:7b
    temperature: 0.1
    max_tokens: 1536
