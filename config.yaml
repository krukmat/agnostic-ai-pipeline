providers:
  ollama:
    type: ollama
    base_url: http://localhost:11434
  openai:
    type: openai
  codex_cli:
    type: codex_cli
    command:
    - codex
    - exec
    cwd: .
    timeout: 300
    input_format: stdin_text
    output_clean: true
    extra_args:
    - -c
    - sandbox_permissions=["disk-full-access"]
    append_temperature: false
    append_max_tokens: false
  claude_cli:
    type: claude_cli
    command:
    - claude
    - -p
    - --print
    - --output-format
    - json
    cwd: .
    timeout: 600
    input_format: stdin_text
    output_clean: true
    parse_json: true
    append_system_prompt: true
    extra_args: []
    debug: true
    debug_args:
    - --verbose
    - --debug
    log_stderr: true
  vertex_cli:
    type: vertex_cli
    project_id: agnostic-pipeline-478600
    location: europe-west1
    model: gemini-2.5-flash
    temperature: 0.2
    max_output_tokens: 2048
  vertex_sdk:
    type: vertex_sdk
    project_id: agnostic-pipeline-478600
    location: europe-west1
    model: gemini-2.5-flash
    temperature: 0.2
    max_output_tokens: 2048
roles:
  ba:
    provider: ollama
    model: granite4
    temperature: 0.4
    max_tokens: 8192
    top_p: 0.95
  architect:
    provider: vertex_sdk
    model: gemini-2.5-flash
    temperature: 0.2
    max_tokens: 12000
    top_p: 0.95
    output_caps:
      stories:
        tokens: 2000      # increased to reduce JSON truncation (next runs)
        min_tokens: 600
      architecture:
        tokens: 2000
        min_tokens: 600
  dev:
    provider: vertex_sdk
    model: gemini-2.5-pro
    temperature: 0.2
    max_tokens: 8192
    top_p: 0.5
    backup_models:
    - provider: codex_cli
      model: default
      reason: Codex CLI for structured output fallback
      cost_tier: high
      specialties:
      - structured_output
      - code_generation
    - provider: ollama
      model: qwen2.5-coder:32b
      reason: Code specialist, runs locally
      cost_tier: free
      specialties:
      - code_generation
      - local_execution
    - provider: vertex_cli
      model: gemini-2.5-pro
      reason: Alternative Gemini access method
      cost_tier: medium
      specialties:
      - general_purpose
  qa:
    provider: vertex_cli
    model: gemini-2.5-pro
    temperature: 0.5
    max_tokens: 8192
    top_p: 0.5
  product_owner:
    provider: vertex_sdk
    model: gemini-2.5-flash
    temperature: 0.6
    max_tokens: 8192
    top_p: 0.95
pipeline:
  force_approval_attempts: 3
  max_recovery_attempts: 2
  auto_recovery_strategy: smart
  model_fallback:
    enabled: true
    auto_suggest: true
    allow_cost_increase: false
    prefer_local: true
a2a:
  execution_mode: local
  agents:
    business_analyst:
      url: http://localhost:8001/
      capabilities:
        streaming: false
      skills: []
      strategy: auto
    product_owner:
      url: http://localhost:8002/
      capabilities:
        streaming: false
      skills: []
      strategy: auto
    architect:
      url: http://localhost:8003/
      capabilities:
        streaming: false
      skills: []
      strategy: auto
    developer:
      url: http://localhost:8004/
      capabilities:
        streaming: false
      skills: []
      strategy: auto
    qa:
      url: http://localhost:8005/
      capabilities:
        streaming: false
      skills: []
      strategy: auto
    orchestrator:
      url: http://localhost:8010/
      capabilities:
        streaming: false
      skills: []
      strategy: auto
  authentication:
    mode: none
features:
  use_dspy_ba: true
  use_dspy_product_owner: true
  use_dspy_architect: false
  architect:
    arch_only: false
    normalize_minified_arch: true
    use_optimized_prompt: false
    prompt_override_file: ""
