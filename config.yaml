providers:
  ollama:
    type: ollama
    base_url: http://localhost:11434
  openai:
    type: openai
    # si quisieras usar un proxy OpenAI-compat:
    # base_url: http://localhost:4010/v1
roles:
  ba:
    provider: ollama
    model: granite:7b
    temperature: 0.4
    max_tokens: 4096
  architect:
    provider: ollama
    model: mistral:7b-instruct
    temperature: 0.3
    max_tokens: 3072
  dev:
    provider: ollama
    model: qwen2.5-coder:7b
    temperature: 0.2
    max_tokens: 2048
  qa:
    provider: ollama
    model: qwen2.5-coder:7b
    temperature: 0.1
    max_tokens: 1536
