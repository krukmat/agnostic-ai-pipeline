{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Owner LoRA Model - Evaluation (Baseline vs Student)\n",
    "\n",
    "Este notebook eval√∫a el modelo Product Owner entrenado con LoRA contra el baseline.\n",
    "\n",
    "**Requisitos**:\n",
    "- Lightning AI Studio con GPU (T4 gratuita, 10h/mes)\n",
    "- El notebook clona autom√°ticamente el repositorio\n",
    "\n",
    "**Setup inicial**:\n",
    "1. Ir a [lightning.ai](https://lightning.ai)\n",
    "2. Crear cuenta gratuita\n",
    "3. New Studio ‚Üí Python\n",
    "4. Subir este notebook\n",
    "5. Start ‚Üí Seleccionar GPU (T4)\n",
    "\n",
    "**Pasos**:\n",
    "1. Verificar GPU\n",
    "2. Instalar dependencias\n",
    "3. Clonar repositorio con el modelo LoRA\n",
    "4. Ejecutar evaluaci√≥n baseline (Qwen2.5-7B sin LoRA)\n",
    "5. Ejecutar evaluaci√≥n student (Qwen2.5-7B + LoRA)\n",
    "6. Comparar resultados\n",
    "7. Guardar resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verificar GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 14 21:52:59 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   19C    P8              8W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Instalar Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install -q transformers>=4.36.0 peft>=0.7.0 bitsandbytes>=0.41.0 accelerate>=0.25.0 torch typer pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clonar Repositorio y Verificar Modelo LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Clonando repositorio con el modelo LoRA...\n",
      "‚úÖ Repositorio ya existe en: /teamspace/studios/this_studio/agnostic-ai-pipeline\n",
      "‚úÖ Modelo encontrado en: /teamspace/studios/this_studio/agnostic-ai-pipeline/artifacts/models/po_student_v1\n",
      "‚úÖ Dataset de validaci√≥n encontrado: /teamspace/studios/this_studio/agnostic-ai-pipeline/artifacts/synthetic/product_owner/product_owner_val.jsonl\n",
      "\n",
      "üìÇ Contenido del modelo:\n",
      "total 93M\n",
      "-rw-r--r-- 1 krukmatias krukmatias 5.1K Nov 14 21:47 README.md\n",
      "-rw-r--r-- 1 krukmatias krukmatias  887 Nov 14 21:47 adapter_config.json\n",
      "-rw-r--r-- 1 krukmatias krukmatias  78M Nov 14 21:47 adapter_model.safetensors\n",
      "-rw-r--r-- 1 krukmatias krukmatias  605 Nov 14 21:47 added_tokens.json\n",
      "-rw-r--r-- 1 krukmatias krukmatias 2.5K Nov 14 21:47 chat_template.jinja\n",
      "-rw-r--r-- 1 krukmatias krukmatias 1.6M Nov 14 21:47 merges.txt\n",
      "-rw-r--r-- 1 krukmatias krukmatias  613 Nov 14 21:47 special_tokens_map.json\n",
      "-rw-r--r-- 1 krukmatias krukmatias  11M Nov 14 21:47 tokenizer.json\n",
      "-rw-r--r-- 1 krukmatias krukmatias 4.6K Nov 14 21:47 tokenizer_config.json\n",
      "-rw-r--r-- 1 krukmatias krukmatias 2.7M Nov 14 21:47 vocab.json\n",
      "  ‚úì adapter_config.json (0.0 MB)\n",
      "  ‚úì adapter_model.safetensors (77.0 MB)\n",
      "  ‚úì tokenizer_config.json (0.0 MB)\n",
      "\n",
      "‚úÖ Todos los archivos del modelo est√°n presentes\n",
      "\n",
      "‚úÖ Working directory: /teamspace/studios/this_studio/agnostic-ai-pipeline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Clonar repositorio con el modelo\n",
    "print(\"üì• Clonando repositorio con el modelo LoRA...\")\n",
    "\n",
    "repo_url = \"https://github.com/krukmat/agnostic-ai-pipeline.git\"\n",
    "repo_branch = \"dspy-multi-role\"\n",
    "repo_path = \"/teamspace/studios/this_studio/agnostic-ai-pipeline\"\n",
    "\n",
    "if not os.path.exists(repo_path):\n",
    "    !git clone --depth 1 --branch {repo_branch} {repo_url} {repo_path}\n",
    "    print(f\"‚úÖ Repositorio clonado (branch: {repo_branch})\")\n",
    "else:\n",
    "    print(f\"‚úÖ Repositorio ya existe en: {repo_path}\")\n",
    "\n",
    "# 2. Verificar que el modelo est√° en el repo\n",
    "model_path = f\"{repo_path}/artifacts/models/po_student_v1\"\n",
    "valset_path = f\"{repo_path}/artifacts/synthetic/product_owner/product_owner_val.jsonl\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"\\n‚ùå ERROR: Modelo no encontrado en: {model_path}\")\n",
    "    raise FileNotFoundError(\"Modelo LoRA no encontrado en el repositorio\")\n",
    "\n",
    "if not os.path.exists(valset_path):\n",
    "    print(f\"\\n‚ùå ERROR: Dataset de validaci√≥n no encontrado en: {valset_path}\")\n",
    "    raise FileNotFoundError(\"Dataset de validaci√≥n no encontrado\")\n",
    "\n",
    "print(f\"‚úÖ Modelo encontrado en: {model_path}\")\n",
    "print(f\"‚úÖ Dataset de validaci√≥n encontrado: {valset_path}\")\n",
    "\n",
    "# 3. Verificar archivos cr√≠ticos del modelo\n",
    "print(f\"\\nüìÇ Contenido del modelo:\")\n",
    "!ls -lh {model_path}\n",
    "\n",
    "required_files = [\"adapter_config.json\", \"adapter_model.safetensors\", \"tokenizer_config.json\"]\n",
    "missing_files = []\n",
    "\n",
    "for file in required_files:\n",
    "    file_path = os.path.join(model_path, file)\n",
    "    if not os.path.exists(file_path):\n",
    "        missing_files.append(file)\n",
    "    else:\n",
    "        file_size = os.path.getsize(file_path) / 1024**2  # MB\n",
    "        print(f\"  ‚úì {file} ({file_size:.1f} MB)\")\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n‚ö†Ô∏è  ADVERTENCIA: Faltan archivos del modelo: {missing_files}\")\n",
    "    raise FileNotFoundError(f\"Archivos cr√≠ticos faltantes: {missing_files}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Todos los archivos del modelo est√°n presentes\")\n",
    "\n",
    "# 4. Cambiar al directorio del repo\n",
    "os.chdir(repo_path)\n",
    "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluaci√≥n Baseline (Qwen2.5-7B sin LoRA)\n",
    "\n",
    "Esta evaluaci√≥n usa el modelo base sin el adapter LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 4 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:46<00:00, 11.67s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:35<00:00,  8.80s/it]\n",
      "/teamspace/studios/this_studio/agnostic-ai-pipeline/scripts/eval_po_student.py:191: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Evaluating 20 samples; saving to inference_results/baseline_20251114_215442.json\n",
      "[1/20] POCON-0196 -> ok score=0.819\n",
      "[2/20] POCON-0006 -> ok score=0.903\n",
      "[3/20] POCON-0049 -> ok score=0.903\n",
      "[4/20] POCON-0061 -> ok score=0.773\n",
      "[5/20] POCON-0119 -> ok score=0.777\n",
      "[6/20] POCON-0181 -> ok score=0.804\n",
      "[7/20] POCON-0163 -> ok score=0.810\n",
      "[8/20] POCON-0059 -> ok score=0.819\n",
      "[9/20] POCON-0016 -> ok score=0.825\n",
      "[10/20] POCON-0170 -> ok score=0.819\n",
      "[11/20] POCON-0215 -> ok score=0.810\n",
      "[12/20] POCON-0120 -> ok score=0.810\n",
      "[13/20] POCON-0217 -> ok score=0.819\n",
      "[14/20] POCON-0047 -> ok score=0.881\n",
      "[15/20] POCON-0101 -> ok score=0.795\n",
      "[16/20] POCON-0139 -> ok score=0.819\n",
      "[17/20] POCON-0094 -> ok score=0.870\n",
      "[18/20] POCON-0007 -> ok score=0.911\n",
      "[19/20] POCON-0092 -> ok score=0.907\n",
      "[20/20] POCON-0035 -> ok score=0.948\n",
      "[done] Results saved to inference_results/baseline_20251114_215442.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /teamspace/studios/this_studio/agnostic-ai-pipeline\n",
    "\n",
    "PYTHONPATH=. python scripts/eval_po_student.py \\\n",
    "  --tag baseline \\\n",
    "  --base-model Qwen/Qwen2.5-7B-Instruct \\\n",
    "  --max-samples 20 \\\n",
    "  --retries 2 \\\n",
    "  --max-new-tokens 1200 \\\n",
    "  --load-4bit \\\n",
    "  --bnb-compute-dtype float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluaci√≥n Student (Qwen2.5-7B + LoRA)\n",
    "\n",
    "Esta evaluaci√≥n usa el modelo base con el adapter LoRA entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:32<00:00,  8.10s/it]\n",
      "/teamspace/studios/this_studio/agnostic-ai-pipeline/scripts/eval_po_student.py:191: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Evaluating 20 samples; saving to inference_results/student_20251114_220448.json\n",
      "[1/20] POCON-0196 -> ok score=0.958\n",
      "[2/20] POCON-0006 -> ok score=0.742\n",
      "[3/20] POCON-0049 -> ok score=0.803\n",
      "[4/20] POCON-0061 -> ok score=0.375\n",
      "[5/20] POCON-0119 -> ok score=0.858\n",
      "[6/20] POCON-0181 -> ok score=0.918\n",
      "[7/20] POCON-0163 -> ok score=0.798\n",
      "[8/20] POCON-0059 -> ok score=0.773\n",
      "[9/20] POCON-0016 -> ok score=0.841\n",
      "[10/20] POCON-0170 -> ok score=0.842\n",
      "[11/20] POCON-0215 -> ok score=0.375\n",
      "[12/20] POCON-0120 -> ok score=0.866\n",
      "[13/20] POCON-0217 -> ok score=0.375\n",
      "[14/20] POCON-0047 -> ok score=0.945\n",
      "[15/20] POCON-0101 -> ok score=0.918\n",
      "[16/20] POCON-0139 -> ok score=0.724\n",
      "[17/20] POCON-0094 -> ok score=0.870\n",
      "[18/20] POCON-0007 -> ok score=0.749\n",
      "[19/20] POCON-0092 -> ok score=0.757\n",
      "[20/20] POCON-0035 -> ok score=0.953\n",
      "[done] Results saved to inference_results/student_20251114_220448.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /teamspace/studios/this_studio/agnostic-ai-pipeline\n",
    "\n",
    "PYTHONPATH=. python scripts/eval_po_student.py \\\n",
    "  --tag student \\\n",
    "  --base-model Qwen/Qwen2.5-7B-Instruct \\\n",
    "  --adapter-path artifacts/models/po_student_v1 \\\n",
    "  --max-samples 20 \\\n",
    "  --retries 2 \\\n",
    "  --max-new-tokens 1200 \\\n",
    "  --load-4bit \\\n",
    "  --bnb-compute-dtype float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Archivos de resultados encontrados:\n",
      "  Baseline: 2 archivo(s)\n",
      "  Student: 1 archivo(s)\n",
      "\n",
      "============================================================\n",
      "COMPARACI√ìN DE RESULTADOS\n",
      "============================================================\n",
      "\n",
      "üìà M√âTRICAS GENERALES\n",
      "\n",
      "M√©trica                        Baseline        Student         Diff\n",
      "----------------------------------------------------------------------\n",
      "MEAN                           0.8411          0.7720          -0.0690 (-8.2%)\n",
      "STD                            0.0492          0.1808          +0.1316 (+267.1%)\n",
      "MIN                            0.7734          0.3750          -0.3984 (-51.5%)\n",
      "MAX                            0.9483          0.9575          +0.0093 (+1.0%)\n",
      "\n",
      "üìã TASA DE √âXITO YAML\n",
      "\n",
      "Modelo                         Total      V√°lidos    Errores    Tasa √âxito\n",
      "----------------------------------------------------------------------\n",
      "Baseline                       20         20         0          100.0%\n",
      "Student                        20         20         0          100.0%\n",
      "\n",
      "‚úÖ CRITERIOS DE ACEPTACI√ìN (9.D.4)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "1. YAML v√°lido ‚â•90%:\n",
      "   Baseline: 100.0% ‚úÖ PASS\n",
      "   Student:  100.0% ‚úÖ PASS\n",
      "\n",
      "2. Student ‚â• 0.9 √ó Baseline:\n",
      "   Target:  0.7569\n",
      "   Actual:  0.7720 ‚úÖ PASS\n",
      "\n",
      "======================================================================\n",
      "RESULTADO GENERAL: ‚úÖ PASS - Listo para 9.D.5\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Buscar archivos de resultados\n",
    "results_dir = Path(\"/teamspace/studios/this_studio/agnostic-ai-pipeline/inference_results\")\n",
    "baseline_files = sorted(results_dir.glob(\"baseline_*.json\"))\n",
    "student_files = sorted(results_dir.glob(\"student_*.json\"))\n",
    "\n",
    "if not baseline_files:\n",
    "    print(\"‚ö†Ô∏è  No se encontraron resultados de baseline\")\n",
    "else:\n",
    "    print(f\"\\nüìä Archivos de resultados encontrados:\")\n",
    "    print(f\"  Baseline: {len(baseline_files)} archivo(s)\")\n",
    "    print(f\"  Student: {len(student_files)} archivo(s)\")\n",
    "\n",
    "# Cargar el resultado m√°s reciente de cada uno\n",
    "if baseline_files and student_files:\n",
    "    with open(baseline_files[-1], 'r') as f:\n",
    "        baseline_data = json.load(f)\n",
    "    \n",
    "    with open(student_files[-1], 'r') as f:\n",
    "        student_data = json.load(f)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"COMPARACI√ìN DE RESULTADOS\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # M√©tricas generales\n",
    "    print(\"üìà M√âTRICAS GENERALES\\n\")\n",
    "    print(f\"{'M√©trica':<30} {'Baseline':<15} {'Student':<15} {'Diff'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    baseline_metrics = baseline_data.get('metrics', {})\n",
    "    student_metrics = student_data.get('metrics', {})\n",
    "    \n",
    "    if baseline_metrics and student_metrics:\n",
    "        for metric in ['mean', 'std', 'min', 'max']:\n",
    "            b_val = baseline_metrics.get(metric, 0)\n",
    "            s_val = student_metrics.get(metric, 0)\n",
    "            diff = s_val - b_val\n",
    "            diff_pct = (diff / b_val * 100) if b_val != 0 else 0\n",
    "            \n",
    "            print(f\"{metric.upper():<30} {b_val:<15.4f} {s_val:<15.4f} {diff:+.4f} ({diff_pct:+.1f}%)\")\n",
    "    \n",
    "    # Tasa de √©xito YAML\n",
    "    print(f\"\\nüìã TASA DE √âXITO YAML\\n\")\n",
    "    print(f\"{'Modelo':<30} {'Total':<10} {'V√°lidos':<10} {'Errores':<10} {'Tasa √âxito'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    b_total = baseline_data.get('total_samples', 0)\n",
    "    b_valid = baseline_data.get('valid_samples', 0)\n",
    "    b_failed = baseline_data.get('failed_samples', 0)\n",
    "    b_rate = (b_valid / b_total * 100) if b_total > 0 else 0\n",
    "    \n",
    "    s_total = student_data.get('total_samples', 0)\n",
    "    s_valid = student_data.get('valid_samples', 0)\n",
    "    s_failed = student_data.get('failed_samples', 0)\n",
    "    s_rate = (s_valid / s_total * 100) if s_total > 0 else 0\n",
    "    \n",
    "    print(f\"{'Baseline':<30} {b_total:<10} {b_valid:<10} {b_failed:<10} {b_rate:.1f}%\")\n",
    "    print(f\"{'Student':<30} {s_total:<10} {s_valid:<10} {s_failed:<10} {s_rate:.1f}%\")\n",
    "    \n",
    "    # Criterios de aceptaci√≥n\n",
    "    print(f\"\\n‚úÖ CRITERIOS DE ACEPTACI√ìN (9.D.4)\\n\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    yaml_valid_threshold = 0.90\n",
    "    quality_threshold = 0.90\n",
    "    \n",
    "    yaml_pass = (b_rate >= yaml_valid_threshold * 100) and (s_rate >= yaml_valid_threshold * 100)\n",
    "    quality_pass = (s_val >= quality_threshold * b_val) if baseline_metrics and student_metrics else False\n",
    "    \n",
    "    print(f\"1. YAML v√°lido ‚â•90%:\")\n",
    "    print(f\"   Baseline: {b_rate:.1f}% {'‚úÖ PASS' if b_rate >= yaml_valid_threshold * 100 else '‚ùå FAIL'}\")\n",
    "    print(f\"   Student:  {s_rate:.1f}% {'‚úÖ PASS' if s_rate >= yaml_valid_threshold * 100 else '‚ùå FAIL'}\")\n",
    "    \n",
    "    if baseline_metrics and student_metrics:\n",
    "        print(f\"\\n2. Student ‚â• 0.9 √ó Baseline:\")\n",
    "        target = quality_threshold * baseline_metrics.get('mean', 0)\n",
    "        actual = student_metrics.get('mean', 0)\n",
    "        print(f\"   Target:  {target:.4f}\")\n",
    "        print(f\"   Actual:  {actual:.4f} {'‚úÖ PASS' if actual >= target else '‚ùå FAIL'}\")\n",
    "    \n",
    "    overall_pass = yaml_pass and quality_pass\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RESULTADO GENERAL: {'‚úÖ PASS - Listo para 9.D.5' if overall_pass else '‚ùå FAIL - Requiere ajustes'}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Casos con errores\n",
    "    if b_failed > 0 or s_failed > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  CASOS CON ERROR DE FORMATO:\\n\")\n",
    "        \n",
    "        if b_failed > 0:\n",
    "            print(\"Baseline:\")\n",
    "            for result in baseline_data.get('results', []):\n",
    "                if result.get('status') == 'format_error':\n",
    "                    print(f\"  - {result.get('concept_id')} (tier: {result.get('tier')})\")\n",
    "        \n",
    "        if s_failed > 0:\n",
    "            print(\"\\nStudent:\")\n",
    "            for result in student_data.get('results', []):\n",
    "                if result.get('status') == 'format_error':\n",
    "                    print(f\"  - {result.get('concept_id')} (tier: {result.get('tier')})\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No se pueden comparar resultados: falta alg√∫n archivo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Guardar Resultados\n",
    "\n",
    "Los resultados se guardan en el teamspace y est√°n disponibles para descarga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Resultados comprimidos en: /teamspace/studios/this_studio/eval_results_20251115.zip\n",
      "\n",
      "üìÇ Copiando archivos JSON al teamspace...\n",
      "  ‚úì comparison_20251114_143731.json ‚Üí /teamspace/studios/this_studio/comparison_20251114_143731.json\n",
      "  ‚úì finetuned_20251114_143731.json ‚Üí /teamspace/studios/this_studio/finetuned_20251114_143731.json\n",
      "  ‚úì baseline_20251114_143731.json ‚Üí /teamspace/studios/this_studio/baseline_20251114_143731.json\n",
      "  ‚úì baseline_20251114_215442.json ‚Üí /teamspace/studios/this_studio/baseline_20251114_215442.json\n",
      "  ‚úì student_20251114_220448.json ‚Üí /teamspace/studios/this_studio/student_20251114_220448.json\n",
      "\n",
      "‚úÖ 5 archivos copiados al teamspace\n",
      "‚úÖ ZIP disponible en: /teamspace/studios/this_studio/eval_results_20251115.zip\n",
      "\n",
      "üí° Usa el navegador de archivos de Lightning (sidebar izquierdo) para descargar.\n",
      "\n",
      "üì¶ Contenido del ZIP:\n",
      "   - baseline_20251114_143731.json (0.01 MB)\n",
      "   - baseline_20251114_215442.json (0.08 MB)\n",
      "   - comparison_20251114_143731.json (0.00 MB)\n",
      "   - finetuned_20251114_143731.json (0.01 MB)\n",
      "   - student_20251114_220448.json (0.10 MB)\n",
      "\n",
      "üìÅ Archivos en teamspace:\n",
      "-rw-r--r-- 1 krukmatias krukmatias 9.4K Nov 14 21:47 /teamspace/studios/this_studio/baseline_20251114_143731.json\n",
      "-rw-r--r-- 1 krukmatias krukmatias  86K Nov 14 22:03 /teamspace/studios/this_studio/baseline_20251114_215442.json\n",
      "-rw-r--r-- 1 krukmatias krukmatias  214 Nov 14 21:47 /teamspace/studios/this_studio/comparison_20251114_143731.json\n",
      "-rw-r--r-- 1 krukmatias krukmatias  29K Nov 14 22:19 /teamspace/studios/this_studio/eval_results_20251115.zip\n",
      "-rw-r--r-- 1 krukmatias krukmatias 9.3K Nov 14 21:47 /teamspace/studios/this_studio/finetuned_20251114_143731.json\n",
      "-rw-r--r-- 1 krukmatias krukmatias 100K Nov 14 22:19 /teamspace/studios/this_studio/student_20251114_220448.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Comprimir resultados\n",
    "results_dir = \"/teamspace/studios/this_studio/agnostic-ai-pipeline/inference_results\"\n",
    "output_dir = \"/teamspace/studios/this_studio\"\n",
    "archive_path = f\"{output_dir}/eval_results_20251115\"\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "    print(\"‚ùå No se encontr√≥ el directorio de resultados\")\n",
    "else:\n",
    "    # Crear ZIP\n",
    "    shutil.make_archive(archive_path, 'zip', results_dir)\n",
    "    zip_file = f\"{archive_path}.zip\"\n",
    "    print(f\"‚úÖ Resultados comprimidos en: {zip_file}\")\n",
    "    \n",
    "    # Copiar archivos JSON individuales al output\n",
    "    print(f\"\\nüìÇ Copiando archivos JSON al teamspace...\")\n",
    "    \n",
    "    json_files = list(Path(results_dir).glob(\"*.json\"))\n",
    "    for json_file in json_files:\n",
    "        dest = Path(output_dir) / json_file.name\n",
    "        shutil.copy2(json_file, dest)\n",
    "        print(f\"  ‚úì {json_file.name} ‚Üí {dest}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ {len(json_files)} archivos copiados al teamspace\")\n",
    "    print(f\"‚úÖ ZIP disponible en: {zip_file}\")\n",
    "    print(f\"\\nüí° Usa el navegador de archivos de Lightning (sidebar izquierdo) para descargar.\")\n",
    "    \n",
    "    # Mostrar contenido del ZIP\n",
    "    print(f\"\\nüì¶ Contenido del ZIP:\")\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zf:\n",
    "        for name in sorted(zf.namelist()):\n",
    "            info = zf.getinfo(name)\n",
    "            size_mb = info.file_size / 1024**2\n",
    "            print(f\"   - {name} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    # Listar archivos en teamspace\n",
    "    print(f\"\\nüìÅ Archivos en teamspace:\")\n",
    "    !ls -lh /teamspace/studios/this_studio/*.json /teamspace/studios/this_studio/*.zip 2>/dev/null || echo \"No hay archivos JSON/ZIP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Instrucciones Finales\n",
    "\n",
    "**Para descargar los resultados**:\n",
    "\n",
    "1. En Lightning AI Studio, ve al **navegador de archivos** (sidebar izquierdo)\n",
    "2. Navega a `/teamspace/studios/this_studio/`\n",
    "3. Encontrar√°s:\n",
    "   - `eval_results_20251115.zip` (todos los resultados comprimidos)\n",
    "   - `baseline_<timestamp>.json` (resultados baseline individuales)\n",
    "   - `student_<timestamp>.json` (resultados student individuales)\n",
    "4. Click derecho en cada archivo ‚Üí **Download**\n",
    "\n",
    "**Siguiente paso (Task 9.D.4)**:\n",
    "\n",
    "1. Subir los archivos JSON a `inference_results/` en el repositorio\n",
    "2. Actualizar `docs/po_distillation_report.md` con los resultados\n",
    "3. Si PASS ‚Üí Avanzar a Task 9.D.5 (integraci√≥n al pipeline)\n",
    "4. Si FAIL ‚Üí Analizar casos `format_error` y ajustar\n",
    "\n",
    "**Nota sobre cuota GPU**:\n",
    "- Lightning AI Studio ofrece 10 horas GPU gratuitas/mes\n",
    "- Esta evaluaci√≥n toma ~30-40 minutos (baseline + student)\n",
    "- Recuerda detener el Studio cuando termines para conservar cuota"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
