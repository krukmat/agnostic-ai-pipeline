# DSPy Integration Plan

## Executive Summary

Este documento analiza la viabilidad y el plan de integraci√≥n de DSPy (Declarative Self-improving Python) en el pipeline multi-role existente. DSPy es un framework de Stanford NLP (ahora parte de Databricks) que permite programar modelos de lenguaje mediante c√≥digo compositivo en lugar de prompts fr√°giles. Versi√≥n actual: **3.0.3** (Agosto 2025).

**Decisi√≥n cr√≠tica requerida**: ¬øExperimentaci√≥n paralela o reemplazo gradual de roles BA/QA?

---

## Sobre DSPy Framework

### Descripci√≥n General
- **Origen**: Stanford NLP (Feb 2022), evolucionado desde DSP hasta DSPy (Oct 2023)
- **Adopci√≥n**: 28,000+ estrellas en GitHub, 160,000 descargas mensuales (mid-2025)
- **Licencia**: MIT
- **Repositorio**: https://github.com/stanfordnlp/dspy
- **Documentaci√≥n**: https://dspy.ai

### Conceptos Core
1. **Signatures**: Declaraciones de transformaciones de texto con tipos de I/O natural
2. **Modules**: Componentes como `dspy.Predict`, `dspy.ChainOfThought`, `dspy.ReAct`
3. **Optimizers** (antes "teleprompters"): Algoritmos que ajustan prompts y/o pesos del modelo
   - `MIPROv2`: Recomendado para datasets grandes (200+ ejemplos, 40+ trials)
   - `BootstrapFewShot`: Para datasets peque√±os
   - `COPRO`: Optimizaci√≥n cooperativa de prompts

### Versi√≥n 3.0 - Novedades (2025)
- **Multi-modal I/O**: `dspy.Image`, `dspy.Audio`, tipos compuestos (`list[dspy.Image]`), Pydantic models
- **Tool Integration**: Soporte nativo para MCP servers y LangChain tools
- **Observability**: Integraci√≥n nativa con MLflow 3.0 (tracing, optimizer tracking, deployment)
- **Breaking Changes**:
  - Requiere Python 3.10‚Äì3.13 (dropped 3.9 support)
  - Removidos community retrievers no mantenidos (#8073)
  - Deprecaciones de `functional/`, `dsp/` legacy clients

---

## Observaciones de los prompts DSPy
- `01-context.md` exige un nuevo paquete `dspy_baseline/` para alojar m√≥dulos Python que expresen las etapas BA y QA como programas DSPy, sin modificar la estructura existente ni exponer secretos.
- `02-structure.md` detalla la jerarqu√≠a exacta de archivos que debemos crear (config, modules, optimizers, scripts, data). Cada m√≥dulo debe exportar funciones importables y usar imports relativos.
- `03-ba-module.md` y `04-qa-module.md` definen las firmas DSPy para BA (`BARequirements`) y QA (`QATestCases`), junto con funciones `generate_requirements` y `generate_testcases` que validan la salida y delegan validaciones a helpers en `common.py` y `metrics.py`.
- `05-optimizer.md` y `06-tune-script.md` solicitan un wrapper m√≠nimo de `dspy.MIPROv2` y un script `tune.py` que combine trainset, m√©tricas y MLflow para compilar los programas optimizados y guardar artefactos.
- `07-metrics-common.md` fija el contenido inicial de `dspy_baseline/config/metrics.py` y `dspy_baseline/modules/common.py` (trainsets diminutos, m√©trica heur√≠stica, TODO para validador YAML).
- `08-makefile.md` y `09-readme.md` piden exponer los scripts v√≠a `make` y documentar la capa DSPy con dependencias y flujo de ejecuci√≥n.

## Impacto en la arquitectura actual
- La automatizaci√≥n actual vive en `scripts/*.py` y se ejecuta v√≠a `make`; no existe paquete `dspy_baseline/`, por lo que la nueva carpeta no colisiona pero s√≠ implica actualizar rutas relativas y entorno (usa `PYTHONPATH` ra√≠z).
- `scripts/run_ba.py` y `scripts/run_qa.py` ya generan artefactos YAML/Markdown con clientes LLM propios (`common.Client`, `logger`); la capa DSPy deber√° convivir con ellos. Se planea introducir scripts `dspy_baseline/scripts/run_ba.py` y `run_qa.py` que puedan reutilizar artefactos existentes (p.ej. `planning/requirements.yaml`) sin romper los flujos actuales.
- El Makefile actual define muchas dianas; las nuevas (`dspy-ba`, `dspy-qa`, `dspy-tune-*`) deber√°n a√±adirse sin interferir con variables de entorno ni macros ya usados.
- `artifacts/` y `planning/` ya son directorios cr√≠ticos; los scripts DSPy guardar√°n resultados bajo `artifacts/` (p.ej. `artifacts/dspy/` o archivos concretos) evitando sobreescribir la salida de los agentes tradicionales.
- El repositorio tiene dependencias gestionadas v√≠a `requirements.txt`. DSPy, MLflow y dependencias extras deber√°n a√±adirse ah√≠ o documentarse como requisito manual, verificando no romper `make setup`.

## Plan de trabajo propuesto
1. **Auditor√≠a inicial**
   - Confirmar que no existe carpeta `dspy_baseline/` ni objetivos relacionados (ya verificado v√≠a `rg`/`ls`).
   - Revisar `requirements.txt` y `README.md` para preparar la introducci√≥n de nuevas dependencias.
2. **Estructura del paquete**
   - Crear √°rbol `dspy_baseline/` con `__init__.py` en cada subcarpeta, `config/`, `modules/`, `optimizers/`, `scripts/`, `data/`, y archivos de datos de ejemplo (`demos/ba_demo.json`, `eval/ba_eval.json`).
   - A√±adir placeholders y docstrings conforme a los prompts, usando `TODO` donde falte integraci√≥n real (p.ej. carga de modelos desde config).
3. **Helpers y m√©tricas**
   - Implementar `dspy_baseline/modules/common.py` con trainsets de juguete y validador YAML provisional (`TODO` para reglas estrictas).
   - Implementar `dspy_baseline/config/metrics.py` con heur√≠sticas descritas, asegurando retornos en `[0,1]`.
4. **Programas DSPy**
   - Implementar `ba_requirements.py` (firma, predictor `dspy.Predict`, validaciones) y `qa_testcases.py` (`dspy.ChainOfThought`, m√©trica opcional).
   - Alinear comentarios con intenci√≥n de optimizaci√≥n futura (MIPROv2).
5. **Optimizaci√≥n y scripts**
   - Implementar `dspy/optimizers/mipro.py` con wrapper indicado.
   - Crear scripts CLI (`dspy_baseline/scripts/run_ba.py`, `run_qa.py`, `tune.py`) que acepten argumentos (`typer` o `argparse`), carguen config, llamen a los programas y escriban resultados bajo `artifacts/`.
   - En `tune.py`, habilitar `mlflow.dspy.autolog()`, cargar trainset/metric y exportar programa compilado a `artifacts/` con carpeta creada si falta.
6. **Makefile y documentaci√≥n**
   - A√±adir targets `dspy-ba`, `dspy-qa`, `dspy-tune-ba`, `dspy-tune-qa` sin eliminar existentes. Si se requiere activar la venv (`$(PY)`), documentar un TODO seg√∫n prompt.
   - A√±adir README corto en `dspy_baseline/` siguiendo `09-readme.md` y apuntar a `provider.example.yaml`.
7. **Integraci√≥n con pipeline**
   - Evaluar c√≥mo los nuevos scripts se encadenan con `make loop` o si quedan como herramientas auxiliares; documentar TODOs para integraci√≥n profunda (p.ej. intercambiar `scripts/run_ba.py` por la versi√≥n DSPy cuando est√© estable).
   - Revisar `config.yaml`/`scripts/set_role.py` para decidir si necesitan conocer la capa DSPy (probablemente s√≥lo documentar).
8. **Dependencias y QA**
   - Decidir si `requirements.txt` debe incluir `dspy-ai`, `mlflow`, `pyyaml` (ya presente). A√±adir o dejar TODO seg√∫n pol√≠tica del repo.
   - Dise√±ar verificaci√≥n m√≠nima (p.ej. ejecutar scripts con mocks o test unitarios stub) y documentar pasos manuales (`make dspy-ba`, etc.).
9. **Revisi√≥n final**
   - Confirmar imports relativos, ausencia de claves duras y coherencia de rutas.
   - Validar que la nueva carpeta no rompe `make setup` ni artefactos existentes (ejecuci√≥n en seco o TODO).

## üö® RED FLAGS Y LIMITACIONES CR√çTICAS

### 1. **BLOCKER: Incompatibilidad de Python**
- **Problema**: DSPy 3.0 requiere Python 3.10‚Äì3.13
- **Situaci√≥n actual**: El proyecto usa Python 3.9.10
- **Impacto**: NO se puede instalar DSPy 3.0 sin actualizar Python
- **Mitigaci√≥n**:
  - Opci√≥n A: Actualizar todo el proyecto a Python 3.10+ (testing completo requerido)
  - Opci√≥n B: Usar DSPy 2.x (versi√≥n legacy, sin features 3.0)
  - Opci√≥n C: Entorno virtual separado para experimentos DSPy (complejidad operativa)

### 2. **Requisitos de Datos para Optimizaci√≥n**
- **MIPROv2** (optimizer planeado) requiere:
  - M√≠nimo 200 ejemplos de entrenamiento para evitar overfitting
  - 40+ trials de optimizaci√≥n (cada trial = llamadas LLM adicionales)
  - ~10 minutos de ejecuci√≥n por optimization run
  - Costo: desde centavos hasta decenas de d√≥lares por run
- **Problema actual**: No existe dataset de ejemplos BA/QA validados
- **Acci√≥n requerida**: Crear corpus de 200+ ejemplos (concept ‚Üí requirements, requirements ‚Üí test cases)

### 3. **Context Length Limitations**
- **Issue #381** (GitHub DSPy): Los modelos actuales tienen l√≠mites de contexto que causan fallos durante compilaci√≥n con few-shot learning
  - GPT-3.5: 4,097 tokens
  - GPT-4: 8,192 tokens (modelos m√°s recientes: 128k)
  - Mistral: 8,000+ tokens
- **Riesgo**: Los programas DSPy pueden exceder l√≠mites cuando se a√±aden ejemplos few-shot durante optimizaci√≥n
- **Mitigaci√≥n**: Usar modelos con contextos grandes (Claude 3.5, GPT-4 Turbo) o limitar n√∫mero de shots

### 4. **Silent Failures en Dependencias**
- **Bug conocido**: Si falta el paquete `datasets` (removido de deps por defecto en 3.0.0b1), MIPROv2 falla silenciosamente y produce peor optimizaci√≥n
- **Problema**: Errores no expl√≠citos complican debugging
- **Mitigaci√≥n**: Instalar todas las dependencias opcionales: `pip install dspy[all]` o verificar manualmente

### 5. **Read-Only Environments**
- **Issue conocido (March 2025)**: M√∫ltiples m√≥dulos DSPy intentan crear archivos de log al importar, fallando en entornos read-only (AWS Lambda, containers inmutables)
- **Impacto en A2A**: Si los servicios corren en containers con filesystems read-only, DSPy puede fallar al importar
- **Mitigaci√≥n**: Verificar permisos de escritura en `/tmp` o configurar cache directory custom

### 6. **Black Box Optimization**
- **Limitaci√≥n inherente**: DSPy optimiza end-to-end sin visibilidad de m√©tricas intermedias (no hay gradientes)
- **Implicaci√≥n**: Dificulta debug de por qu√© un optimizer eligi√≥ ciertos prompts
- **Compromiso aceptado**: Trade-off conocido de prompt optimization

### 7. **Observability Gaps**
- `inspect_history` solo registra llamadas LLM, no otros componentes (retrievers, tools, m√≥dulos custom)
- Logs monol√≠ticos dificultan organizaci√≥n cuando hay m√∫ltiples llamadas por predicci√≥n
- **Mitigado en parte**: MLflow 3.0 tracing ayuda pero no resuelve todo

### 8. **Synchronous Execution**
- DSPy tiene limitaciones en ejecuci√≥n as√≠ncrona; llamadas de red pueden bloquear
- **Impacto en A2A**: Puede afectar latencia en el modo distribuido (servicios HTTP)
- **Issue #8273**: Propuesta de capa de protocolo agent para MCP y A2A a√∫n en discusi√≥n

### 9. **Dependency Weight**
- `mlflow`: ~200MB con dependencias transitivas
- `dspy-ai`: ~50MB + deps de LLM providers
- **Impacto**: Aumenta tama√±o de entorno virtual y tiempo de instalaci√≥n
- **Considerar**: Dependencias opcionales (`extras_require`) si no todo el equipo necesita DSPy

### 10. **Learning Curve**
- DSPy introduce paradigma diferente (programaci√≥n vs prompting)
- Equipo necesitar√° familiarizarse con signatures, modules, optimizers
- **Tiempo estimado**: 1-2 semanas para proficiencia b√°sica

---

## Riesgos Adicionales, Dependencias y Preguntas Abiertas
- **Configuraci√≥n de proveedores**: falta definir c√≥mo `dspy` leer√° credenciales existentes (`config/provider.example.yaml` solo cubre ejemplo). Se requiere decidir si reutilizar `config.yaml` global o mantener archivos separados.
- **Compatibilidad con la orquestaci√≥n**: las rutas de salida (`planning/requirements.yaml`, `artifacts/qa/`) ya est√°n ocupadas por los agentes tradicionales. Habr√° que decidir si los programas DSPy sobre-escriben, generan versiones alternativas o s√≥lo sirven como experimentos (a√±adir `TODO` en scripts).
- **Gesti√≥n de dependencias**: `mlflow` y `dspy-ai` pueden ser pesados; verificar si deben a√±adirse al entorno principal o condicionar su instalaci√≥n (p.ej. `extra[dspy]`).
- **Evaluaciones y datasets**: los prompts piden archivos `demos` y `eval` pero no definen contenido. Usar ejemplos m√≠nimos y dejar `TODO` para reemplazarlos por datos reales.
- **Testing**: no hay instrucciones expl√≠citas sobre tests para la capa DSPy. Considerar a√±adir `pytest` b√°sicos o, al menos, descripciones en README sobre c√≥mo validar manualmente.

---

## An√°lisis de Costos y Performance

### Costos de Optimizaci√≥n (MIPROv2)
| Escenario | Ejemplos | Trials | Tiempo estimado | Costo LLM estimado* |
|-----------|----------|--------|-----------------|---------------------|
| M√≠nimo viable | 50 | 20 | ~5 min | $0.50 - $2 |
| Recomendado | 200 | 40 | ~10 min | $2 - $10 |
| Enterprise | 500+ | 100+ | ~30 min | $10 - $50+ |

*Basado en GPT-4, var√≠a significativamente por provider (Ollama local = $0)

### Performance Esperado
- **Sin optimizaci√≥n** (baseline `dspy.Predict`): Similar a prompts actuales
- **Con MIPROv2**: +20-30% mejora en m√©tricas de calidad (seg√∫n casos reportados)
  - Ejemplo DSPy docs: ReAct agent 24% ‚Üí 51% accuracy
  - RAG system: 53% ‚Üí 61% accuracy

### ROI Estimado
- **Inversi√≥n inicial**: 2-3 semanas (setup + creaci√≥n de dataset + optimizaci√≥n)
- **Beneficio esperado**: Mejora consistente en calidad de outputs BA/QA sin ajuste manual continuo
- **Mantenimiento**: Reoptimizaci√≥n peri√≥dica cuando cambia distribuci√≥n de inputs

---

## Estrategia de Migraci√≥n Propuesta

### Fase 0: Decisiones Pre-requisito (BLOCKER)
- [ ] **Decidir versi√≥n de Python**: Actualizar a 3.10+ o usar DSPy 2.x
- [ ] **Decidir scope**: ¬øExperimento paralelo o reemplazo progresivo?
- [ ] **Aprobar incremento de dependencias**: +250MB en .venv

### Fase 1: Proof of Concept (2 semanas)
**Objetivo**: Validar DSPy con BA role en modo experimental

1. **Setup aislado** (d√≠as 1-2)
   - Branch feature: `feature/dspy-poc`
   - Upgrade Python a 3.10+ en entorno de desarrollo (si aprobado)
   - Instalar `dspy-ai>=3.0.3` y `mlflow>=3.0` en .venv
   - Crear carpeta `dspy_baseline/` seg√∫n estructura prompts

2. **BA Module baseline** (d√≠as 3-5)
   - Implementar `ba_requirements.py` con `dspy.Predict` (sin optimizaci√≥n)
   - Script `dspy_baseline/scripts/run_ba.py` que genere `artifacts/dspy/requirements_dspy.yaml`
   - Comparar outputs con `planning/requirements.yaml` (actual) manualmente

3. **Dataset creation** (d√≠as 6-8)
   - Recolectar 50 ejemplos hist√≥ricos de concepts ‚Üí requirements
   - Validar calidad manualmente (2 revisores por ejemplo)
   - Formato JSON: `{"concept": "...", "requirements": {...}}`
   - Almacenar en `dspy/data/demos/ba_demo.json`, `dspy/data/eval/ba_eval.json`

4. **Optimization trial** (d√≠as 9-10)
   - Implementar `tune.py` con `MIPROv2` (50 ejemplos, 20 trials)
   - Ejecutar optimizaci√≥n: `make dspy-tune-ba` (~5 min)
   - Comparar programa compilado vs baseline (m√©tricas: completitud YAML, validez estructura)

5. **Evaluaci√≥n PoC** (d√≠as 11-14)
   - Probar programa optimizado con 10 concepts nuevos
   - Comparar con outputs actuales (agente BA tradicional)
   - Documento de resultados: ¬ømejora justifica complejidad?
   - **Go/No-Go decision**

### Fase 2: Producci√≥n BA (si Go, 2-3 semanas)
- Expandir dataset a 200+ ejemplos (crowdsource con equipo)
- Reoptimizar con 40 trials
- Integraci√≥n con `make loop`: flag `USE_DSPY_BA=1`
- M√©tricas en MLflow dashboard
- Documentaci√≥n en `dspy/README.md`

### Fase 3: QA Module (3-4 semanas)
- Repetir proceso para rol QA
- Dataset: requirements ‚Üí test cases (200+ ejemplos)
- Integraci√≥n con `make qa`

### Fase 3b: Decomiso BA Tradicional (2-3 d√≠as)
- Inventario de entry points (`Makefile`, scripts, docs) que apuntan a `scripts/run_ba.py`.
- Redefinir `make ba` como alias de `make dspy-ba` y ajustar orquestadores (`loop`, `iteration`, `run_orchestrator.py`).
- Deprecar/eliminar `scripts/run_ba.py` y referencias asociadas.
- Ejecutar QA puntual (`make ba ‚Üí po ‚Üí plan`) asegurando que Product Owner y Architect consumen DSPy.
- Actualizar documentaci√≥n (`README`, `DSPY_INTEGRATION_PLAN`, changelog) anunciando el cambio.
- _(Siguiente)_ A√±adir flag configurable `features.use_dspy_ba` en `config.yaml` para permitir fallback legacy. Documentar la fase en `docs/phase3b_configuration.md`.

### Fase 4: Integraci√≥n DSPy ‚Üí Product Owner (3 d√≠as)
- **Objetivo**: Ajustar el rol Product Owner para consumir `planning/requirements.yaml` generado por DSPy y seguir produciendo `product_vision.yaml` y `product_owner_review.yaml` √∫tiles.
- **Tareas**:
  1. Revisar dependencias del prompt (`prompts/product_owner.md`) y decidir si se ampl√≠a el YAML DSPy (overview, stakeholders, personas) o si se infieren dentro del rol PO.
  2. Ejecutar `make dspy-ba` + `make po` con al menos dos conceptos (simple y medium) para validar la cadena `concept ‚Üí visi√≥n ‚Üí review`.
  3. Ajustar `dspy_baseline/modules/ba_requirements.py` o el prompt del PO seg√∫n gaps detectados (por ejemplo, agregar secciones faltantes, enriquecer meta). *(Durante la validaci√≥n inicial se detect√≥ que el bloque REVIEW no se genera; queda pendiente reforzar el prompt o agregar validaciones adicionales).* 
  4. Documentar hallazgos y outputs en `docs/phase4_product_owner.md` (crear) e incluir checklist de QA puntual.
  5. _(Nice-to-have)_ Evaluar un m√≥dulo DSPy que genere borradores de visi√≥n y review cuando exista dataset suficiente (requirements ‚Üî feedback) para automatizar a√∫n m√°s este rol.

### Fase 5: Integraci√≥n DSPy ‚Üí Architect (4 d√≠as)
- **Objetivo**: Garantizar que el flujo de Architect (planificaci√≥n de historias) funciona con la salida DSPy enriquecida.
- **Tareas**:
  1. Validar el clasificador de complejidad (`classify_complexity_with_llm`); si el YAML DSPy es corto, inyectar res√∫menes o reutilizar el concepto original para evitar clasificaciones sesgadas a ‚Äúsimple‚Äù.
  2. Ejecutar `make dspy-ba`, `make po` y `make plan` con los mismos conceptos de Fase 4; evaluar `planning/stories.yaml` (historias, acceptance, riesgos).
  3. Ajustar prompts (`prompts/architect*.md`) o la generaci√≥n DSPy para cubrir informaci√≥n que antes ven√≠a del BA tradicional.
  4. Registrar resultados en `docs/phase5_architect.md` y preparar recomendaciones para la decisi√≥n final (`merge / iterate`).
  5. _(Nice-to-have)_ Evaluar un m√≥dulo DSPy dedicado para Architect (firmas espec√≠ficas de historias) si se dispone de dataset suficiente.
  6. _(Estado actual)_ QA puntual ejecutado con conceptos ‚ÄúPlataforma de eventos‚Ä¶‚Äù y ‚ÄúERP manufactura‚Äù; se identific√≥ necesidad de ampliar prompts para generar historias UI/UX.

> Tras las Fases 4 y 5, re-ejecutar QA puntual (PO + Architect) para confirmar que la cadena completa CONCEPT ‚Üí Requirements ‚Üí Vision/Stories sigue operativa.

---

## Plan de Testing DSPy Integration

### Tests Unitarios
```bash
tests/dspy/
‚îú‚îÄ‚îÄ test_ba_module.py          # Validar BARequirements signature
‚îú‚îÄ‚îÄ test_qa_module.py          # Validar QATestCases signature
‚îú‚îÄ‚îÄ test_common.py             # Validar helpers YAML
‚îî‚îÄ‚îÄ test_metrics.py            # Validar m√©tricas heur√≠sticas
```

### Tests de Integraci√≥n
- `test_dspy_ba_end_to_end.py`: Concept ‚Üí requirements YAML completo
- `test_dspy_tune_smoke.py`: Verificar MIPROv2 no crashea (con dataset peque√±o)
- `test_mlflow_logging.py`: Verificar traces se registran correctamente

### Tests de Compatibilidad
- Verificar outputs DSPy son parseables por Product Owner y Architect
- Verificar `planning/requirements.yaml` schema si DSPy reemplaza agente actual

### CI/CD Considerations
- Tests DSPy requieren credenciales LLM (mock en CI, o usar Ollama local)
- Optimization tests son costosos/lentos ‚Üí skip en CI, solo smoke tests

---

## M√©tricas de √âxito

### KPIs T√©cnicos
1. **Calidad Output BA**:
   - Completitud YAML (todos campos requeridos presentes): ‚â•95%
   - Validez sint√°ctica: 100%
   - Coherencia sem√°ntica (eval manual): ‚â•85%

2. **Calidad Output QA**:
   - Cobertura de requirements en test cases: ‚â•90%
   - Tests ejecutables (no pseudoc√≥digo): ‚â•95%

3. **Performance**:
   - Latencia BA generation: ‚â§60s (p95)
   - Latencia QA generation: ‚â§90s (p95)

4. **Confiabilidad**:
   - Success rate (sin crashes): ‚â•99%
   - Consistency (mismos inputs ‚Üí mismos outputs): ‚â•80%

### KPIs de Negocio
- Reducci√≥n en tiempo de revisi√≥n manual de requirements: -30%
- Reducci√≥n en iteraciones de QA feedback loop: -20%
- Developer satisfaction score: ‚â•8/10

---

## Checklist de Implementaci√≥n

### Pre-implementaci√≥n
- [ ] Aprobar upgrade Python 3.10+ o confirmar uso DSPy 2.x
- [ ] Confirmar budget para optimization runs (~$50 para PoC completo)
- [ ] Asignar resources: 1 dev full-time por 2 semanas (PoC)
- [ ] Revisar issues abiertos DSPy relevantes: https://github.com/stanfordnlp/dspy/issues
- [ ] Decidir estrategia de versionado (branch, feature flag, entorno separado)

### Durante implementaci√≥n
- [ ] Documentar decisiones de arquitectura en ADRs
- [ ] Code reviews obligatorios en cambios core
- [ ] Validar manualmente primeros 20 outputs DSPy
- [ ] Setup MLflow tracking server (local o remoto)
- [ ] Configurar alertas en caso de degradaci√≥n de m√©tricas

### Post-implementaci√≥n
- [ ] Training session equipo sobre DSPy (1-2 horas)
- [ ] Runbook para debugging issues DSPy
- [ ] Plan de rollback si resultados son peores que baseline
- [ ] Schedule reoptimizaci√≥n peri√≥dica (cada 3 meses o cuando dataset crece 30%)

---

## Decisiones Requeridas del Equipo

1. **Python Version Upgrade**: ¬øActualizar a 3.10+ ahora o usar DSPy 2.x legacy?
   - **Recomendaci√≥n**: Upgrade a 3.10+ para aprovechar DSPy 3.0 features y futuras actualizaciones

2. **Modo de operaci√≥n**: ¬øExperimento paralelo o reemplazo gradual?
   - **Recomendaci√≥n**: Experimento paralelo (outputs a `artifacts/dspy/`) durante PoC, reemplazo gradual solo si resultados son >20% mejor

3. **Dependency management**: ¬øAgregar a requirements.txt o extras_require?
   - **Recomendaci√≥n**: `extras_require` (`pip install -e .[dspy]`) para no impactar usuarios que no usan DSPy

4. **Dataset creation**: ¬øQui√©n crea los 200+ ejemplos?
   - **Recomendaci√≥n**: Combinar hist√≥rico (si existe logs), synthetic generation (LLM genera ejemplos), y validaci√≥n manual (2 personas por ejemplo)

5. **MLflow hosting**: ¬øLocal o remoto?
   - **Recomendaci√≥n**: Local (`mlruns/` en .gitignore) para PoC, remoto (MLflow tracking server) para producci√≥n

6. **Rollback plan**: Si DSPy empeora resultados, ¬øc√≥mo revertir?
   - **Recomendaci√≥n**: Feature flag `USE_DSPY_BA=0` (default) permite toggle instant√°neo

---

## Referencias y Recursos

### Documentaci√≥n Oficial
- DSPy Docs: https://dspy.ai
- DSPy GitHub: https://github.com/stanfordnlp/dspy
- MLflow DSPy Flavor: https://mlflow.org/docs/latest/genai/flavors/dspy
- DSPy Optimizer Tracking: https://dspy.ai/tutorials/optimizer_tracking/

### Papers y Art√≠culos
- DSPy Paper (arXiv): https://arxiv.org/pdf/2310.03714
- "Compiling Declarative Language Model Calls": https://hai.stanford.edu/research/dspy-compiling-declarative-language-model-calls-into-state-of-the-art-pipelines
- Comparative Study Teleprompters: https://arxiv.org/html/2412.15298v1

### Community
- DSPy Discord: Ver GitHub README
- Stack Overflow tag: `dspy`
- Issue tracking: https://github.com/stanfordnlp/dspy/issues

---

## Conclusi√≥n y Recomendaciones Finales

### Veredicto: üü° **PROCEDER CON CAUTELA**

DSPy es un framework maduro y bien soportado (Stanford/Databricks, 28k+ stars), pero la integraci√≥n requiere inversi√≥n significativa:

**Pros**:
- ‚úÖ Optimizaci√≥n autom√°tica de prompts (menos tuning manual)
- ‚úÖ Integraci√≥n nativa MLflow para observability
- ‚úÖ Paradigma program√°tico m√°s mantenible que prompt strings
- ‚úÖ Comunidad activa y desarrollo continuo

**Cons**:
- ‚ùå **BLOCKER**: Requiere Python 3.10+ (upgrade mandatorio)
- ‚ùå Necesita 200+ ejemplos validados (esfuerzo significativo)
- ‚ùå Curva de aprendizaje para equipo
- ‚ùå Dependencias pesadas (+250MB)
- ‚ùå Costos de optimizaci√≥n (aunque manejables con Ollama local)

**Recomendaciones**:

1. **Short term** (pr√≥ximas 2 semanas):
   - Aprobar upgrade Python 3.10+ como pre-requisito
   - Ejecutar PoC Fase 1 con BA role (scope limitado, 50 ejemplos)
   - Decisi√≥n Go/No-Go basada en resultados cuantitativos

2. **Medium term** (si Go en PoC):
   - Expandir a producci√≥n BA con dataset completo (200+ ejemplos)
   - Comenzar PoC QA role en paralelo
   - Mantener agentes tradicionales como fallback (feature flag)

3. **Long term** (6+ meses):
   - Si DSPy demuestra valor en BA/QA, evaluar Architect/Dev roles
   - Considerar contribuciones upstream (Issue #8273: A2A protocol layer)
   - Posible talk/blog post compartiendo learnings

**Siguiente paso inmediato**: Reuni√≥n de decisi√≥n con stakeholders para aprobar/rechazar upgrade Python y presupuesto PoC.

---

> **√öltima actualizaci√≥n**: 2025-11-03
> **Autor**: An√°lisis t√©cnico basado en documentaci√≥n oficial DSPy 3.0.3, research papers, y GitHub issues
> **Estado**: Pendiente aprobaci√≥n de decisiones cr√≠ticas (Python upgrade, budget, scope)

> Acciones siguientes: revisar con el equipo si la integraci√≥n debe reemplazar a los agentes actuales o convivir como experimento; confirmar la pol√≠tica de dependencias antes de agregar `dspy-ai`/`mlflow`; definir ubicaci√≥n final de artefactos DSPy para no pisar la automatizaci√≥n vigente.
